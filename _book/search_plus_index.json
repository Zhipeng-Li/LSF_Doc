{"./":{"url":"./","title":"前言","keywords":"","body":"LSF handbook 内容简介 主要内容是 IBM 官方 LSF manual 的文档翻译，具体内容涉及 LSF 的产品介绍、安装升级、用户操作、作业调度、集群运维、功能开发及拓展等。 其次结合译者的工作需求，会有一些相关知识点的增补，与实际操作经验的总结。大致包含 Linux 运行环境的配置、vim编辑器操作、性能调优、队列日志分析、EDA 作业优化、同类调度器（Slurm／PSB）的功能对比等等。 行文结构 依照 Part > Chapter > Section > Subsection > Article 的行文结构 Part I 入门介绍篇 chapter1 LSF 介绍 重点： lsf 快速入门章节 chapter2 安装、升级与迁移 Part II 基础操作篇 chapter3 用户操作基础 重点：作业生命周期 chapter4 管理员操作基础 重点：配置文件、日志排错 Part III 作业调度篇 chapter5 作业调度管理 重点：LSF daemons， bsub命令 Part IV 集群运维篇 chapter6 集群维护管理 chapter7 参考 Part V 功能拓展篇 chapter8 LSF 拓展 chapter9 最佳实践与建议 Part VI 经验总结篇 chapter10 Linux 操作进阶 重点：免密、脚本编程、vim编辑器等 chapter11 行业结合经验 重点：日志分析，高级调度策略等 chapter11 调度器对比 重点：slurm，PBS等 其它说明 译作初衷 IBM 旗下的作业调度系统 LSF， 作为一款在 HPC 领域内应用广泛的商业调度器，其 manual 是针对多种商业客户而编写的，文档受众主要是各大中小型企业的集群管理者，其次则为数量更多的集群使用者，与少部分功能开发者。但实际上，因为每个企业/非企业级用户的软硬件基础架构，与业务场景会有不同，所以，作为集群的管理者，除了需要熟悉官网中介绍的功能操作外，也有必要结合实际的工作需求，基于所在行业，进行实际经验的总结与梳理等。 故而，本LSF 中文手册是从集群管理者的角度出发，基于 LSF manual，进行的一些翻译与增补，鉴于译者水平精力有限，出现错误纰漏之处在所难免，希望读者不吝批评指正。 版本 基于 版本为 LSF 10.1.0 的 LSF manual。 更新 长期不定时更新关于 前置知识，行业经验的总结。 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-06 23:48:44 "},"chapter1/LSF_introduction.html":{"url":"chapter1/LSF_introduction.html","title":"Chapter 1 LSF 介绍","keywords":"","body":"Chapter 1 LSF 介绍 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-05 13:09:38 "},"chapter1/section1/function.html":{"url":"chapter1/section1/function.html","title":"1.1 LSF 功能","keywords":"","body":"1.1 LSF 功能 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-05 13:09:38 "},"chapter1/section2/architect.html":{"url":"chapter1/section2/architect.html","title":"1.2 LSF 架构","keywords":"","body":"1.2 LSF 架构 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-05 13:09:38 "},"chapter1/section3/concept.html":{"url":"chapter1/section3/concept.html","title":"1.3 LSF 基础概念","keywords":"","body":"1.3 LSF 基础概念 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-05 13:09:38 "},"chapter1/section7/LSF_quick_reference.html":{"url":"chapter1/section7/LSF_quick_reference.html","title":"1.7 LSF 快速上手","keywords":"","body":"1.7 LSF 快速上手 本文主要是关于 LSF 命令、守护进程、配置文件、日志文件以及重要的集群配置参数的快速介绍。 Unix 及 Linux 系统下的安装目录示意图 守护进程的错误日志文件 守护进程的错误日志文件，存储在由 LSF_LOGDIR 变量定义的文件目录下面，该变量值在 lsf.conf 文件中指定。 LSF base system daemon log files LSF batch system daemon log files pim.log.host_name mbatchd.log.host_name res.log.host_name sbatchd.log.host_name lim.log.host_name mbschd.log.host_name 如果在 ego.conf 文件中定义了变量 EGO_LOGDIR ，那么 lim.log.host_name 文件则存储在由变量 EGO_LOGDIR 指定的文件目录中。 配置文件 lsf.conf, lsf.shared, 和 lsf.cluster.cluster_name 文件， 位于由 LSF_CONFDIR 变量定义的文件目录下面，该变量值在 lsf.conf 文件中指定。 lsb.params, lsb.queues, lsb.modules, 和 lsb.resources 文件，位于 LSB_CONFDIR/cluster_name/configdir/ 目录下面。 文件 描述 install.config LSF 的安装与配置选项 lsf.conf 描述集群配置和操作的通用环境配置文件 lsf.shared 所有群集共享的定义文件。用于定义群集名称、主机类型、主机型号和站点定义的资源 lsf.cluster.cluster_name 用于定义主机、管理员和站点定义的共享资源的位置的集群配置文件 lsb.applications 定义应用程序配置文件，来指定同类型作业的公共参数 lsb.params 配置 LSF 批处理参数 lsb.queues 批处理队列配置文件 lsb.resources 配置资源分配限制、导出和资源使用限制 lsb.serviceclasses 将LSF集群中的服务级别协议（SLA）定义为服务类，该服务类定义了 SLA 的属性 lsb.users 配置用户组、用户和用户组的分层公平共享，以及用户和用户组的作业槽数限制 lsf.conf 配置文件中的集群配置参数 参数 描述 Unix 默认值 LSF_BINDIR 包含 LSF 用户命令的目录，由相同类型的所有主机共享 LSF_TOP/version/OStype/bin LSF_CONFDIR 所有 LSF 配置文件的目录 LSF_TOP/conf LSF_ENVDIR 包含 lsf.conf 文件的目录。必须由root 用户所拥有。 /etc (if LSF_CONFDIR is not defined) LSF_INCLUDEDIR 包含 LSF API 头文件 lsf.h 和 lsbatch.h的目录 LSF_TOP/version/include LSF_LIBDIR LSF 库，由相同类型的所有主机共享 LSF_TOP/version/OStype/lib LSF_LOGDIR （可选）LSF 守护程序日志的目录。必须由 root 拥有。 /tmp LSF_LOG_MASK 从 LSF 命令记录错误消息的级别 LOG_WARNING LSF_MANDIR 包含 LSF 手册页的目录 LSF_TOP/version/man LSF_MISC 示例C程序和Shell脚本，以及外部LIM（elim）的模板 LSF_TOP/version/misc LSF_SERVERDIR 由 LSF 守护程序启动的所有服务器二进制文件和 Shell 脚本以及外部可执行文件的目录。必须由root拥有，并由相同类型的所有主机共享 LSF_TOP/version/OStype/etc LSF_TOP 顶级安装目录。 LSF_TOP 的路径必须共享，并且集群中的所有主机都可以访问。它不能是根目录（/）。 Not definedRequired for installation LSB_CONFDIR LSF批处理配置目录的目录，包含用户和主机列表，操作参数和批处理队列 LSF_CONFDIR/lsbatch LSF_LIVE_CONFDIR LSF 实时重新配置目录的目录，该目录由 bconf 命令编写。 LSB_SHAREDIR/cluster_name/live_confdir LSF_SHAREDIR 每个集群的 LSF 批处理作业历史记录和记帐日志文件的目录，必须由首要的 LSF 管理员拥有 LSF_TOP/work LSF_LIM_PORT 用于与 lim 守护程序进行通信的 TCP 服务端口 7879 LSF_RES_PORT 用于与 res 守护程序通信的 TCP 服务端口 6878 LSF_MBD_PORT 用于与 mbatchd 守护程序进行通信的 TCP 服务端口 6881 LSF_SBD_PORT 用于与 sbatchd 守护程序进行通信的TCP服务端口 6882 管理命令 注：只有 LSF 管理员和 root 用户可以使用这些命令。 命令 描述 lsadmin LSF 管理员工具，用于控制 LSF 集群中 LIM 和 RES 守护程序的运行，lsadmin help 显示所有子命令 lsfinstall 使用 install.config 输入文件安装 LSF lsfrestart 在本地集群中的所有主机上重新启动 LSF 守护程序 lsfshutdown 关闭本地集群中所有主机上的 LSF 守护程序 lsfstartup 在本地集群中的所有主机上启动 LSF 守护程序 badmin 用于控制 LSF 批处理系统（sbatchd，mbatchd，主机和队列）的 LSF 管理工具 badmin help 显示所有子命令 bconf 更改活动内存中的 LSF 配置 守护进程 进程名 描述 lim Load Information Manager (LIM): 负载信息管理器：收集有关集群中所有服务器主机的负载和资源信息，并通过 LSLIB 为应用程序提供主机选择服务。 LIM 维护有关静态系统资源和动态负载索引的信息 mbatchd Master Batch Daemon (MBD): 主批处理守护程序：接受并保存所有批处理作业。 MBD通过联系 主LIM 定期检查所有服务器主机上的负载索引。 mbschd Master Batch Scheduler Daemon：主批处理调度守护程序：执行LSF的调度功能，并将作业调度决策发送到 MBD 以进行调度。 该服务在 LSF 主服务器主机上运行 sbatchd Slave Batch Daemon (SBD)：从属批处理守护程序（SBD）：接受来自 MBD 的作业执行请求，并监视作业进度。 控制作业执行，强制执行批处理策略，将作业状态报告给 MBD，然后启动 MBD。 pim Process Information Manager (PIM): 进程信息管理器（PIM）：监视提交的作业在运行时所使用的资源。 PIM 用于强制执行资源限制和负载阈值以及公平分配调度。 res Remote Execution Server (RES): 远程执行服务器（RES）：接受来自所有负载共享应用程序的远程执行请求，并处理远程主机上的I / O以进行负载共享过程。 用户命令 查看集群信息的命令 命令 描述 bhosts 显示主机及其静态和动态资源 blimits 显示正在运行的作业的资源分配限制的相关信息 bparams 显示可调批次系统参数的相关信息 bqueues 显示批处理队列的相关信息 busers 显示用户和用户组的相关信息 lshosts 显示主机及其静态资源信息 lsid 显示当前的 LSF 版本号，集群名称和主控主机名 lsinfo 显示负载分担配置信息 lsload 显示主机的动态负载索引 监测作业与任务的命令 命令 描述 bacct 报告已完成的 LSF 作业的会计统计数据 bapp 显示附加到应用程序配置文件的作业的相关信息 bhist 显示作业的相关历史信息 bjobs 显示作业的相关信息 bpeek 显示未完成作业的标准输出和标准错误 bsla 显示服务类配置的相关信息，以用于面向目标的服务级别协议调度 bstatus 读取或设置外部作业状态消息和数据文件 提交与控制作业的命令 命令 描述 bbot 移动正在等待的作业到队列的末尾 bchkpnt 检查点可检查的工作 bkill 向作业发送信号，一般用于结束作业 bmig 迁移可检查点的或可重新运行的作业 bmod 修改作业的提交选项 brequeue 杀死并重新安排作业 bresize 释放槽位并取消挂起的作业调整大小分配请求 brestart 重新启动检查点作业 bresume 恢复暂停的作业 bstop 暂停作业 bsub 提交作业 bswitch 将未完成的作业从一个队列移至另一队列 btop 移动正在等待的作业到队列首部 bsub 命令 bsub [options] command [arguments] 命令中的部分选项 选项 描述 -ar 指定作业可自动调整大小 -H 提交时将工作保持在 PSUSP 状态 **-I\\ -Ip\\ -Is** 提交批处理交互式作业。 -Ip 创建伪终端。 -is 在shell模式下创建一个伪终端。 -K 提交作业并等待作业完成 -r 使作业可重新运行 -x 排他执行 -app application_profile_name 将作业提交到指定的应用程序配置文件 -b begin_time 在[[month：] day：]：minute 格式的指定日期和时间或之后调度作业 -C core_limit 为属于此作业的所有进程设置每个进程（soft）核心文件的大小限制（KB） -c cpu_time[/host_name \\ /host_model] 限制作业可以使用的总CPU时间。 CPU时间的格式为[hour:]minutes -cwd \"current_working_directory\" 指定作业的当前工作目录 -D data_limit 为属于作业的每个进程设置每个进程（soft）数据段的大小限制（KB） -E \"pre_exec_command [arguments]\" 在作业运行之前，在执行主机上运行指定的pre-exec命令 -Ep \"post_exec_command [arguments]\" 作业完成后，在执行主机上运行指定的post-exec命令 -e error_file 将标准错误输出附加到文件 -eo error_file 将作业的标准错误输出覆盖到指定文件 -F file_limit 为属于作业的每个进程设置每个进程（soft）文件大小限制（KB） -f \"local_file op[remote_file]\" ... 在本地（提交）主机和远程（执行）主机之间复制文件。op是>， 之一 -i input_file \\ -is input_file 从指定文件获取作业的标准输入 -J \"job_name[index_list]%job_slot_limit\" 为作业分配指定的名称。 作业数组 index_list 的格式 start [-end [：step]]，而 ％job_slot_limit 是可以同时运行的最大作业数。 -k \"chkpnt_dir [chkpnt_period][method=method_name]\" 使作业可检查，并指定检查点目录，以分钟为单位的时间段和方法 -M mem_limit 设置每个进程（soft）的内存限制（KB） -m \"host_name [@cluster_name][[!] \\ +[pref_level]] \\ host_group[[!] \\ +[pref_level]] \\ compute_unit[[!] \\ +[pref_level]]...\" 在指定的主机之一上运行作业。 主机或组的名称后的加号（+）表示首选项。 可选地有，正整数表示偏好级别。 数字越高表示偏好越大。 -n min_proc[,max_proc] 指定并行作业所需的最小和最大处理器数量 -o output_file 将标准输出附加到文件 -oo output_file 将作业的标准输出覆盖到指定文件 -p process_limit 限制整个作业的进程数量 -q \"queue_name ...\" 将作业提交到指定的队列之一 -R \"res_req\" [-R \"res_req\" ...] 指定主机资源要求 -S stack_limit 为属于作业的每个进程设置每个进程（soft）堆栈段大小限制（KB） -sla service_class_name 指定要在其中运行作业的服务类 -T thread_limit 设置整个作业的并发线程数限制 -t term_time 以 [[month：] day：] hour：minute 的形式指定作业终止的截止日期 -v swap_limit 设置整个作业的总进程虚拟内存限制（KB） -W run_time[/host_name \\ /host_model] 以[hour：] minute形式设置作业的运行时限制 -h 将命令用法打印到 stderr 并退出 -V 将 LSF 发行版本打印到 stderr 并退出 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-07 01:02:49 "},"chapter2/install_upgrade_and_migrate.html":{"url":"chapter2/install_upgrade_and_migrate.html","title":"Chapter 2 安装、升级与迁移","keywords":"","body":"Chapter 2 安装、升级与迁移 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-05 13:09:38 "},"chapter3/user_fundations.html":{"url":"chapter3/user_fundations.html","title":"Chapter 3 用户操作基础","keywords":"","body":"Chapter 3 用户操作基础 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-05 13:09:38 "},"chapter3/section1/LSF_overview.html":{"url":"chapter3/section1/LSF_overview.html","title":"3.1 LSF 概览","keywords":"","body":"3.1 LSF 概览 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-05 13:09:38 "},"chapter3/section1/cluster_components.html":{"url":"chapter3/section1/cluster_components.html","title":"集群组件","keywords":"","body":"集群组件 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-05 13:09:38 "},"chapter3/section2/Inside_an_LSF_cluster.html":{"url":"chapter3/section2/Inside_an_LSF_cluster.html","title":"3.2 LSF 细观","keywords":"","body":"3.2 LSF 细观 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-05 13:09:38 "},"chapter3/section2/LSF_daemons_and_processes.html":{"url":"chapter3/section2/LSF_daemons_and_processes.html","title":"LSF 服务与进程","keywords":"","body":"LSF 服务与进程 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-05 13:09:38 "},"chapter3/section2/cluster_communication_paths.html":{"url":"chapter3/section2/cluster_communication_paths.html","title":"集群通信方式","keywords":"","body":"集群通信方式 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-05 13:09:38 "},"chapter3/section2/falut_tolerance.html":{"url":"chapter3/section2/falut_tolerance.html","title":"容错","keywords":"","body":"容错 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-05 13:09:38 "},"chapter3/section2/security.html":{"url":"chapter3/section2/security.html","title":"安全","keywords":"","body":"安全 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-05 13:09:38 "},"chapter3/section3/inside_workload_management.html":{"url":"chapter3/section3/inside_workload_management.html","title":"3.3 作业负载管理","keywords":"","body":"3.3 作业负载管理 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-05 13:09:38 "},"chapter3/section3/job_lifecycle.html":{"url":"chapter3/section3/job_lifecycle.html","title":"作业生命周期","keywords":"","body":"作业生命周期 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-05 13:09:38 "},"chapter3/section3/job_submission.html":{"url":"chapter3/section3/job_submission.html","title":"作业提交","keywords":"","body":"作业提交 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-05 13:09:38 "},"chapter3/section3/job_scheduling_and_dispatch.html":{"url":"chapter3/section3/job_scheduling_and_dispatch.html","title":"作业调度","keywords":"","body":"作业调度 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-05 13:09:38 "},"chapter3/section3/host_selection.html":{"url":"chapter3/section3/host_selection.html","title":"节点选择","keywords":"","body":"节点选择 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-05 13:09:38 "},"chapter3/section3/job_execution_environment.html":{"url":"chapter3/section3/job_execution_environment.html","title":"作业运行环境","keywords":"","body":"作业运行环境 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-05 13:09:38 "},"chapter3/section4/LSF_with_EGO_enabled.html":{"url":"chapter3/section4/LSF_with_EGO_enabled.html","title":"3.4 启用 EGO 的 LSF","keywords":"","body":"3.4 启用 EGO 的 LSF © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-05 13:09:38 "},"chapter3/section4/EGO_component_overview.html":{"url":"chapter3/section4/EGO_component_overview.html","title":"EGO 组件概览","keywords":"","body":"EGO 组件概览 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-05 13:09:38 "},"chapter3/section4/resources.html":{"url":"chapter3/section4/resources.html","title":"资源","keywords":"","body":"资源 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-05 13:09:38 "},"chapter3/section4/LSF_resource_sharing.html":{"url":"chapter3/section4/LSF_resource_sharing.html","title":"LSF 资源共享","keywords":"","body":"LSF 资源共享 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-05 13:09:38 "},"chapter4/administrator_fundations.html":{"url":"chapter4/administrator_fundations.html","title":"Chapter 4 管理员操作基础","keywords":"","body":"Chapter 4 管理员操作基础 本章内容是 IBM Spectrum LSF 的管理员概述，掌握本章，可以了解如何管理各种类型的工作负载和集群操作。 LSF 集群概览 概述您的群集以及重要的LSF目录和配置文件的位置。 使用 LSF 调度 启动和停止 LSF 守护程序，重新配置集群属性。 检查 LSF 状态并提交 LSF 作业。 解决 LSF 问题 解决常见的 LSF 问题并了解 LSF 错误信息。 如果在这里找不到解决问题的方法，请联系 IBM 支持。 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-07 01:11:02 "},"chapter4/section1/cluster_overview.html":{"url":"chapter4/section1/cluster_overview.html","title":"4.1 集群概览","keywords":"","body":"4.1 集群概览 Get an overview of your cluster and the location of important LSF directories and configuration files. LSF Terms and Concepts Before you use LSF for the first time, you should read the LSF Foundations Guide for a basic understanding of workload management and job submission, and the Administrator Foundations Guide for an overview of cluster management and operations.Cluster characteristics Find the name of your cluster after installation, cluster administrators, and where hosts are defined.File systems, directories, and files LSF is designed for networks where all hosts have shared file systems, and files have the same names on all hosts.Important directories and configuration files LSF configuration is administered through several configuration files, which you use to modify the behavior of your cluster. © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-06 23:31:01 "},"chapter4/section1/terms_and_concepts.html":{"url":"chapter4/section1/terms_and_concepts.html","title":"术语与概念","keywords":"","body":"术语与概念 Before you use LSF for the first time, you should read the LSF Foundations Guide for a basic understanding of workload management and job submission, and the Administrator Foundations Guide for an overview of cluster management and operations. Parent topic: LSF cluster overview Job states IBM Spectrum LSF jobs have several states. PEND Waiting in a queue for scheduling and dispatch. RUN Dispatched to a host and running. DONE Finished normally with zero exit value. EXIT Finished with nonzero exit value. PSUSP Suspended while the job is pending. USUSP Suspended by user. SSUSP Suspended by the LSF system. POST_DONE Post-processing completed without errors. POST_ERR Post-processing completed with errors. UNKWN The mbatchd daemon lost contact with the sbatchd daemon on the host where the job runs. WAIT For jobs submitted to a chunk job queue, members of a chunk job that are waiting to run. ZOMBI A job becomes ZOMBI if the execution host is unreachable when a non-rerunnable job is killed or a rerunnable job is requeued. Host An LSF host is an individual computer in the cluster. Each host might have more than one processor. Multiprocessor hosts are used to run parallel jobs. A multiprocessor host with a single process queue is considered a single machine. A box full of processors that each have their own process queue is treated as a group of separate machines. Tip The names of your hosts should be unique. They cannot be the same as the cluster name or any queue that is defined for the cluster. Job An LSF job is a unit of work that runs in the LSF system. A job is a command that is submitted to LSF for execution, by using the bsub command. LSF schedules, controls, and tracks the job according to configured policies. Jobs can be complex problems, simulation scenarios, extensive calculations, anything that needs compute power. Job files When a job is submitted to a queue, LSF holds it in a job file until conditions are right for it run. Then, the job file is used to run the job. On UNIX, the job file is a Bourne shell script that is run at execution time. On Windows, the job file is a batch file that is processed at execution time. Interactive batch job An interactive batch job is a batch job that allows you to interact with the application and still take advantage of LSFscheduling policies and fault tolerance. All input and output are through the terminal that you used to type the job submission command. When you submit an interactive job, a message is displayed while the job is awaiting scheduling. A new job cannot be submitted until the interactive job is completed or terminated. Interactive task An interactive task is a command that is not submitted to a batch queue and scheduled by LSF, but is dispatched immediately. LSF locates the resources that are needed by the task and chooses the best host among the candidate hosts that has the required resources and is lightly loaded. Each command can be a single process, or it can be a group of cooperating processes. Tasks are run without using the batch processing features of LSF but still with the advantage of resource requirements and selection of the best host to run the task based on load. Local task A local task is an application or command that does not make sense to run remotely. For example, the ls command on UNIX. Remote task A remote task is an application or command that that can be run on another machine in the cluster. Host types and host models Hosts in LSF are characterized by host type and host model. The following example is a host with type X86_64, with host models Opteron240, Opteron840, Intel_EM64T, and so on. Host type An LSF host type is the combination of operating system and host CPU architecture. All computers that run the same operating system on the same computer architecture are of the same type. These hosts are binary-compatible with each other. Each host type usually requires a different set of LSF binary files. Host model An LSF host model is the host type of the computer, which determines the CPU speed scaling factor that is applied in load and placement calculations. The CPU factor is considered when jobs are being dispatched. Resources LSF resources are objects in the LSF system resources that LSF uses track job requirements and schedule jobs according to their availability on individual hosts. Resource usage The LSF system uses built-in and configured resources to track resource availability and usage. Jobs are scheduled according to the resources available on individual hosts. Jobs that are submitted through the LSF system will have the resources that they use monitored while they are running. This information is used to enforce resource limits and load thresholds as well as fairshare scheduling. LSF collects the following kinds of information: Total CPU time that is consumed by all processes in the job Total resident memory usage in KB of all currently running processes in a job Total virtual memory usage in KB of all currently running processes in a job Currently active process group ID in a job Currently active processes in a job On UNIX and Linux, job-level resource usage is collected through PIM. Load indices Load indices measure the availability of dynamic, non-shared resources on hosts in the cluster. Load indices that are built into the LIM are updated at fixed time intervals. External load indices Defined and configured by the LSF administrator and collected by an External Load Information Manager (ELIM) program. The ELIM also updates LIM when new values are received. Static resources Built-in resources that represent host information that does not change over time, such as the maximum RAM available to user processes or the number of processors in a machine. Most static resources are determined by the LIM at start-up time. Static resources can be used to select appropriate hosts for particular jobs that are based on binary architecture, relative CPU speed, and system configuration. Load thresholds Two types of load thresholds can be configured by your LSF administrator to schedule jobs in queues. Each load threshold specifies a load index value: The loadSched load threshold determines the load condition for dispatching pending jobs. If a host’s load is beyond any defined loadSched, a job cannot be started on the host. This threshold is also used as the condition for resuming suspended jobs. The loadStop load threshold determines when running jobs can be suspended. To schedule a job on a host, the load levels on that host must satisfy both the thresholds that are configured for that host and the thresholds for the queue from which the job is being dispatched. The value of a load index might either increase or decrease with load, depending on the meaning of the specific load index. Therefore, when you compare the host load conditions with the threshold values, you need to use either greater than (>) or less than ( Runtime resource usage limits Limit the use of resources while a job is running. Jobs that consume more than the specified amount of a resource are signaled. Hard and soft limits Resource limits that are specified at the queue level are hard limits while limits that are specified with job submission are soft limits. See the setrlimit man page for information about hard and soft limits. Resource allocation limits Restrict the amount of a resource that must be available during job scheduling for different classes of jobs to start, and which resource consumers the limits apply to. If all of the resource is consumed, no more jobs can be started until some of the resource is released. Resource requirements (bsub -R) The bsub -R option specifies resources requirements for the job. Resource requirements restrict which hosts the job can run on. Hosts that match the resource requirements are the candidate hosts. When LSF schedules a job, it collects the load index values of all the candidate hosts and compares them to the scheduling conditions. Jobs are only dispatched to a host if all load values are within the scheduling thresholds. © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-05 19:22:25 "},"chapter4/section1/cluster_characteristics.html":{"url":"chapter4/section1/cluster_characteristics.html","title":"集群特征","keywords":"","body":"集群特征 Find the name of your cluster after installation, cluster administrators, and where hosts are defined. Cluster name and administrators Your cluster is installed according to the installation options specified by the lsfinstall -f install.configcommand and the options you chose in the install.config file. The cluster name that you specified at installation is part of the name of the LSF_CONFDIR/lsf.cluster.cluster_name file. /usr/share/lsf/lsf_10/conf/lsf.cluster.lsf_10 Cluster administrators are listed in the ClusterAdmins section of the LSF_CONFDIR/lsf.cluster.cluster_namefile. LSF hosts Host types that are installed in your cluster are listed in the Hosts section of theLSF_CONFDIR/lsf.cluster.cluster_name file. The LSF master host is the first host that is configured in the Hosts section ofLSF_CONFDIR/lsf.cluster.cluster_name file. LSF server hosts that are defined in your cluster are indicated by 1 in the server column of the Hosts section in theLSF_CONFDIR/lsf.cluster.cluster_name file. LSF client-only hosts that are defined in your cluster are indicated by 0 in the server column of the Hosts section in theLSF_CONFDIR/lsf.cluster.cluster_name file. © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-06 23:31:01 "},"chapter4/section1/filesystems_directories_and_files.html":{"url":"chapter4/section1/filesystems_directories_and_files.html","title":"文件系统、目录和文件","keywords":"","body":"文件系统、目录和文件 LSF is designed for networks where all hosts have shared file systems, and files have the same names on all hosts. LSF includes support for copying user data to the execution host before a batch job runs, and for copying results back after the job runs. In networks where the file systems are not shared, this support can be used to give remote jobs access to local data. Supported file systems UNIX On UNIX systems, LSF supports the following shared file systems:Network File System (NFS)NFS file systems can be mounted permanently or on demand by using the automount command.Andrew File System (AFS)Supported on an on-demand basis under the parameters of the 9.1.2 integration with some published configuration parameters. Supports sequential and parallel user jobs that access AFS, JOB_SPOOL_DIR on AFS, and job output and error files on AFS.Distributed File System (DCE/DFS)Supported on an on-demand basis. Windows On Windows, directories that contain LSF files can be shared among hosts from a Windows server machine. Non-shared directories and files LSF is used in networks with shared file space. When shared file space is not available, LSF can copy needed files to the execution host before the job runs, and copy result files back to the submission host after the job completes. Some networks do not share files between hosts. LSF can still be used on these networks, with reduced fault tolerance. Example directory structures The following figures show typical directory structures for a new installation on UNIX and Linux or on Microsoft Windows. Depending on which products you installed and platforms you selected, your directory structure might be different. The following figures show typical directory structures for a new installation on UNIX and Linux or on Microsoft Windows. Depending on which products you installed and platforms you selected, your directory structure might be different. UNIX and Linux The following figure shows a typical directory structure for a new UNIX or Linux installation with the lsfinstall command.Microsoft Windows directory structure The following figure shows a typical directory structure for a new Windows installation. The following figure shows a typical directory structure for a new UNIX or Linux installation with the lsfinstall command. The following figure shows a typical directory structure for a new Windows installation. © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-06 23:31:01 "},"chapter4/section1/important_directories_and_configuration_files.html":{"url":"chapter4/section1/important_directories_and_configuration_files.html","title":"重要的文件目录与配置文件","keywords":"","body":"重要的文件目录与配置文件 LSF configuration is administered through several configuration files, which you use to modify the behavior of your cluster. Four important LSF configuration files The following are the four most important files you work with most often: LSF_CONFDIR/lsf.conf LSF_CONFDIR/lsf.cluster.cluster_name LSF_CONFDIR/lsf.shared LSB_CONFDIR/cluster_name/configdir/lsb.queues These files are created during product installation according to the options you specified in the install.config file. After installation, you can change the configuration parameters in these files to suit the needs of your site. Who owns these files Except for LSF_CONFDIR/lsf.conf, which is owned by root, all of these files are owned by the primary LSF administrator, and readable by all cluster users. lsf.conf The most important file in LSF. It contains the paths to the configuration directories, log directories, libraries, and other global configuration information. The location of the lsf.conf file is defined by the LSF_ENVDIR variable. If LSF cannot find this file, it cannot start properly.By default, LSF checks the directory that is defined by the LSF_ENVDIR parameter for the location of the lsf.conf file. If the lsf.conf file is not in LSF_ENVDIR, LSF looks for it in the /etc directory. lsf.cluster.cluster_name Defines the host names, models, and types of all of the hosts in the cluster. It also defines the user names of the LSF administrators, and the locations of different shared resources for one cluster. lsf.shared This file is like a dictionary that defines all the keywords that are used by the cluster. You can add your own keywords to specify the names of resources or host types. lsb.queues Defines the workload queues and their parameters for one cluster. LSF directories The following directories are owned by the primary LSF administrator and are readable by all cluster users: Directory Description Example LSF_CONFDIR LSF configuration directory /usr/share/lsf/cluster1/conf/ LSB_CONFDIR Batch system configuration directory /usr/share/lsf/cluster1/conf/lsbatch/ LSB_SHAREDIR Job history directory /usr/share/lsf/cluster1/work/ LSF_LOGDIR Server daemon error logs, one for each daemon /usr/share/lsf/cluster1/log/ The following directories are owned by root and are readable by all cluster users: Directory Description Example LSF_BINDIR LSF user commands, which are shared by all hosts of the same type /usr/share/lsf/cluster1/10.1/sparc-sol10/bin/ LSF_INCLUDEDIR Header files lsf/lsf.h and lsf/lsbatch.h /usr/share/lsf/cluster1/10.1/include/ LSF_LIBDIR LSF libraries, which are shared by all hosts of the same type /usr/share/lsf/cluster1/10.1/sparc-sol10/lib/ LSF_MANDIR LSF man pages /usr/share/lsf/cluster1/10.1/man/ LSF_MISC Examples and other miscellaneous files /usr/share/lsf/cluster1/10.1/misc/ LSF_SERVERDIR Server daemon binary files, scripts, and other utilities, which are shared by all hosts of the same type /usr/share/lsf/cluster1/10.1/sparc-sol10/etc/ LSF_TOP Top-level installation directory /usr/share/lsf/cluster1/ Other configuration directories can be specified in the LSF_CONFDIR/lsf.conf file. LSF cluster configuration files The following files are owned by the primary LSF administrator and are readable by all cluster users: File Example Global configuration files, which describe the configuration and operation of the cluster /usr/share/lsf/cluster1/conf/ego/cluster1/kernel/ego.conf/usr/share/lsf/cluster1/conf/lsf.conf Keyword definition file that is shared by all clusters. Defines cluster name, host types, host models, and site-specific resources /usr/share/lsf/cluster1/conf/lsf.shared Cluster configuration file that defines hosts, administrators, and location of site-defined shared resources /usr/share/lsf/cluster1/conf/lsf.cluster.cluster1 Mapping files for task names and their default resource requirements /usr/share/lsf/cluster1/conf/lsf.task/usr/share/lsf/cluster1/conf/lsf.task.cluster1 LSF batch workload system configuration files The following files are owned by the primary LSF administrator and are readable by all cluster users: File Example Server hosts and their attributes, such as scheduling load thresholds, dispatch windows, and job slot limits. If no hosts are defined in this file, then all LSF server hosts listed in LSF_CONFDIR/lsf.cluster.cluster_name are assumed to be LSF batch server hosts. /usr/share/lsf/cluster1/conf/lsbatch/cluster1/configdir/lsb.hosts LSF scheduler and resource broker plug-in modules. If no scheduler or resource broker modules are configured, LSF uses the default scheduler plug-in module named schmod_default. /usr/share/lsf/cluster1/conf/lsbatch/cluster1/configdir/lsb.modules LSF batch system parameter file /usr/share/lsf/cluster1/conf/lsbatch/cluster1/configdir/lsb.params Job queue definitions /usr/share/lsf/cluster1/conf/lsbatch/cluster1/configdir/lsb.queues Resource allocation limits, exports, and resource usage limits. /usr/share/lsf/cluster1/conf/lsbatch/cluster1/configdir/lsb.resources LSF user groups, hierarchical fairshare for users and user groups, and job slot limits for users and user groups. Also used to configure account mappings for the LSF multicluster capability. /usr/share/lsf/cluster1/conf/lsbatch/cluster1/configdir/lsb.users Application profiles, which contain common parameters for the same type of jobs, including the execution requirements of the applications, the resources they require, and how they are run and managed. This file is optional. Use the DEFAULT_APPLICATION parameter in the lsb.params file to specify a default application profile for all jobs. LSF does not automatically assign a default application profile. /usr/share/lsf/cluster1/conf/lsbatch/cluster1/configdir/lsb.applicatons LSF batch log files File Example Batch events log /usr/share/lsf/cluster1/work/ cluster1/logdir/lsb.events Batch accounting log /usr/share/lsf/cluster1/work/ cluster1/logdir/lsb.acct Daemon log files LSF server daemon log files are stored in the directory that is specified by LSF_LOGDIR in LSF_CONFDIR/lsf.conf. File Example Load information manager (lim) /usr/share/lsf/cluster1/log/lim.log.hosta Remote execution server (res) /usr/share/lsf/cluster1/log/res.log.hosta Master batch daemon (mbatchd) /usr/share/lsf/cluster1/log/ mbatchd.log.hosta Master scheduler daemon (mbschd) /usr/share/lsf/cluster1/log/mbschd.log.hosta Slave batch daemon (sbatchd) /usr/share/lsf/cluster1/log/sbatchd.log.hosta Process information manager (pim) /usr/share/lsf/cluster1/log/ pim.log.hosta Who owns and who should write to LSF_LOGDIR NoteMake sure that the primary LSF administrator owns the LSF log directory (LSF_LOGDIR parameter), and that root can write to this directory. If an LSF server cannot write to LSF_LOGDIR parameter, the error logs are created in /tmp. Where to go next Use your new IBM Spectrum LSF cluster, described in Work with LSF. © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-06 23:31:01 "},"chapter4/section2/work_with_LSF.html":{"url":"chapter4/section2/work_with_LSF.html","title":"4.2 使用 LSF","keywords":"","body":"4.2 使用 LSF Start and stop LSF daemons, and reconfigure cluster properties. Check LSF status and submit LSF jobs. Start, stop, and reconfigure LSF Use LSF administration commands lsadmin and badmin to start and stop LSF daemons, and reconfigure cluster properties. Check LSF status Use LSF administration commands to check cluster configuration, see cluster status, and LSF batch workload system configuration and status. Run LSF jobs Use the bsub and lsrun commands to run jobs through LSF. Use the bjobs command to see the status of your jobs. Control job execution with the bstop, bresume, and bkill commands. Manage users, hosts, and queues Make your cluster available to users with cshrc.lsf and profile.lsf. Add or remove hosts and queues from your cluster. Configure LSF startup Use the lsf.sudoers file so that LSF administrators can start and stop LSF daemons. Set up LSF to start automatically. Manage software licenses and other shared resources Set up an LSF external LIM (ELIM) to monitor software licenses as dynamic shared resources. Parent topic: IBM Spectrum LSF administrator foundations © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-05 19:25:33 "},"chapter4/section2/subsection1/start_stop_and_reconfigure_LSF.html":{"url":"chapter4/section2/subsection1/start_stop_and_reconfigure_LSF.html","title":"开启、结束与重配置 LSF","keywords":"","body":"开启、结束与重配置 LSF Use LSF administration commands lsadmin and badmin to start and stop LSF daemons, and reconfigure cluster properties. Two LSF administration commands (lsadmin and badmin) ImportantOnly LSF administrators or root can run these commands. To start and stop LSF, and to reconfigure LSF after you change any configuration file, use the following commands: The lsadmin command controls the operation of the lim and res daemons. The badmin command controls the operation of the mbatchd and sbatchd daemons. If you installed LSF as a non-root user By default, only root can start LSF daemons. If the lsfinstall command detected that you installed as non-root user, you chose to configure either a multi-user cluster or a single-user cluster: Multi-user configuration Only root can start LSF daemons. Any user can submit jobs to your cluster.For information about changing ownership and permissions for the lsadmin and badmincommands, see Troubleshooting LSF problems.To permit LSF administrators to start and stop LSF daemons, set up the /etc/lsf.sudoers file, as described in Configure LSF Startup. Single-user Your user account must be primary LSF administrator. You are able to start LSF daemons, but only your user account can submit jobs to the cluster. Your user account must be able to read the system kernel information, such as /dev/kmem. Setting up the LSF environment with cshrc.lsf and profile.lsf Before you use LSF, you must set up the LSF execution environment with the cshrc.lsf or profile.lsf file.Starting your cluster Use the lsadmin and badmin commands to start the LSF daemons.Stopping your cluster Use the lsadmin and badmin commands to stop the LSF daemons.Reconfiguring your cluster with lsadmin and badmin Use the lsadmin and badmin commands to reconfigure LSF after you change any configuration file. © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-06 23:31:01 "},"chapter4/section2/subsection1/Setting up the LSF environment.html":{"url":"chapter4/section2/subsection1/Setting up the LSF environment.html","title":"Setting up the LSF environment","keywords":"","body":"Setting up the LSF environment Before you use LSF, you must set up the LSF execution environment with the cshrc.lsf or profile.lsf file. Procedure After you log in to an LSF host, use one of the following shell environment files to set your LSF environment. In the csh or tcsh shell, run the source command: % source /conf/cshrc.lsf In the sh , ksh , or bash shell run the following command: $ . /conf/profile.lsf The files cshrc.lsf and profile.lsf are created during installation by the lsfinstall command to set up the LSF operating environment. © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-06 23:31:01 "},"chapter4/section2/subsection1/Starting your cluster.html":{"url":"chapter4/section2/subsection1/Starting your cluster.html","title":"Starting your cluster","keywords":"","body":"Starting your cluster Use the lsadmin and badmin commands to start the LSF daemons. Procedure Log in as root to each LSF server host. If you installed a single-user cluster as a non-root user, log in as primary LSF administrator. Start with the LSF master host, and repeat these steps on all LSF hosts. Use the following commands to start the LSF cluster: # lsadmin limstartup # lsadmin resstartup # badmin hstartup Before you use any LSF commands, wait a few minutes for the lim daemon all hosts to do the following operations: Contact each other Select the master host Exchange initialization information © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-06 23:31:01 "},"chapter4/section2/subsection1/Stopping your cluster.html":{"url":"chapter4/section2/subsection1/Stopping your cluster.html","title":"Stopping your cluster","keywords":"","body":"Stopping your cluster Use the lsadmin and badmin commands to stop the LSF daemons. Procedure Log in as root to each LSF server host. If you installed a single-user cluster as a non-root user, log in as primary LSF administrator. Use the following commands to stop the LSF cluster: # badmin hshutdown all # lsadmin resshutdown all # lsadmin limshutdown all © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-06 23:31:01 "},"chapter4/section2/subsection1/Reconfiguring your cluster.html":{"url":"chapter4/section2/subsection1/Reconfiguring your cluster.html","title":"Reconfiguring your cluster","keywords":"","body":"Reconfiguring your cluster Use the lsadmin and badmin commands to reconfigure LSF after you change any configuration file. Procedure Log in as root to each LSF server host. If you installed a single-user cluster as a non-root user, log in as primary LSF administrator. Use the following commands to reconfigure the LSF cluster: Reload modified LSF configuration files and restart lim : ``` # lsadmin reconfig ``` Reload modified LSF batch configuration files: ``` # badmin reconfig ``` Reload modified LSF batch configuration files and restart mbatchd : ``` # badmin mbdrestart ``` This command also reads the LSF_LOGDIR/lsb.events file, so it can take some time to complete if a lot of jobs are running. © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-06 23:31:01 "},"chapter4/section2/subsection2/check_LSF_status.html":{"url":"chapter4/section2/subsection2/check_LSF_status.html","title":"检查 LSF 状态","keywords":"","body":"检查 LSF 状态 Use LSF administration commands to check cluster configuration, see cluster status, and LSF batch workload system configuration and status. Example command output The LSF commands that are shown in this section show examples of typical output. The output that you see might differ according to your configuration. The commands are described briefly so that you can easily use them to verify your LSF installation. See the LSF Command Reference or the LSF man pages for complete usage and command options. You can use these commands on any LSF host. If you get proper output from these commands, your cluster is ready to use. If your output has errors, see Troubleshooting LSF problems for help. Check cluster configuration with the lsadmin command The lsadmin command controls the operation of an LSF cluster and administers the LSF daemons lim and res.Check cluster status with the lsid and lsload commands The lsid command tells you if your LSF environment is set up properly. The lsload command displays the current load levels of the cluster.Check LSF batch system configuration with badmin The badmin command controls and monitors the operation of the LSF batch workload system.Find out batch system status with bhosts and bqueues Use the bhosts command to see whether the LSF batch workload system is running properly. The bqueues command displays the status of available queues and their configuration parameters. © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-06 23:31:01 "},"chapter4/section2/subsection2/Check cluster configuration.html":{"url":"chapter4/section2/subsection2/Check cluster configuration.html","title":"Check cluster configuration","keywords":"","body":"Check cluster configuration The lsadmin command controls the operation of an LSF cluster and administers the LSF daemons lim and res. Use the lsadmin ckconfig command to check the LSF configuration files. The -v option displays detailed information about the LSF configuration: The messages that are shown in the following output are typical of lsadmin ckconfig -v. Other messages might indicate problems with your LSF configuration. % lsadmin ckconfig -v Checking configuration files ... EGO 3.6.0 build 800000, Jul 25 2017 Copyright International Business Machines Corp. 1992, 2016. US Government Users Restricted Rights - Use, duplication or disclosure restricted by GSA ADP Schedule Contract with IBM Corp. binary type: linux2.6-glibc2.3-x86_64 Reading configuration from /opt/lsf/conf/lsf.conf Aug 3 13:45:27 2017 20884 6 3.6.0 Lim starting... Aug 3 13:45:27 2017 20884 6 3.6.0 LIM is running in advanced workload execution mode. Aug 3 13:45:27 2017 20884 6 3.6.0 Master LIM is not running in EGO_DISABLE_UNRESOLVABLE_HOST mode. Aug 3 13:45:27 2017 20884 5 3.6.0 /opt/lsf/10.1/linux2.6-glibc2.3-x86_64/etc/lim -C Aug 3 13:45:27 2017 20884 7 3.6.0 Could not construct product entitlement version array Aug 3 13:45:27 2017 20884 Last message repeated 1 time(s). Aug 3 13:45:27 2017 20884 6 3.6.0 initEntitlement: EGO_AUDIT_MAX_SIZE was not set. Default value will be used. Aug 3 13:45:27 2017 20884 6 3.6.0 initEntitlement: EGO_AUDIT_MAX_ROTATE was not set. Default value will be used. Aug 3 13:45:27 2017 20884 6 3.6.0 LIM is running as IBM Spectrum LSF Standard Edition. Aug 3 13:45:27 2017 20884 6 3.6.0 reCheckClass: numhosts 1 so reset exchIntvl to 15.00 Aug 3 13:45:27 2017 20884 6 3.6.0 Checking Done. --------------------------------------------------------- No errors found. See Troubleshooting LSF problems or the LSF Command Reference for help with some common configuration errors. © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-06 23:31:01 "},"chapter4/section2/subsection2/Check cluster status.html":{"url":"chapter4/section2/subsection2/Check cluster status.html","title":"Check cluster status","keywords":"","body":"Check cluster status The lsid command tells you if your LSF environment is set up properly. The lsload command displays the current load levels of the cluster. lsid command The lsid command displays the current LSF version number, cluster name, and host name of the current LSF master host for your cluster. The LSF master name that is displayed by the lsid command can vary, but it is usually the first host that is configured in the Hosts section of the LSF_CONFDIR/lsf.cluster.cluster_name file. % lsid IBM Spectrum LSF Standard 10.1.0.0, Apr 04 2016 Copyright International Business Machines Corp, 1992-2016. US Government Users Restricted Rights - Use, duplication or disclosure restricted by GSA ADP Schedule Contract with IBM Corp. My cluster name is cluster1 My master name is hosta If you see the message Cannot open lsf.conf file The LSF_ENVDIR environment variable is probably not set correctly. Use the cshrc.lsf or profile.lsf file to set up your environment. See Troubleshooting LSF problems for more help lsload command The output of the lsload command contains one line for each host in the cluster. Normal status is ok for all hosts in your cluster. % lsload HOST_NAME status r15s r1m r15m ut pg ls it tmp swp mem hosta ok 0.0 0.0 0.1 1% 0.0 1 224 43G 67G 3G hostc -ok 0.0 0.0 0.0 3% 0.0 3 0 38G 40G 7G hostf busy *6.2 6.9 9.5 85% 1.1 30 0 5G 400G 385G hosth busy 0.1 0.1 0.3 7% *17 6 0 9G 23G 28G hostv unavail A busy status is shown for hosts with any load index beyond their configured thresholds. An asterisk (*) marks load indexes that are beyond their thresholds, causing the host status to be busy. A minus sign (-) in front of the value ok means that res is not running on that host. If you see one of the following messages after you start or reconfigure LSF, wait a few seconds and try the lsload command again to give the lim daemon on all hosts time to initialize. lsid: getentitlementinfo() failed: LIM is down; try later or LSF daemon (LIM) not responding ... still trying If the problem persists, see Troubleshooting LSF problems for help. Other useful commands The bparams command displays information about the LSF batch system configuration parameters. The bhist command displays historical information about jobs. © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-06 23:31:01 "},"chapter4/section2/subsection2/Check LSF batch system configuration.html":{"url":"chapter4/section2/subsection2/Check LSF batch system configuration.html","title":"Check LSF batch system configuration","keywords":"","body":"Check LSF batch system configuration The badmin command controls and monitors the operation of the LSF batch workload system. Use the badmin ckconfig command to check the LSF batch system configuration files. The -v option displays detailed information about the configuration: The messages in the following output are typical of badmin ckconfig -v. Other messages might indicate problems with your LSF batch workload system configuration. % badmin ckconfig -v Checking configuration files ... Dec 20 12:22:55 2015 20246 9 9.1.3 minit: Trying to call LIM to get cluster name ... Dec 20 12:22:55 2015 20246 9 9.1.3 Batch is enabled Dec 20 12:22:55 2015 4433 9 9.1.3 Checking Done --------------------------------------------------------- No errors found. See Troubleshooting LSF problems or the LSF Command Reference for help with some common configuration errors. © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-06 23:31:01 "},"chapter4/section2/subsection2/Find out batch system status.html":{"url":"chapter4/section2/subsection2/Find out batch system status.html","title":"Find out batch system status","keywords":"","body":"Find out batch system status Use the bhosts command to see whether the LSF batch workload system is running properly. The bqueues command displays the status of available queues and their configuration parameters. To use LSF batch commands, the cluster must be up and running. See Starting your cluster for information about starting LSF daemons. bhosts command The bhosts command displays the status of LSF batch server hosts in the cluster, and other details about the batch hosts: Maximum number of job slots that are allowed by a single user Total number of jobs in the system, running jobs, jobs that are suspended by users, and jobs that are suspended by the system Total number of reserved job slots Normal status ok for all hosts in your cluster. % bhosts HOST_NAME STATUS JL/U MAX NJOBS RUN SSUSP USUSP RSV hosta ok - - 0 0 0 0 0 hostb ok - - 0 0 0 0 0 hostc ok - - 0 0 0 0 0 hostd ok - - 0 0 0 0 0 If you see the following message when you start or reconfigure LSF, wait a few seconds and try the bhosts command again to give the mbatchd daemon time to initialize. batch system daemon not responding ... still trying If the problem persists, see Solving common LSF problems for help. bqueues command LSF queues organize jobs with different priorities and different scheduling policies. The bqueues command displays the status of available queues and their configuration parameters. For a queue to accept and dispatch jobs, the status must be Open:Active. % bqueues QUEUE_NAME PRIO STATUS MAX JL/U JL/P JL/H NJOBS PEND RUN SUSP owners 43 Open:Active - - - - 0 0 0 0 priority 43 Open:Active - - - - 0 0 0 0 night 40 Open:Inact - - - - 0 0 0 0 chkpnt_rerun_qu 40 Open:Active - - - - 0 0 0 0 short 35 Open:Active - - - - 0 0 0 0 license 33 Open:Active - - - - 0 0 0 0 normal 30 Open:Active - - - - 0 0 0 0 idle 20 Open:Active - - - - 0 0 0 0 To see more detailed queue information, use the bqueues -l command: % bqueues -l normal QUEUE: normal -- For normal low priority jobs, running only if hosts are lightly loaded. This is the default queue. PARAMETERS/STATISTICS PRIO NICE STATUS MAX JL/U JL/P JL/H NJOBS PEND RUN SSUSP USUSP RSV 30 20 Open:Active - - - - 0 0 0 0 0 0 Interval for a host to accept two jobs is 0 seconds SCHEDULING PARAMETERS r15s r1m r15m ut pg io ls it tmp swp mem loadSched - - - - - - - - - - - loadStop - - - - - - - - - - - SCHEDULING POLICIES: FAIRSHARE NO_INTERACTIVE USER_SHARES: [default, 1] USERS: all HOSTS: all The bqueues -l command shows the following kinds of information about the queue: What kinds of jobs are meant to run on the queue Resource usage limits Hosts and users able to use the queue Scheduling threshold values: loadSched is the threshold for LSF to stop dispatching jobs automatically loadStop is the threshold for LSF to suspend a job automatically Other useful commands The bparams command displays information about the LSF batch system configuration parameters. The bhist command displays historical information about jobs. © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-06 23:31:01 "},"chapter4/section2/subsection3/run_jobs.html":{"url":"chapter4/section2/subsection3/run_jobs.html","title":"运行作业","keywords":"","body":"运行作业 Use the bsub and lsrun commands to run jobs through LSF. Use the bjobs command to see the status of your jobs. Control job execution with the bstop, bresume, and bkill commands. Run LSF jobs with bsub and lsrun Use two basic commands to run jobs through LSF: bsub submits jobs to the LSF batch scheduler. LSF schedules and dispatches jobs to the best available host based on the scheduling policies you configure in your LSF queues. The lsrun command runs an interactive task on the best available host, based on current system load information gathered by the lim daemon. For most jobs, all you need to do is add either the lsrun or bsub command in front of the job commands you normally use. You usually don't need to modify your executable applications or execution scripts. Submit batch jobs with bsub The bsub command submits jobs to LSF batch scheduling queues.Display job status with bjobs Use the bjobs command to see the job ID and other information about your jobs.Control job execution with bstop, bresume, and bkill Use LSF commands to suspend (bstop), resume (bresume), and kill (bkill) jobs.Run interactive tasks with lsrun and lsgrun The lsrun command runs a task on either the current local host or remotely on the best available host, provided it can find the necessary resources and the appropriate host type. The lsgrun command is similar to lsrun, but it runs a task on a group of hosts.Integrate your applications with LSF By integrating your applications with LSF, you can make sure that your users can submit and run their jobs with correct and complete job submission options without making them learn LSF commands. © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-06 23:31:01 "},"chapter4/section2/subsection3/Submit batch jobs.html":{"url":"chapter4/section2/subsection3/Submit batch jobs.html","title":"Submit batch jobs","keywords":"","body":"Submit batch jobs The bsub command submits jobs to LSF batch scheduling queues. The following command submits a sleep job to the default queue (normal): % bsub sleep 60 Job is submitted to default queue . When a job is submitted to LSF, it is assigned a unique job ID, in this case 3616. You can specify a wide range of job options on the bsub command. For example, you can specify a queue, and the job command sleep 60 is the last option: % bsub -q short sleep 60 Job is submitted to queue . What LSF does with job output By default, when the job is finished, LSF sends email with a job report and any output and error messages to the user account from which the job was submitted. You can optionally save standard output and standard error to files with the -o and -e options. The following command appends the standard output and standard error of the job to the files output.3640 and errors.3640 in the jobs subdirectory of the home directory of user1. % bsub -q short -o /home/user1/job/output.%J -e /home/user1/job/errors.%J ls -l Job is submitted to queue . The %J variable is replaced by the job ID when the files are created. Using %J helps you find job output when you run a lot of jobs. Interactive batch jobs with bsub -I To submit an interactive job through LSF, use the -I option: The following command submits a batch interactive job that displays the output of the ls command: % bsub -I ls To submit a batch interactive job by using a pseudo-terminal, use the bsub -Ip option. To submit a batch interactive job and create a pseudo-terminal with shell mode support, use the bsub -Is option. © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-06 23:31:01 "},"chapter4/section2/subsection3/Display job status.html":{"url":"chapter4/section2/subsection3/Display job status.html","title":"Display job status","keywords":"","body":"Display job status Use the bjobs command to see the job ID and other information about your jobs. The status of each LSF job is updated periodically. % bjobs JOBID USER STAT QUEUE FROM_HOST EXEC_HOST JOB_NAME SUBMIT_TIME 1266 user1 RUN normal hosta hostb sleep 60 Jun 5 17:39:58 The job that is named sleep 60 runs for 60 seconds. When the job completes, LSF sends email to report the job completion. You can use the job ID to monitor the status of a specific job. If all hosts are busy, the job is not started immediately and the STAT column says PEND. © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-06 23:31:01 "},"chapter4/section2/subsection3/Control job execution.html":{"url":"chapter4/section2/subsection3/Control job execution.html","title":"Control job execution","keywords":"","body":"Control job execution Use LSF commands to suspend (bstop), resume (bresume), and kill (bkill) jobs. bstop command To suspend a running job, use the bstop command and specify the job ID: % bstop 1266 Job is being stopped If the job was running when it was stopped, the bjobs command shows USUSP status for job 1266: % bjobs JOBID USER STAT QUEUE FROM_HOST EXEC_HOST JOB_NAME SUBMIT_TIME 1266 user1 USUSP normal hosta hostb sleep 60 Jun 5 17:39:58 Job owners can suspend only their own jobs. LSF administrators can suspend any job. bresume command To resume a suspended job, use the bresume command. % bresume 1266 Job is being resumed If the job resumes immediately, the bjobs command shows RUN status for job 1266: % bjobs JOBID USER STAT QUEUE FROM_HOST EXEC_HOST JOB_NAME SUBMIT_TIME 1266 user1 RUN normal hosta hostb sleep 60 Jun 5 17:39:58 Job owners can resume only their own jobs. LSF administrators can resume any job. bkill command To kill a job, use the bkill command, which sends a signal to the specified jobs. For example, if the job owner or the LSF administrator runs the following command, job 1266 is killed: % bkill 1266 Job is being terminated © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-06 23:31:01 "},"chapter4/section2/subsection3/Run interactive tasks.html":{"url":"chapter4/section2/subsection3/Run interactive tasks.html","title":"Run interactive tasks","keywords":"","body":"Run interactive tasks The lsrun command runs a task on either the current local host or remotely on the best available host, provided it can find the necessary resources and the appropriate host type. The lsgrun command is similar to lsrun, but it runs a task on a group of hosts. The following command runs the ls command. In this case, the command ran through LSF on the local host: % lsrun ls -l /usr/share/lsf/cluster1/conf/ total 742 -rw-r--r-- 1 root lsf 11372 Jul 16 16:23 cshrc.lsf -rw-r--r-- 1 root lsf 365 Oct 25 10:55 hosts drwxr-xr-x 3 lsfadmin lsf 512 Jul 16 15:53 lsbatch -rw-r--r-- 1 lsfadmin lsf 1776 Nov 23 15:13 lsf.conf -rw-r--r-- 1 lsfadmin lsf 8453 Nov 16 17:46 lsf.shared -rw-r--r-- 1 lsfadmin lsf 578 Jul 16 15:53 lsf.task -rw-r--r-- 1 root lsf 10485 Jul 16 17:08 profile.lsf You can also specify a host where you want to run a command. For example, the following command runs the hostname command on the remote host hosta: % lsrun -v -m hosta hostname > hosta The following command runs the hostname command on three remote hosts: % lsgrun -v -m \"hosta hostb hostc\" hostname > hosta > hostb > hostc © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-06 23:31:01 "},"chapter4/section2/subsection3/Integrate your applications with LSF.html":{"url":"chapter4/section2/subsection3/Integrate your applications with LSF.html","title":"Integrate your applications with LSF","keywords":"","body":"Integrate your applications with LSF By integrating your applications with LSF, you can make sure that your users can submit and run their jobs with correct and complete job submission options without making them learn LSF commands. Integrate applications with LSF three ways: Wrapper shell scripts Wrapper binary executables Modifying existing application source code and interfaces Wrapper shell scripts The easiest integration method is to put the bsub command into an executable file like a shell script. A wrapper script is an executable file for launching your application through LSF. It gives users a simple interface to run their jobs that is easy to deploy and maintain. For example, if your application is called abc, rename abc to abc_real and create a wrapper script that is called abc: #! /bin/sh bsub -R \"rusage[abc_license=1:duration=1]\" abc_real When users run abc, they are actually running a script to submit a job abc_real to LSF that uses 1 shared resource named abc_license. For more information about specifying shared resources by using the resource requirement (rusage) string on the -R option of the bsub command, see Manage software licenses and other shared resources. By adding appropriate options to the script, you can enhance your integration: Requeue jobs based on license availability Copy input and output files to and from the local directory on the execution host Calculate and estimate resource requirements Wrapper binary programs A wrapper binary is similar to a wrapper shell script in the form of a compiled binary executable. Compiled wrapper files usually run faster and more efficiently than shell scripts, and they also have access to the LSF API (LSLIB and LSBLIB). Binary code is also more secure because users cannot modify it without the source code and appropriate libraries, but it is more time consuming to develop wrapper binary programs than wrapper shell scripts. Modifying existing application source code and interfaces LSF is already integrated closely with many commonly used software products. IBM and other software application vendors provide facilities and services for closer integration of LSF and other applications. By modifying existing application user interfaces, you can enable easy job submission, license maximization, parallel execution, and other advanced LSF features. In some cases, you are able to run an LSF job directly from the application user interface. Where to go next Learn more about administering your cluster, described in Manage users, hosts, and queues. © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-06 23:31:01 "},"chapter4/section2/subsection4/manage_users_hosts_and_queues.html":{"url":"chapter4/section2/subsection4/manage_users_hosts_and_queues.html","title":"管理用户、节点与队列","keywords":"","body":"管理用户、节点与队列 Make your cluster available to users with cshrc.lsf and profile.lsf. Add or remove hosts and queues from your cluster. Making your cluster available to users with cshrc.lsf and profile.lsf Make sure that all LSF users include either the cshrc.lsf or profile.lsf file at the end of their own .cshrc or .profile file, or run one of these two files before you use LSF.Adding a host to your cluster Use the LSF installation script lsfinstall to add new hosts and host types to your cluster.Removing a host from your cluster Removing a host from LSF involves closing a host to prevent any additional jobs from running on the host and removing references to the host from the lsf.cluster.cluster_name file and other configuration files.Adding a queue Edit the lsb.queues file to add the new queue definition. Adding a queue does not affect pending or running jobs.Removing a queue Edit lsb.queues to remove a queue definition. © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-06 23:31:01 "},"chapter4/section2/subsection4/Making your cluster available to users.html":{"url":"chapter4/section2/subsection4/Making your cluster available to users.html","title":"Making your cluster available to users","keywords":"","body":"Making your cluster available to users Make sure that all LSF users include either the cshrc.lsf or profile.lsf file at the end of their own .cshrc or .profile file, or run one of these two files before you use LSF. About this task To set up the LSF environment for your users, use the following two shell files: LSF_CONFDIR/cshrc.lsf Use this file for csh or tcsh shell. LSF_CONFDIR/profile.lsf Use this file for sh, ksh, or bash shell. Procedure For csh or tcsh shell: Add the cshrc.lsf file to the end of the .cshrc file for all users: Copy the contents of the cshrc.lsf file into the .cshrc file. Add a line with the source command to the end of the .cshrc file: For example, if your the LSF_TOP directory for your cluster is /usr/share/lsf/conf, add the following line to the .cshrc file: ``` source /usr/share/lsf/conf/cshrc.lsf ``` For sh, ksh, or bash shell: Add the profile.lsf file to the end of the .profile file for all users: Copy the contents of the profile.lsf file into the .profile file. For example, if your the LSF_TOP directory for your cluster is /usr/share/lsf/conf , add a line similar to the following to the end of the .profile file: ``` . /usr/share/lsf/conf/profile.lsf ``` © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-06 23:31:01 "},"chapter4/section2/subsection4/Adding a host to your cluster.html":{"url":"chapter4/section2/subsection4/Adding a host to your cluster.html","title":"Adding a host to your cluster","keywords":"","body":"Adding a host to your cluster Use the LSF installation script lsfinstall to add new hosts and host types to your cluster. Before you begin Make sure that you have the LSF distribution files for the host types you want to add. For example, to add a Linux system that runs x86-64 Kernel 2.6 and 3.x to your cluster, get the file lsf10.1_linux2.6-glibc2.3-x86_64.tar.Z. Distribution packages for all supported LSF releases are available for download through IBM Passport Advantage. See LSF System Requirements on IBM developerWorks for a complete list of supported operating systems. The following videos provide more help about downloading LSF through IBM Passport Advantage: YouTube IBM Education Assistant About this task Adding a host to your cluster has the following major steps: Install LSF binary files for the host type. Add host information to the lsf.cluster.cluster_name file. Set up the new host. Procedure Install the binary files for a new host type. Use the lsfinstall command to add new host types to your cluster. If you already have the distribution files for the host types you want to add, you can skip these steps. Log on as root to any host that can access the LSF installation script directory. Change to the installation script directory. # cd /usr/share/lsf/cluster1/10.1/install Edit the install.config file to specify the options you want for new host types. For more information about the install.config file, see the IBM Spectrum LSF Configuration Reference. For information about the lsfinstall command, see Installing IBM Spectrum LSF on UNIX and Linux and the IBM Spectrum LSF Command Reference. Run the ./lsfinstall -f install.config command. Follow the steps for host setup in After Installing LSF in Installing IBM Spectrum LSF on UNIX and Linux (or in the lsf_getting_started.html file that is generated by the lsfinstall script) to set up the new hosts. Add host information to the lsf.cluster.cluster_name file. Log on to the LSF master host as the primary LSF administrator. Edit the LSF_CONFDIR/lsf.cluster.cluster_name file, and add host information for the new host to the Host section. Add the name of the host. Add model or type. If you enter the ! keyword in the model and type columns, the host model is automatically detected by lim running on the host. You might want to use the default values for that host type now, and change them later on when you have more experience or more information. Specify LSF server or client in the server column: - 1 (one) indicates an LSF server host. - 0 (zero) indicates an LSF client-only host. By default, all hosts are considered LSF server hosts. ``` HOSTNAME model type server r1m mem RESOURCES REXPRI hosta ! SUNSOL 1 1.0 4 () 0 hostb ! LINUX 0 1.0 4 () 0 hostc ! HPPA 1 1.0 4 () 0 End Host ``` Save the changes to LSF_CONFDIR/lsf.cluster.cluster_name. Reconfigure lim to enable the new host in the cluster. % lsadmin reconfig Checking configuration files ... No errors found. Do you really want to restart LIMs on all hosts? [y/n] y Restart LIM on ...... done Restart LIM on ...... done Restart LIM on ...... done The lsadmin reconfig command checks for configuration errors. If no unrecoverable errors are found, you are asked to confirm that you want to restart lim on all hosts and lim is reconfigured. If unrecoverable errors are found, reconfiguration exits. Reconfigure mbatchd. % badmin reconfig Checking configuration files ... No errors found. Do you want to reconfigure? [y/n] y Reconfiguration initiated The badmin reconfig command checks for configuration errors. If no unrecoverable errors are found, you are asked to confirm reconfiguration. If unrecoverable errors are found, reconfiguration exits. (Optional) Use the hostsetup command to set up the new host. Log on as root to any host that can access the LSF installation script directory. Change to the installation script directory. # cd /usr/share/lsf/cluster1/10.1/install Run the hostsetup command to set up the new host. # ./hostsetup --top=\"/usr/share/lsf/lsf_62\" --boot=\"y\" For information about the hostsetup command, see Installing IBM Spectrum LSF on UNIX and Linux and the IBM Spectrum LSF Command Reference. Start LSF on the new host. Run the following commands: # lsadmin limstartup # lsadmin resstartup # badmin hstartup Run the bhosts and lshosts commands to verify your changes. If any host type or host model is UNKNOWN or DEFAULT, see Working with hosts in IBM Spectrum LSF Cluster Management and Operations to fix the problem. Results Use dynamic host configuration to add hosts to the cluster without manually changing the LSF configuration. For more information about adding hosts dynamically, see IBM Spectrum LSF Cluster Management and Operations. If you get errors, see Troubleshooting LSF problems for help with some common configuration errors. © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-06 23:31:01 "},"chapter4/section2/subsection4/Removing a host from your cluster.html":{"url":"chapter4/section2/subsection4/Removing a host from your cluster.html","title":"Removing a host from your cluster","keywords":"","body":"Removing a host from your cluster Removing a host from LSF involves closing a host to prevent any additional jobs from running on the host and removing references to the host from the lsf.cluster.cluster_name file and other configuration files. About this task CAUTION Never remove the master host from LSF. If you want to change your current default master host, change the lsf.cluster.cluster_name file to assign a different default master host. Then remove the host that was formerly the master host. Procedure Log on to the LSF host as root. Run badmin hclose to close the host. Closing the host prevents jobs from being dispatched to the host and allows running jobs to finish. Stop all running daemons manually. Remove any references to the host in the Host section of the LSF_CONFDIR/lsf.cluster.cluster_name file. Remove any other references to the host, if applicable, from the following configuration files: LSF_CONFDIR/lsf.shared LSB_CONFDIR/cluster_name/configdir/lsb.hosts LSB_CONFDIR/cluster_name/configdir/lsb.queues LSB_CONFDIR/cluster_name/configdir/lsb.resources Log off the host to be removed, and log on as root or the primary LSF administrator to any other host in the cluster. Run the lsadmin reconfig command to reconfigure LIM. % lsadmin reconfig Checking configuration files ... No errors found. Do you really want to restart LIMs on all hosts? [y/n] y Restart LIM on ...... done Restart LIM on ...... done The lsadmin reconfig command checks for configuration errors. If no errors are found, you are asked to confirm that you want to restart lim on all hosts and lim is reconfigured. If unrecoverable errors are found, reconfiguration exits. Run the badmin mbdrestart command to restart mbatchd. % badmin reconfig Checking configuration files ... No errors found. Do you want to reconfigure? [y/n] y Reconfiguration initiated The badmin mbdrestart command checks for configuration errors. If no unrecoverable errors are found, you are asked to confirm reconfiguration. If unrecoverable errors are found, reconfiguration exits. If you configured LSF daemons to start automatically at system startup, remove the LSF section from the host’s system startup files. For more information about automatic LSF daemon startup, see Setting up automatic LSF startup If any users of the host use the lstcsh shell as their login shell, change their login shell to tcsh or csh. Remove lstcsh from the /etc/shells file. Results Use dynamic host configuration to remove hosts to the cluster without manually changing the LSF configuration. For more information about removing hosts dynamically, see IBM Platform LSF Cluster Management and Operations. If you get errors, see ../lsf_admin/chap_troubleshooting_lsf.html#v3523448 for help with some common configuration errors. © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-06 23:31:01 "},"chapter4/section2/subsection4/Adding a queue.html":{"url":"chapter4/section2/subsection4/Adding a queue.html","title":"Adding a queue","keywords":"","body":"Adding a queue Edit the lsb.queues file to add the new queue definition. Adding a queue does not affect pending or running jobs. Procedure Log in as the administrator on any host in the cluster. Edit the LSB_CONFDIR/cluster_name/configdir/lsb.queues file to add the new queue definition. You can copy another queue definition from this file as a starting point. Remember to change the QUEUE_NAME parameter of the copied queue. Save the changes to the lsb.queues file. When the configuration files are ready, run the badmin ckconfig command to check the new queue definition. If any errors are reported, fix the problem and check the configuration again. Run the badmin reconfig command to reconfigure the cluster. % badmin reconfig Checking configuration files ... No errors found. Do you want to reconfigure? [y/n] y Reconfiguration initiated The badmin reconfig command also checks for configuration errors. If no unrecoverable errors are found, you are asked to confirm reconfiguration. If unrecoverable errors are found, reconfiguration exits. Results If you get errors, see Troubleshooting LSF problems for help with some common configuration errors. For more information about the lsb.queues file, see the Configuration Reference. For more information about the badmin reconfig command, see the Command Reference. Example Begin Queue QUEUE_NAME = normal PRIORITY = 30 STACKLIMIT= 2048 DESCRIPTION = For normal low priority jobs, running only if hosts are lightly loaded. QJOB_LIMIT = 60 # job limit of the queue PJOB_LIMIT = 2 # job limit per processor ut = 0.2 io = 50/240 USERS = all HOSTS = all NICE = 20 End Queue © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-06 23:31:01 "},"chapter4/section2/subsection4/Removing a queue.html":{"url":"chapter4/section2/subsection4/Removing a queue.html","title":"Removing a queue","keywords":"","body":"Removing a queue Edit lsb.queues to remove a queue definition. Before you begin Important Before you remove a queue, make sure that no jobs are running in the queue. Use the bqueues command to view a list of existing queues and the jobs that are running in those queues. If jobs are in the queue that you want to remove, you must switch pending and running jobs to another queue, then remove the queue. If you remove a queue that has pending jobs in it, the jobs are temporarily moved to a lost_and_found queue. The job state does not change. Running jobs continue, and jobs that are pending in the original queue are pending in the lost_and_found queue. Jobs remain pending until the user or the queue administrator uses the bswitch command to switch the jobs into a regular queue. Jobs in other queues are not affected. Procedure Log in as the primary administrator on any host in the cluster. Close the queue to prevent any new jobs from being submitted. badmin qclose night Queue night is closed Switch all pending and running jobs into another queue. For example, the bswitch -q night idle 0 command chooses jobs from the night queue to the idle queue. The job ID number 0 switches all jobs. bjobs -u all -q night JOBID USER STAT QUEUE FROM_HOST EXEC_HOST JOB_NAME SUBMIT_TIME 5308 user5 RUN night hostA hostD job5 Nov 21 18:16 5310 user5 PEND night hostA hostC job10 Nov 21 18:17 bswitch -q night idle 0 Job is switched to queue Job is switched to queue Edit the LSB_CONFDIR/cluster_name/configdir/lsb.queues file and remove or comment out the definition for the queue that you want to remove. Save the changes to the lsb.queues file. Run the badmin reconfig command to reconfigure the cluster. % badmin reconfig Checking configuration files ... No errors found. Do you want to reconfigure? [y/n] y Reconfiguration initiated The badmin reconfig command checks for configuration errors. If no unrecoverable errors are found, you are asked to confirm reconfiguration. If unrecoverable errors are found, reconfiguration exits. Results If you get errors, see Troubleshooting LSF problems for help with some common configuration errors. For more information about the lsb.queues file, see the Configuration Reference. For more information about the badmin reconfig command, see the Command Reference. © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-06 23:31:01 "},"chapter4/section2/subsection5/configure_LSF_startup.html":{"url":"chapter4/section2/subsection5/configure_LSF_startup.html","title":"配置 LSF 启动","keywords":"","body":"配置 LSF 启动 Use the lsf.sudoers file so that LSF administrators can start and stop LSF daemons. Set up LSF to start automatically. Allowing LSF administrators to start LSF daemons with lsf.sudoers To allow LSF administrators to start and stop LSF daemons, configure the /etc/lsf.sudoers file. If the lsf.sudoers file does not exist, only root can start and stop LSF daemons.Setting up automatic LSF startup Configure LSF daemons to start automatically on every LSF server host in the cluster. © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-06 23:31:01 "},"chapter4/section2/subsection5/Allowing LSF administrators to start LSF daemons.html":{"url":"chapter4/section2/subsection5/Allowing LSF administrators to start LSF daemons.html","title":"Allowing LSF administrators to start LSF daemons","keywords":"","body":"Allowing LSF administrators to start LSF daemons To allow LSF administrators to start and stop LSF daemons, configure the /etc/lsf.sudoers file. If the lsf.sudoers file does not exist, only root can start and stop LSF daemons. About this task Using the lsf.sudoers file requires you to enable the setuid bit. Since this allows LSF administration commands to run with root privileges, do not proceed if you do not want these commands to run with root privileges. Procedure Log on as root to each LSF server host. Start with the LSF master host, and repeat these steps on all LSF hosts. Create an /etc/lsf.sudoers file on each LSF host and specify the LSF_STARTUP_USERS and LSF_STARTUP_PATH parameters. LSF_STARTUP_USERS=\"lsfadmin user1\" LSF_STARTUP_PATH=/usr/share/lsf/cluster1/10.1/sparc-sol2/etc LSF_STARTUP_PATH is normally the path to the LSF_SERVERDIR directory, where the LSF server binary files (lim, res, sbatchd, mbatchd, mbschd, and so on) are installed, as defined in your LSF_CONFDIR/lsf.conf file. The lsf.sudoers file must have file permission mode -rw------- (600) and be readable and writable only by root: # ls -la /etc/lsf.sudoers -rw------- 1 root lsf 95 Nov 22 13:57 lsf.sudoers Run the lsfrestart command to restart the cluster: # lsfrestart © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-06 23:31:01 "},"chapter4/section2/subsection5/Setting up automatic LSF startup.html":{"url":"chapter4/section2/subsection5/Setting up automatic LSF startup.html","title":"Setting up automatic LSF startup","keywords":"","body":"Setting up automatic LSF startup Configure LSF daemons to start automatically on every LSF server host in the cluster. Procedure Use the boot=y option of the hostsetup command. © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-06 23:31:01 "},"chapter4/section2/subsection6/manage_software_licenses_and_other_resources.html":{"url":"chapter4/section2/subsection6/manage_software_licenses_and_other_resources.html","title":"管理软件许可证及其他共享资源","keywords":"","body":"管理软件许可证及其他共享资源 Set up an LSF external LIM (ELIM) to monitor software licenses as dynamic shared resources. How LSF uses dynamic shared resources LSF recognizes two main types of resources: Host-based resources are available on all hosts in the cluster, for example, host type and model, or nodelocked software licenses. Shared resources are managed as dynamic load indexes available for a group of hosts in the cluster, for example, networked floating software licenses, shared file systems. Shared resources are shared by a group of LSF hosts. LSF manages shared resources for host selection and batch or interactive job execution. These resources are dynamic resources because the load on the system changes with the availability of the resources. Software licenses as shared resources The most common application of shared resources is to manage software application licenses. You submit jobs that require those licenses and LSF runs the jobs according to their priorities when licenses are available. When licenses are not available, LSF queues the jobs then dispatches them when licenses are free. Configuring application licenses as shared resources ensures optimal use of costly and critical resources. Define dynamic shared resources in an ELIM For LSF to use a shared resource like a software license, you must define the resource in the Resource section of the lsf.shared file. You define the type of resource and how often you want LSF to refresh the value of the resource. For LSF to track the resources correctly over time, you must define them as external load indexes. LSF updates load indexes periodically with a program called an External Load Information Manager (ELIM). An ELIM can be a shell script or a compiled binary program, which returns the values of the shared resources you define. The ELIM must be named elim and located in the LSF_SERVERDIR directory: /usr/share/lsf/lsf/cluser1/10.1/sparc-sol2/etc/elim You can find examples of sample ELIMs in the misc/examples directory. Example of shared licenses In the lsf.shared file, define two dynamic shared resources for software licenses, named license1 and license2: Begin Resource RESOURCENAME TYPE INTERVAL INCREASING RELEASE DESCRIPTION # Keywords license1 Numeric 30 N Y (license1 resource) license2 Numeric 30 N Y (license2 resource) End Resource The TYPE parameter for a shared resource can be one of the following types: Numeric Boolean String In this case, the resource is Numeric. The INTERVAL parameter specifies how often you want the value to be refreshed. In this example, the ELIM updates the value of the shared resources license1 and license2 every 30 seconds. The N in the INCREASING column means that the license resources are decreasing; that is, as more licenses become available, the load becomes lower. The Y in the RELEASE column means that the license resources are released when a job that uses the license is suspended. Map dynamic shared resources to hosts To make LSF aware of where the defined dynamic shared resources license1 and license2 you defined, map them to the hosts where they are located. In the LSF_CONFDIR/lsf.cluster.cluster_name file, configure a ResourceMap section to specify the mapping between shared resources license1 and license2 you defined in the LSF_CONFDIR/lsf.shared file, and the hosts you want to map them to: Begin ResourceMap RESOURCENAME LOCATION license1 [all] license1 [all] End ResourceMap In this resource map, the [all] attribute under the LOCATION parameter means that resources license1 and license2 under the RESOURCENAME parameter are available on all hosts in the cluster. Only one ELIM needs to run on the master host because the two resources are the same for all hosts. If the location of the resources is different on different hosts, a different ELIM must run on every host. Monitor dynamic shared resources For LSF to receive external load indexes correctly, the ELIM must send a count of the available resources to standard output in the following format: number_indexes [index_name index_value] ... The fields in this example contain the following information: 2 license1 3 license2 2 The total number of external load indexes (2) The name of the first external load index (license1) The value of the first load index (3) The name of the second external load index (license2) The value of the second load index (2) Write the ELIM program The ELIM must be an executable program, named elim, located in the LSF_SERVERDIR directory. When the lim daemon is started or restarted, it runs the elim program on the same host and takes the standard output of the external load indexes that are sent by the elim program. In general, you can define any quantifiable resource as an external load index, write an ELIM to report its value, and use it as an LSF resource. The following example ELIM program uses license1 and license2 and assumes that the FLEXlm license server controls them: #!/bin/sh NUMLIC=2 # number of dynamic shared resources while true do TMPLICS='/usr/share/lsf/cluster1/10.1/sparc-sol2/etc/lic -c /usr/share/lsf/cluster1/conf/license.dat -f license1, license2' LICS='echo $TMPLICS | sed -e s/-/_/g' echo $NUMLIC $LICS # $NUMLIC is number of dynamic shared resource sleep 30 # Resource done In the script, the sed command changes the minus sign (-) to underscore (_) in the license feature names because LSF uses the minus sign for calculation, and it is not allowed in resource names. The lic utility is available from IBM Support. You can also use the FLEXlm command lmstat to make your own ELIM. Use the dynamic shared resources To enable the new shared resources in your cluster, restart LSF with the following commands: lsadmin reconfig badmin reconfig If no errors are found, use the lsload -l command to verify the value of your dynamic shared resources: HOST_NAME status r15s r1m r15m ut pg io ls it tmp swp mem license1 license2 hosta ok 0.1 0.3 0.4 8% 0.2 50 73 0 62M 700M 425M 3 0 hostb ok 0.1 0.1 0.4 4% 5.7 3 3 0 79M 204M 64M 3 0 Submit jobs that use shared resources To submit a batch job that uses one license1 resource, use the command following command: % bsub -R 'rusage[license1=1:duration=1]' myjob In the resource requirement (rusage) string, duration=1 means that license1 is reserved for 1 minute to give LSF time to check it out from FLEXlm. You can also specify the resource requirement string at queue level, in the RES_REQ parameter for the queue. In the LSB_CONFDIR/cluster_name/configdir/lsb.queues file, specify the following resource requirement string: Begin Queue QUEUE_NAME = license1 RES_REQ=rusage[license1=1:duration=1] ... End Queue Then, submit a batch job that uses one license1 resource by using the following command: % bsub -q license1 myjob When licenses are available, LSF runs your jobs right away; when all licenses are in use, LSF puts your job in a queue and dispatches them as licenses become available. This way, all of your licenses are used to the best advantage. For more information For more information about the lsf.shared and lsf.cluster.cluster_name files and the parameters for configuring shared resources, see the Configuration Reference. For more information about adding external resources to your cluster and configuring an ELIM to customize resources, see External load indices in Administering IBM Spectrum LSF. © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-06 23:31:01 "},"chapter4/section3/troubleshooting_LSF_problems.html":{"url":"chapter4/section3/troubleshooting_LSF_problems.html","title":"4.3 LSF 排错","keywords":"","body":"4.3 LSF 排错 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-05 13:09:38 "},"chapter4/section3/solving_common_LSF_problems.html":{"url":"chapter4/section3/solving_common_LSF_problems.html","title":"常见 LSF 问题","keywords":"","body":"常见 LSF 问题 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-05 13:09:38 "},"chapter4/section3/LSF_error_messages.html":{"url":"chapter4/section3/LSF_error_messages.html","title":"LSF 错误信息","keywords":"","body":"LSF 错误信息 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-05 13:09:38 "},"chapter5/run_jobs.html":{"url":"chapter5/run_jobs.html","title":"Chapter 5 作业调度管理","keywords":"","body":"Chapter 5 作业调度管理 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-05 13:09:38 "},"chapter5/section1/about_IBM_Spectrum_LSF.html":{"url":"chapter5/section1/about_IBM_Spectrum_LSF.html","title":"5.1 关于 IBM Spectrum LSF","keywords":"","body":"5.1 关于 IBM Spectrum LSF © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-05 13:09:38 "},"chapter5/section1/LSF_clusters_jobs_and_queues.html":{"url":"chapter5/section1/LSF_clusters_jobs_and_queues.html","title":"LSF 集群，作业与队列","keywords":"","body":"LSF 集群，作业与队列 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-05 13:09:38 "},"chapter5/section1/hosts.html":{"url":"chapter5/section1/hosts.html","title":"节点","keywords":"","body":"节点 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-05 13:09:38 "},"chapter5/section1/LSF_daemons.html":{"url":"chapter5/section1/LSF_daemons.html","title":"LSF daemons","keywords":"","body":"LSF daemons © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-05 13:09:38 "},"chapter5/section1/Batch_jobs_and_tasks.html":{"url":"chapter5/section1/Batch_jobs_and_tasks.html","title":"Batch jobs and tasks","keywords":"","body":"Batch jobs and tasks © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-05 13:09:38 "},"chapter5/section1/Host_types_and_host_models.html":{"url":"chapter5/section1/Host_types_and_host_models.html","title":"Host types and host models","keywords":"","body":"Host types and host models © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-05 13:09:38 "},"chapter5/section1/Users_and_administrators.html":{"url":"chapter5/section1/Users_and_administrators.html","title":"Users and administrators","keywords":"","body":"Users and administrators © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-05 13:09:38 "},"chapter5/section1/Resources.html":{"url":"chapter5/section1/Resources.html","title":"Resources","keywords":"","body":"Resources © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-05 13:09:38 "},"chapter5/section1/Job_lifecycle.html":{"url":"chapter5/section1/Job_lifecycle.html","title":"Job lifecycle","keywords":"","body":"Job lifecycle © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-05 13:09:38 "},"chapter5/section2/working_with_jobs.html":{"url":"chapter5/section2/working_with_jobs.html","title":"5.2 作业运行","keywords":"","body":"5.2 作业运行 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-05 13:09:38 "},"chapter5/section2/submitting_jobs_using_bsub.html":{"url":"chapter5/section2/submitting_jobs_using_bsub.html","title":"Submitting jobs (bsub)","keywords":"","body":"Submitting jobs (bsub) © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-05 13:09:38 "},"chapter5/section3/monitoring_jobs.html":{"url":"chapter5/section3/monitoring_jobs.html","title":"5.3 作业监控","keywords":"","body":"5.3 作业监控 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-05 13:09:38 "},"chapter6/Administer_LSF.html":{"url":"chapter6/Administer_LSF.html","title":"Chapter 6 LSF 集群维护管理","keywords":"","body":"Chapter 6 LSF 集群维护管理 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-05 13:09:38 "},"chapter7/Reference.html":{"url":"chapter7/Reference.html","title":"Chapter 7 参考文档","keywords":"","body":"Chapter 7 参考 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-05 13:09:38 "},"chapter8/Extend_LSF.html":{"url":"chapter8/Extend_LSF.html","title":" Chapter 8 LSF 拓展","keywords":"","body":"Chapter 8 LSF 拓展 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-05 13:09:38 "},"chapter9/Best_practices_and_tips.html":{"url":"chapter9/Best_practices_and_tips.html","title":"Chapter 9 最佳实践与建议","keywords":"","body":"Chapter 9 经验与建议总结 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-05 13:09:38 "},"NOTE.html":{"url":"NOTE.html","title":"后记","keywords":"","body":"后记 进度记录 2020.7.2 开始，预计一个月左右，业余时间完成翻译初稿。 首先翻译 快速入门部分； 然后是管理员基本操作； 作业调度管理； 接着是管理员高级操作； 最后是个人经验，以及部分开发拓展部分－－－按需选择； 视情况进行安装等章节的翻译； 联系作者 作者技术博客：BYA's Blog 微信: 15656575965 （联系请备注：姓名－行业） Tel：17626037549 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-06 23:47:28 "}}