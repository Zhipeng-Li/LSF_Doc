{"./":{"url":"./","title":"前言","keywords":"","body":"LSF handbook 内容简介 主要内容是 IBM 官方 LSF manual 的文档翻译，具体内容涉及 LSF 的产品介绍、安装升级、用户操作、作业调度、集群运维、功能开发及拓展等。 其次结合译者的工作需求，会有一些相关知识点的增补，与实际操作经验的总结。大致包含 Linux 运行环境的常见服务配置、vim 编辑器操作、系统性能调优、队列日志分析、EDA 作业优化、同类调度器（Slurm/PBS）的功能对比等等。 重点章节 依照 Part > Chapter > Section > Subsection > Article 的行文结构 Part I 入门介绍篇 chapter1 LSF 介绍 重点： lsf 快速入门章节 chapter2 安装、升级与迁移 Part II 基础操作篇 chapter3 用户操作基础 重点：文件目录，LSF 守护程序与进程，作业生命周期，调度策略 chapter4 管理员操作基础 重点：重要配置文件、服务的启动，资源管理等，日志排错 Part III 作业调度篇 chapter5 作业调度管理 重点：LSF daemons 相关， bsub 命令参数及功能 Part IV 集群运维篇 chapter6 集群维护管理 重点： chapter7 参考文档 重点： Part V 功能拓展篇 chapter8 LSF 拓展 chapter9 最佳实践与建议 Part VI 经验总结篇 chapter10 Linux 操作进阶 重点：常见服务操作、免密、文件服务器、bash脚本编程、vim编辑器等 chapter11 实际实施经验 重点：日志分析，高级调度策略实施等 chapter12 调度器产品对比、行业领域结合等 重点：slurm，PBS等 译作初衷 IBM 旗下的作业调度系统 LSF， 作为一款在 HPC 领域内应用广泛的商业调度器，其 manual 是针对多种商业客户而编写的，文档受众主要是各大中小型企业的集群管理者，其次则为数量更多的集群使用者，与少部分功能开发者。但实际上，因为每个企业 / 非企业级用户的软硬件基础架构，与业务场景会有不同，所以，作为集群的管理者，除了需要熟悉官网中介绍的功能操作外，也有必要结合实际的工作需求，基于所在行业，进行实际经验的总结与梳理等。 故而，本LSF 中文手册是从集群管理及二次开发者的角度出发，基于 LSF manual，进行的一些翻译与增补，鉴于译者水平精力有限，出现错误纰漏之处在所难免，希望读者不吝批评指正。 版本 基于 版本为 LSF 10.1.0 的 LSF manual。 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-11 09:17:00 "},"chapter1/LSF_introduction.html":{"url":"chapter1/LSF_introduction.html","title":"Chapter 1 LSF 介绍","keywords":"","body":"Chapter 1 LSF 介绍 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-08 21:54:54 "},"chapter1/section1/brief_introduction.html":{"url":"chapter1/section1/brief_introduction.html","title":"1.1 LSF 简介","keywords":"","body":"1.1 LSF 简介 Spectrum LSF: 高效的集群管理系统 计算机通过执行程序，帮助科研人员进行科学研究。通常，计算机的使用者不关心程序的执行过程，他们只希望更快更有效地获取运算结果。而为了提供强大的计算能力，大量的计算资源以集群的形式出现。集群系统的使用和有效管理都面临着挑战。 LSF（Load Sharing Facility）是一款分布式集群管理系统软件，负责计算资源的管理和批处理作业的调度。它给用户提供统一的集群资源访问接口，让用户透明地访问整个集群资源。同时提供了丰富的功能和可定制的策略。LSF 具有良好的可伸缩性和高可用性，支持几乎所有的主流操作系统。它通常是高性能计算环境中不可或缺的基础软件。 LSF 虽然是一款商业软件，但它同时也提供免费的社区版供大家下载和使用。 简单的使用 LSF 的使用者可以大约分为两类，普通用户和集群系统管理员。普通用户可以通过命令，将计算程序提交给集群执行，获取计算结果。系统管理员可以通过配置文件和管理命令，管理集群以及统计计算资源的使用情况。 图 1. LSF 结构图 普通用户提交可执行程序或脚本给 LSF。LSF 将已提交的程序称为作业。作业在LSF 的队列 (Queue) 里排队 (PEND) ，等待调度。 清单 1. 提交作业 lsfrhel01 # bsub –R \"linux\" sleep 1000 Job is submitted to default queue . lsfrhel01 # bjobs JOBID USER STAT QUEUE FROM_HOST EXEC_HOST JOB_NAME SUBMIT_TIME 1 tom PEND normal lsfrhel01 *eep 1000 May 9 15:42 LSF 根据配置的调度策略，把作业分配到最合适的计算节点上执行 (RUN) 。用户可以通过命令行查看，控制作业的执行过程。除此之外，LSF 还为用户提供了作业修改，需求描述，作业控制等多种命令行工具。 清单 2. 查看运行作业 lsfrhel01 # bjobs JOBID USER STAT QUEUE FROM_HOST EXEC_HOST JOB_NAME SUBMIT_TIME 1 tom RUN normal lsfrhel01 lsfrhel02 *eep 1000 May 9 15:42 系统管理员通常需要了解整个集群系统中作业和资源的使用状况，LSF 提供的命令帮助管理员快速直观地看到系统概况：系统中队列的状态，机器的状态，作业的资源使用概况，等等。除此之外，LSF 还为管理员提供了丰富的集群配置，控制，管理等功能。 清单 3. 查看 LSF 系统信息 lsfrhel01 # bqueues normal QUEUE_NAME PRIO STATUS MAX JL/U JL/P JL/H NJOBS PEND RUN SUSP normal 30 Open:Active - - - - 1 0 1 0 lsfrhel01 # bhosts HOST_NAME STATUS JL/U MAX NJOBS RUN SSUSP USUSP RSV lsfrhel01 ok - 4 0 0 0 0 0 lsfrhel02 ok - 8 1 1 0 0 0 lsfrhel01 # bacct Accounting information about jobs that are: - submitted by users tom, - accounted on all projects. - completed normally or exited - executed on all hosts. - submitted to all queues. - accounted on all service classes. ------------------------------------------------------------------------------ SUMMARY: ( time unit: second ) Total number of done jobs: 1 Total number of exited jobs: 4 Total CPU time consumed: 4.4 Average CPU time consumed: 0.9 Maximum CPU time of a job: 4.3 Minimum CPU time of a job: 0.0 Total wait time in queues: 276962.0 Average wait time in queue:55392.4 Maximum wait time in queue:276953.0 Minimum wait time in queue: 2.0 Average turnaround time: 60739 (seconds/job) Maximum turnaround time: 276953 Minimum turnaround time: 12 Average hog factor of a job: 0.00 ( cpu time / turnaround time ) Maximum hog factor of a job: 0.00 Minimum hog factor of a job: 0.00 Average expansion factor of a job: 55610.95 ( turnaround time / run time ) Maximum expansion factor of a job: 276953.00 Minimum expansion factor of a job: 1.00 Total Run time consumed: 6981 Average Run time consumed: 1396 Maximum Run time of a job: 6914 Minimum Run time of a job: 0 Total throughput: 409.09 (jobs/hour) during 0.01 hours Beginning time: May 9 15:39 Ending time: May 9 15:40 基本概念 LSF 是资源管理的工具，它管理的主要对象有三个方面：机器（节点），作业和用户。 一般情况下，集群中的机器都是对等的，称为服务节点（Sever Host）。它们既可以提交作业，也可以执行作业。按照职能的不同，服务节点可以有不同的身份。管理作业调度资源的节点称为主节点（Master host），一个集群只有一个主节点。对于一次作业提交来说，提交作业的节点为提交节点（Submission host），被分配并执行作业的节点是执行节点（Execution host）。 图 2. 节点角色 为了便于机器的管理，LSF 提供节点组（Host group）的概念。任意的机器集合可以被作为整体命名，通过名字在集群范围内整体引用。一种典型的用法是将节点按照内存大小进行分类：大内存节点组和小内存节点组。不同的作业可以请求不同的节点组，LSF 按照请求，分配该节点组中的机器来执行作业。节点组还有一个灵活的特性：为方便管理，同一台机器可以定义到不同的节点组。 图 3. 节点分组 作业占用机器资源进行计算。每一个机器被划分成若干个槽位（slot） 。每一个槽位通常可以容纳一个作业运行。 通常情况下，每一个槽位和一个CPU 核心（core）相对应。槽位是一个容易混淆的概念，为便于理解，可以将槽位等同于CPU 核心来看待。对于并行作业来说，比如MPI 作业，每一个都需要占用更多的槽位加速计算。 作业的管理是以队列（Queue）为单位的，调度的策略也是按照队列定制的。作为作业的容器，可以配置多个队列定制不同策略。LSF 按照队列的优先级，从高到低调度每一个队列中的作业，为其分配资源。高优先级队列的作业对资源的使用具有优先权。 图 4. 调度方式 　　 用户是作业提交者，也是资源的真正使用者。对于资源和作业的管理，不少策略都是以用户为单位。为了更好管理用户，LSF 引入用户组（user group）。若干用户集合可以统一命名，并使用该命名统一引用。例如，一个人属于一个部门，一个部门可以作为一个用户组，这种情况下就可以使用部门名字命名和引用。 体系结构 LSF 采用传统的客户机服务器模式，主节点负责整个集群的管理，从节点负责管理运行其上的作业的管理。每一个服务节点包含三个后台进程： Sbatchd 批处理作业管理进程。负责在本节点上执行作业，监控作业状态以及收集其使用的资源。同时根据用户请求或者系统策略触发，控制作业的状态，比如发送SIGSTOP 给作业、挂起作业运行等。 RES 远程执行服务进程。负责执行远程客户请求的任务，主要用于并行作业的远程任务启动、监控以及控制。 LIM 采集本节点的负载信息进程。将收集到的资源负载信息周期上报给主节点上的LIM。主节点的LIM 拥有整个集群资源状态，为其它服务提供集群系统范围内的资源当前快照。 主管理节点在这个基础上，包括两个额外的进程： Mbatchd 集群管理服务进程。它是 LSF 的中心：通过主节点的 LIM 获取机器以及相关资源信息，处理远程用户的作业提交请求，委托 mbschd 将作业调度到相关资源上，发送调度结果到指定机器，并通过和 sbatchd 的交互，监控作业使用的资源，控制作业的执行过程。 Mbschd 调度策略服务进程。从 mbachd 获取作业和资源信息，根据定制策略，为 mbatchd 提供作业调度服务。 图 5. 体系结构 当一个新作业提交的时候，集群管理服务进程检查作业的合法性，并将作业放入到指定的队列中等待调度。经过调度的作业，将获得执行机器上的slot、内存等资源。LSF 负责保留相关资源并将作业指派到已分配机器上执行。作业的状态和实际使用的资源通过批处理作业管理进程报告给集群管理进程。 集群系统构建在分布式网络节点上，节点的失效和网路设备的故障都会导致集群系统的基础环境改变。集群系统的高可用性功能可以保证即使在底层设备故障，LSF 系统仍然可以提供可靠稳定的服务。LSF 采用主备模式实现系统的故障恢复。LSF 的主节点将运行时事件信息写入网络文件系统。当主节点失效后，相关备节点进行选举，选出新的主节点。新的主节点从事件信息文件中恢复作业信息、机器状态，最终获取集群控制权，恢复LSF 状态。 为了更进一步扩展 LSF 的资源管理范围和方式，在单一集群基础上，LSF 还提供多集群互联技术（Multi-Cluster）。多集群通过互联，共享跨集群资源，提供更强大的计算能力。下面是一种典型的多集群架构，集群 1 负责接收作业提交和转发，集群 2 和 集群 3 负责作业执行。 图 6. 多集群结构 资源调度 LSF 收集每一个节点的处理器、内存、交换区、临时存储区等资源信息。主节点掌握全局资源信息。资源的管理和调度以这些信息为基础。 清单 4. 查看节点资源负载信息 lsfrhel01 # lshosts HOST_NAME type model cpuf ncpus maxmem maxswp server RESOURCES lsfrhel01 X86_64 PC6000 116.1 2 1.4G 1.4G Yes (mg) lsfrhel02 X86_64 PC6000 116.1 2 1.4G 1.4G Yes () lsfrhel01 # lsload HOST_NAME status r15s r1m r15m ut pg ls it tmp swp mem lsfrhel01 ok 0.4 0.0 0.0 0% 0.0 1 179 10G 1.4G 1.1G lsfrhel02 ok 1.5 0.3 0.3 1% 0.1 2 1 2391M 1.2G 603M 作业在使用资源的时候，主要考虑三个方面：如何根据作业的资源描述选择合适的执行机器，如何预留机器资源减少运行时的资源竞争，以及如何限制作业对资源的使用避免作业过度消耗资源。 首先是选择执行机。每一个作业会有不同的资源需求。可以通过 LSF 定义的资源描述模式请求资源，运行节点由LSF 根据作业对资源描述来匹配。当一个作业需要一定量内存的时候，\"select[mem>512]\" 表示选择内存大于512M 的机器来运行作业。调度器为其选择拥有合适资源的机器。 其次是确保执行机上的资源分配。既然我们选择了大内存的机器，那么别人的作业也可以选择大内存的机器。如果很多作业都在大内存机器上运行，资源竞争会导致内存短缺，作业最终无法占用到请求的资源。为了更好的确保资源，我们需要在作业运行时保留这些资源不会再分配给别人的作业。\"rusage[mem=512]\" 表示保留 512M 内存给作业，节点当前可用内存将会有 512M 分配给该作业，节点可分配资源将减少512M 的内存。通过 LSF 的策略，可用内存的分配得到控制，已经保留的内存将不会再分配给其它的作业。 最后是保证资源不被过度消耗。资源的保留是通过 LSF 的策略保证，但是作业的进程在运行时，却尚未受 LSF 控制。我们需要在执行节点上，对作业的系统资源设置限制，保证作业（进程）本身不会吃掉过多的内存。通常我们可以设置一个资源上限，防止作业过度消耗资源。比如，内存限制 '1024M'，防止作业使用超过1G 的内存。 清单 5. 提交内存需求的作业 $ bsub -M 1024 -R \"select[mem>512] rusage[mem=512]\" sleep 1000 Job is submitted to default queue . $ bjobs -l 644 Job , User , Project , Status , Queue , Command Wed Dec 28 16:04:00: Submitted from host , CWD , Requested Resources 512] rusage[mem=512]>; MEMLIMIT 1 G Wed Dec 28 16:04:00: Started 1 Task(s) on Host(s) , Allocated 1 Slot (s) on Host(s) , Execution Home , Exe cution CWD ; Wed Dec 28 16:05:00: Resource usage collected. The CPU time used is 4 seconds. MEM: 6 Mbytes; SWAP: 0 Mbytes; NTHREAD: 4 PGID: 23106; PIDs: 23106 23108 23110 SCHEDULING PARAMETERS: r15s r1m r15m ut pg io ls it tmp swp mem loadSched - - - - - - - - - - - loadStop - - - - - - - - - - - RESOURCE REQUIREMENT DETAILS: Combined: select[(mem>512) && (type == local)] order[r15s:pg] rusage[mem=512.0 0] Effective: select[(mem>512) && (type == local)] order[r15s:pg] rusage[mem=512. 00] 对于资源的选择、预留和限制，内存只是一个例子。 LSF 还支持很多种资源，比如 CPU、交换区、临时目录大小等等。虽然这些默认管理的资源始终是有限的，但是 LSF 还提供了资源管理的扩展机制。用户可以统过编写自己的 ELIM 来收集自定义的资源。比如，收集系统的网络带宽以及网络负载，作业可以通过资源描述，选择拥有相应带宽的节点执行作业。资源的种类很多，LSF 提供布尔类型、字符串类型和数字类型来描述多种多样的资源。 定制策略 LSF 提供了非常丰富的调度策略供 LSF 系统管理员选择配置。策略针对作业和资源，从提高资源利用率、优化资源分配、提高用户满意度等各个方面为资源的使用者和管理者提供可配置策略。 默认的策略是先来先服务（FCFS）。顾名思义，先提交的作业先调度，优先获得资源。排在后面的作业必须等待前面的作业运行完成才能开始。后提交作业可能在很长时间得不到机会运行。考虑到资源分配的公平问题，比如部门之间应该平等使用资源，公平共享（Fairshare）策略得以引入。通过配置以部门为单位的用户组平等分享资源，不同用户的作业优先考虑资源所有权。避免了先来先服务导致的资源不平等，从而优化资源分配。 公平共享是从资源分配公平性上考虑问题的，它达到的是一个最终资源使用公平的平衡。但是在使用过程中，如果其他人或者部门没有作业，那么整个集群的资源将会被分配给目前已经提交的作业。作业执行期间，即便新的作业拥有更多的资源优先使用权，也要等待其他作业执行完毕，释放资源后才能执行。这样在短时期内资源使用是不公平的。对于拥有资源使用权却无法执行的情况，资源保障（Resource Guarantee）策略可以为用户保留资源，随时可用。这是在牺牲资源利用率基础上提高用户满意度。 对于并行作业，通常要求很多的 slot，当系统资源紧张的时候，即使等待很久依然无法运行。当系统释放一个 slot 的时候，这个作业因为发现资源不够，便放弃对资源的优先权。排在后面的只需要一个 slot 的作业将获取资源执行。这个过程反复着，并行作业始终无法获取足够资源。为了解决这个问题引入了资源保留策略。虽然当前资源不够运行的，但是这个资源暂时被并行作业占有，等待后面不断释放的资源，直到 slot 满足作业的需求。 但是长时间的等待还是会浪费这些空闲资源，因此又加入回填策略。在不影响并行作业启动时间的前提下，可以将短作业优先调度到已经被保留的 slot 上。 对于特权或者紧急的用户或者应用，当系统资源完全使用的时候。为了尽快执行，抢占（Preemption）策略可以通过抢占已执行作业的资源，执行自己的作业。 除此之外，为了满足用户需求，LSF 提供了资源预留（Advanced Reservation）、SLA 等等。为了提高资源利用率，LSF 提供了NUMA绑定（Affinity）策略等等。对于详尽的策略，可以参考 LSF 管理员手册。LSF 的很多策略都可以自由组合，通过管理员的配置，最终形成丰富的，满足各种需求的定制策略。 即便所有的策略都不能满足你的要求，LSF 的调度策略还实现了插件机制。新的扩展可以通过 LSF 提供的API 实现新的策略。 总结 在高性能计算领域，作业管理系统是一种成熟的系统软件技术。LSF 作为其中的佼佼者，提供了统一的访问接口，丰富的调度策略，灵活的配置和部署。LSF 的体系结构和部署方式让它可以有效管理一定规模的集群系统，目前商业版可以支持多达数千台节点和数百万作业的管理。LSF 的故障恢复机制足以保证集群系统的高可用性。在分布式计算技术不断发展的今天，作为传统的批处理管理软件，依然发挥着重要的作用。 参考资源 下载 IBM Spectrum LSF Community Edition，安装，试用LSF 的基本功能 参考 IBM Spectrum LSF Knowledge Center,了解更多LSF 的使用和集群管理方法 参考 IBM Spectrum LSF 首页,察看更多LSF 产品和技术的最新信息 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-11 09:17:01 "},"chapter1/section2/system_requirements_and_compatibility.html":{"url":"chapter1/section2/system_requirements_and_compatibility.html","title":"1.2 LSF 系统要求与兼容性","keywords":"","body":"1.2 LSF 系统要求与兼容性 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-11 09:17:01 "},"chapter1/section2/operating_system_support.html":{"url":"chapter1/section2/operating_system_support.html","title":"操作系统支持","keywords":"","body":"操作系统支持 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-11 09:17:01 "},"chapter1/section2/master_host_selection.html":{"url":"chapter1/section2/master_host_selection.html","title":"主机选择","keywords":"","body":"主机选择 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-11 09:17:01 "},"chapter1/section2/server_host_compatibility.html":{"url":"chapter1/section2/server_host_compatibility.html","title":"服务器主机兼容性","keywords":"","body":"服务器主机兼容性 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-11 09:17:01 "},"chapter1/section2/add-on_compatibility.html":{"url":"chapter1/section2/add-on_compatibility.html","title":"附加兼容性","keywords":"","body":"附加兼容性 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-11 09:17:01 "},"chapter1/section2/API_compatibility.html":{"url":"chapter1/section2/API_compatibility.html","title":"API 兼容性","keywords":"","body":"API 兼容性 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-11 09:17:01 "},"chapter1/section3/limitations.html":{"url":"chapter1/section3/limitations.html","title":"1.3 局限性","keywords":"","body":"1.3 局限性 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-11 09:17:01 "},"chapter1/section4/release_notes.html":{"url":"chapter1/section4/release_notes.html","title":"1.4 版本更新说明","keywords":"","body":"1.6 版本更新说明 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-11 09:17:01 "},"chapter1/section5/LSF_quick_reference.html":{"url":"chapter1/section5/LSF_quick_reference.html","title":"1.5 LSF 快速上手","keywords":"","body":"1.7 LSF 快速上手 本文主要是关于 LSF 命令、守护进程、配置文件、日志文件以及重要的集群配置参数的快速介绍。 Unix 及 Linux 系统下的安装目录示意图 守护进程的错误日志文件 守护进程的错误日志文件，存储在由 LSF_LOGDIR 变量定义的文件目录下面，该变量值在 lsf.conf 文件中指定。 LSF base system daemon log files LSF batch system daemon log files pim.log.host_name mbatchd.log.host_name res.log.host_name sbatchd.log.host_name lim.log.host_name mbschd.log.host_name 如果在 ego.conf 文件中定义了变量 EGO_LOGDIR ，那么 lim.log.host_name 文件则存储在由变量 EGO_LOGDIR 指定的文件目录中。 配置文件 lsf.conf, lsf.shared, 和 lsf.cluster.cluster_name 文件， 位于由 LSF_CONFDIR 变量定义的文件目录下面，该变量值在 lsf.conf 文件中指定。 lsb.params, lsb.queues, lsb.modules, 和 lsb.resources 文件，位于 LSB_CONFDIR/cluster_name/configdir/ 目录下面。 文件 描述 install.config LSF 的安装与配置选项 lsf.conf 描述集群配置和操作的通用环境配置文件 lsf.shared 所有群集共享的定义文件。用于定义群集名称、主机类型、主机型号和站点定义的资源 lsf.cluster.cluster_name 用于定义主机、管理员和站点定义的共享资源的位置的集群配置文件 lsb.applications 定义应用程序配置文件，来指定同类型作业的公共参数 lsb.params 配置 LSF 批处理参数 lsb.queues 批处理队列配置文件 lsb.resources 配置资源分配限制、导出和资源使用限制 lsb.serviceclasses 将LSF集群中的服务级别协议（SLA）定义为服务类，该服务类定义了 SLA 的属性 lsb.users 配置用户组、用户和用户组的分层公平共享，以及用户和用户组的作业槽数限制 lsf.conf 配置文件中的集群配置参数 参数 描述 Unix 默认值 LSF_BINDIR 包含 LSF 用户命令的目录，由相同类型的所有主机共享 LSF_TOP/version/OStype/bin LSF_CONFDIR 所有 LSF 配置文件的目录 LSF_TOP/conf LSF_ENVDIR 包含 lsf.conf 文件的目录。必须由root 用户所拥有。 /etc (if LSF_CONFDIR is not defined) LSF_INCLUDEDIR 包含 LSF API 头文件 lsf.h 和 lsbatch.h的目录 LSF_TOP/version/include LSF_LIBDIR LSF 库，由相同类型的所有主机共享 LSF_TOP/version/OStype/lib LSF_LOGDIR （可选）LSF 守护程序日志的目录。必须由 root 拥有。 /tmp LSF_LOG_MASK 从 LSF 命令记录错误消息的级别 LOG_WARNING LSF_MANDIR 包含 LSF 手册页的目录 LSF_TOP/version/man LSF_MISC 示例C程序和Shell脚本，以及外部LIM（elim）的模板 LSF_TOP/version/misc LSF_SERVERDIR 由 LSF 守护程序启动的所有服务器二进制文件和 Shell 脚本以及外部可执行文件的目录。必须由root拥有，并由相同类型的所有主机共享 LSF_TOP/version/OStype/etc LSF_TOP 顶级安装目录。 LSF_TOP 的路径必须共享，并且集群中的所有主机都可以访问。它不能是根目录（/）。 Not definedRequired for installation LSB_CONFDIR LSF批处理配置目录的目录，包含用户和主机列表，操作参数和批处理队列 LSF_CONFDIR/lsbatch LSF_LIVE_CONFDIR LSF 实时重新配置目录的目录，该目录由 bconf 命令编写。 LSB_SHAREDIR/cluster_name/live_confdir LSF_SHAREDIR 每个集群的 LSF 批处理作业历史记录和记帐日志文件的目录，必须由首要的 LSF 管理员拥有 LSF_TOP/work LSF_LIM_PORT 用于与 lim 守护程序进行通信的 TCP 服务端口 7879 LSF_RES_PORT 用于与 res 守护程序通信的 TCP 服务端口 6878 LSF_MBD_PORT 用于与 mbatchd 守护程序进行通信的 TCP 服务端口 6881 LSF_SBD_PORT 用于与 sbatchd 守护程序进行通信的TCP服务端口 6882 管理命令 注：只有 LSF 管理员和 root 用户可以使用这些命令。 命令 描述 lsadmin LSF 管理员工具，用于控制 LSF 集群中 LIM 和 RES 守护程序的运行，lsadmin help 显示所有子命令 lsfinstall 使用 install.config 输入文件安装 LSF lsfrestart 在本地集群中的所有主机上重新启动 LSF 守护程序 lsfshutdown 关闭本地集群中所有主机上的 LSF 守护程序 lsfstartup 在本地集群中的所有主机上启动 LSF 守护程序 badmin 用于控制 LSF 批处理系统（sbatchd，mbatchd，主机和队列）的 LSF 管理工具 badmin help 显示所有子命令 bconf 更改活动内存中的 LSF 配置 守护进程 进程名 描述 lim Load Information Manager (LIM): 负载信息管理器：收集有关集群中所有服务器主机的负载和资源信息，并通过 LSLIB 为应用程序提供主机选择服务。 LIM 维护有关静态系统资源和动态负载索引的信息 mbatchd Master Batch Daemon (MBD): 主批处理守护程序：接受并保存所有批处理作业。 MBD通过联系 主LIM 定期检查所有服务器主机上的负载索引。 mbschd Master Batch Scheduler Daemon：主批处理调度守护程序：执行LSF的调度功能，并将作业调度决策发送到 MBD 以进行调度。 该服务在 LSF 主服务器主机上运行 sbatchd Slave Batch Daemon (SBD)：从属批处理守护程序（SBD）：接受来自 MBD 的作业执行请求，并监视作业进度。 控制作业执行，强制执行批处理策略，将作业状态报告给 MBD，然后启动 MBD。 pim Process Information Manager (PIM): 进程信息管理器（PIM）：监视提交的作业在运行时所使用的资源。 PIM 用于强制执行资源限制和负载阈值以及公平分配调度。 res Remote Execution Server (RES): 远程执行服务器（RES）：接受来自所有负载共享应用程序的远程执行请求，并处理远程主机上的I / O以进行负载共享过程。 用户命令 查看集群信息的命令 命令 描述 bhosts 显示主机及其静态和动态资源 blimits 显示正在运行的作业的资源分配限制的相关信息 bparams 显示可调批次系统参数的相关信息 bqueues 显示批处理队列的相关信息 busers 显示用户和用户组的相关信息 lshosts 显示主机及其静态资源信息 lsid 显示当前的 LSF 版本号，集群名称和主控主机名 lsinfo 显示负载分担配置信息 lsload 显示主机的动态负载索引 监测作业与任务的命令 命令 描述 bacct 报告已完成的 LSF 作业的会计统计数据 bapp 显示附加到应用程序配置文件的作业的相关信息 bhist 显示作业的相关历史信息 bjobs 显示作业的相关信息 bpeek 显示未完成作业的标准输出和标准错误 bsla 显示服务类配置的相关信息，以用于面向目标的服务级别协议调度 bstatus 读取或设置外部作业状态消息和数据文件 提交与控制作业的命令 命令 描述 bbot 移动正在等待的作业到队列的末尾 bchkpnt 检查点可检查的工作 bkill 向作业发送信号，一般用于结束作业 bmig 迁移可检查点的或可重新运行的作业 bmod 修改作业的提交选项 brequeue 杀死并重新安排作业 bresize 释放槽位并取消挂起的作业调整大小分配请求 brestart 重新启动检查点作业 bresume 恢复暂停的作业 bstop 暂停作业 bsub 提交作业 bswitch 将未完成的作业从一个队列移至另一队列 btop 移动正在等待的作业到队列首部 bsub 命令 bsub [options] command [arguments] 命令中的部分选项 选项 描述 -ar 指定作业可自动调整大小 -H 提交时将工作保持在 PSUSP 状态 **-I\\ -Ip\\ -Is** 提交批处理交互式作业。 -Ip 创建伪终端。 -is 在shell模式下创建一个伪终端。 -K 提交作业并等待作业完成 -r 使作业可重新运行 -x 排他执行 -app application_profile_name 将作业提交到指定的应用程序配置文件 -b begin_time 在[[month：] day：]：minute 格式的指定日期和时间或之后调度作业 -C core_limit 为属于此作业的所有进程设置每个进程（soft）核心文件的大小限制（KB） -c cpu_time[/host_name \\ /host_model] 限制作业可以使用的总CPU时间。 CPU时间的格式为[hour:]minutes -cwd \"current_working_directory\" 指定作业的当前工作目录 -D data_limit 为属于作业的每个进程设置每个进程（soft）数据段的大小限制（KB） -E \"pre_exec_command [arguments]\" 在作业运行之前，在执行主机上运行指定的pre-exec命令 -Ep \"post_exec_command [arguments]\" 作业完成后，在执行主机上运行指定的post-exec命令 -e error_file 将标准错误输出附加到文件 -eo error_file 将作业的标准错误输出覆盖到指定文件 -F file_limit 为属于作业的每个进程设置每个进程（soft）文件大小限制（KB） -f \"local_file op[remote_file]\" ... 在本地（提交）主机和远程（执行）主机之间复制文件。op是>， 之一 -i input_file \\ -is input_file 从指定文件获取作业的标准输入 -J \"job_name[index_list]%job_slot_limit\" 为作业分配指定的名称。 作业数组 index_list 的格式 start [-end [：step]]，而 ％job_slot_limit 是可以同时运行的最大作业数。 -k \"chkpnt_dir [chkpnt_period][method=method_name]\" 使作业可检查，并指定检查点目录，以分钟为单位的时间段和方法 -M mem_limit 设置每个进程（soft）的内存限制（KB） -m \"host_name [@cluster_name][[!] \\ +[pref_level]] \\ host_group[[!] \\ +[pref_level]] \\ compute_unit[[!] \\ +[pref_level]]...\" 在指定的主机之一上运行作业。 主机或组的名称后的加号（+）表示首选项。 可选地有，正整数表示偏好级别。 数字越高表示偏好越大。 -n min_proc[,max_proc] 指定并行作业所需的最小和最大处理器数量 -o output_file 将标准输出附加到文件 -oo output_file 将作业的标准输出覆盖到指定文件 -p process_limit 限制整个作业的进程数量 -q \"queue_name ...\" 将作业提交到指定的队列之一 -R \"res_req\" [-R \"res_req\" ...] 指定主机资源要求 -S stack_limit 为属于作业的每个进程设置每个进程（soft）堆栈段大小限制（KB） -sla service_class_name 指定要在其中运行作业的服务类 -T thread_limit 设置整个作业的并发线程数限制 -t term_time 以 [[month：] day：] hour：minute 的形式指定作业终止的截止日期 -v swap_limit 设置整个作业的总进程虚拟内存限制（KB） -W run_time[/host_name \\ /host_model] 以[hour：] minute形式设置作业的运行时限制 -h 将命令用法打印到 stderr 并退出 -V 将 LSF 发行版本打印到 stderr 并退出 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-11 09:17:01 "},"chapter2/install_upgrade_and_migrate.html":{"url":"chapter2/install_upgrade_and_migrate.html","title":"Chapter 2 安装、升级与迁移","keywords":"","body":"Chapter 2 安装、升级与迁移 规划您的安装，并安装新的 IBM Spectrum LSF 集群产品，或在 UNIX，Linux 或 Microsoft Windows 主机上升级LSF。 为了使您的集群保证最新状态，建议使用 IBM Fix Central 中的最新修订包。 在 IBM Knowledge Center上的 LSF 安装指南 中可以找到从 Fix Central 应用 Fices 的详细步骤。 在 UNIX 和 Linux 上安装 IBM Spectrum LSF 规划安装，并在 UNIX 或 Linux 主机上安装新的生产版 IBM Spectrum LSF 集群。 在 Windows 上安装 IBM Spectrum LSF 规划安装，并在 Microsoft Windows 主机上安装新的生产版 IBM Spectrum LSF 集群。 使用 IBM Spectrum Cluster Foundation 从裸机安装和部署 IBM Spectrum LSF 使用 IBM Spectrum Cluster Foundation 在受支持的Linux主机上，裸机安装和部署新的生产版 IBM Spectrum LSF 集群。IBM Spectrum Cluster Foundation 是面向技术计算用户的，功能强大的集群管理框架。它提供了一套全面的功能，可帮助从基础架构级别管理硬件和软件。 它使操作系统和软件组件的部署以及复杂的活动（如应用程序集群的创建和系统维护）变得自动化。 升级和迁移 IBM Spectrum LSF 升级 UNIX 或 Linux 集群，或将现有集群迁移到 UNIX，Linux 或 Windows 上的 LSF 10.1版本。 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-09 09:11:45 "},"chapter3/user_fundations.html":{"url":"chapter3/user_fundations.html","title":"Chapter 3 用户操作基础","keywords":"","body":"Chapter 3 用户操作基础 概述 IBM Spectrum LSF 的工作负载管理概念和操作。 IBM Spectrum LSF 概述 了解 LSF 如何满足您的作业要求，并找到最佳资源来运行该作业的。 深入 LSF 集群内部 了解在 LSF 主机上运行的各种守护进程，LSF 集群通信路径，以及 LSF 如何容许集群中的主机故障。 深入工作负载管理 了解 LSF 的作业生命周期。 使用 bsub 命令将作业提交到队列，并指定作业的提交选项以修改默认作业行为。 提交的作业在队列中等待，直到将它们调度并调度到主机来执行。 在作业分发时，LSF 会检查哪些主机有资格运行该作业。 启用 EGO 的 LSF 使启用企业网格协调器（enterprise grid orchestrator EGO）的 LSF 能够提供系统基础结构，来控制和管理集群资源。 资源是应用程序使用的物理和逻辑实体。 LSF资源按照EGO资源分配计划中的定义进行共享。 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-11 10:45:05 "},"chapter3/section1/LSF_overview.html":{"url":"chapter3/section1/LSF_overview.html","title":"3.1 LSF 概览","keywords":"","body":"3.1 LSF 概览 了解 LSF 是如何满足您的作业要求，并找到最佳资源来运行该作业的。 IBM Spectrum LSF 介绍 IBM Spectrum LSF (\"LSF\", load sharing facility 的简称) 软件是行业领先的企业级软件。LSF 将工作分散在现有的各种 IT 资源中，以创建共享的，可扩展的和容错的基础架构，从而提供更快，更可靠的工作负载性能并降低成本。 LSF 平衡负载和分配资源，并提供对这些资源的访问。 LSF集群组件 LSF 集群管理资源，接受和调度工作负载以及监视所有事件。 用户和管理员可以通过命令行界面，API 或通过IBM Spectrum LSF Application Center (PAC) 访问 LSF。 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-11 09:28:38 "},"chapter3/section1/LSF_introduction.html":{"url":"chapter3/section1/LSF_introduction.html","title":"LSF 介绍","keywords":"","body":"LSF 介绍 IBM Spectrum LSF (\"LSF\", load sharing facility 的简称) 软件是行业领先的企业级软件。LSF 将工作分散在现有的各种 IT 资源中，以创建共享的，可扩展的和容错的基础架构，从而提供更快，更可靠的工作负载性能并降低成本。 LSF 平衡负载和分配资源，并提供对这些资源的访问。 LSF 提供了一个资源管理框架，可满足您的工作要求，找到最佳资源来运行该工作并监视其进度。 作业始终根据主机负载和站点策略运行。 Cluster（集群） 运行 LSF 的一组计算机（主机），它们作为一个单元一起工作，结合了计算能力，工作量和资源。 集群为计算资源网络提供单系统映像。 可以通过多种方式将主机分组到集群中。 集群可以包含： 单个管理组中的所有主机 子网中的所有主机 Hosts（主机） 集群中的主机执行不同的功能。 Master host （主节点） LSF 服务器主机，充当集群的整体协调器，负责所有作业的调度和分配。 Server host （服务主机） 提交并运行作业的主机。 Client host （客户主机） 仅提交作业和任务的主机。 Execution host （执行主机） 运行作业和任务的主机。 Submission host （提交主机） 从中提交作业和任务的主机。 Job（作业） 作业是在 LSF 系统中运行的工作单元。 它是一个提交给 LSF 来执行的命令。 LSF 则根据配置的策略，来调度，控制和跟踪作业。 作业可以是复杂的问题，模拟方案，大规模计算或任何需要计算力的事物。 Job slot（作业槽位） 作业槽是一个存储区，在 LSF 系统中将单个工作单元分配到该存储区中。 主机可以配置有多个作业槽，并且您可以从队列中分派作业，直到所有作业槽都被填满。 您可以将作业槽与集群中的 CPU 总数相关联。 Queue（队列） 集群范围内的作业容器。 所有作业都在队列中等待，直到将它们调度并分配到主机为止。 队列不对应单个主机； 每个队列都可以使用集群中的所有服务器主机，或服务器主机的已配置子集。 将作业提交到队列时，无需指定执行主机。 LSF 会将作业分派到集群中，最佳可用的执行主机来运行该作业。 队列执行不同的作业调度和控制策略。 Resources（资源） 资源是集群中可用于运行作业的对象。 例如，资源包括但不限于主机，CPU 槽和许可证。 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-11 09:17:01 "},"chapter3/section1/cluster_components.html":{"url":"chapter3/section1/cluster_components.html","title":"集群组件","keywords":"","body":"集群组件 LSF 集群管理资源，接受和调度工作负载，以及监视所有事件。 用户和管理员可以通过命令行界面，API 或通过IBM Spectrum LSF Application Center (PAC web 界面) 访问 LSF。 IBM Spectrum LSF LSF的核心包括守护程序和其他功能，用于调度和运行作业以及管理集群资源。 IBM Spectrum LSF License Scheduler 策略，控制组织中不同用户之间共享软件许可证的方式。 IBM Spectrum LSF License Scheduler 可与 FlexNet™ 和其他产品一起使用，以控制和监视许可证的使用情况。 LSF 文档 IBM Spectrum LSF documentation in IBM Knowledge Center IBM知识中心是 IBM 产品文档以及您访问所有IBM Spectrum LSF信息的访问点。 在 IBM Knowledge Center 中的所有内容中搜索您感兴趣的主题，或者在产品中搜索，或者将搜索范围限制为产品的一个版本。 使用您的 IBMid 登录以充分利用 IBM Knowledge Center 中提供的个性化功能。 同样，可以通过向主题添加评论来与同事和 IBM 进行交流。 在原始版本的IBM Spectrum LSF 10.1之后，会定期更新和重新生成，可通过IBM Knowledge Center获得的文档。 LSF 随附的 IBM Spectrum LSF Application Center（LSF Application Center）Basic Edition中，提供了该文档的脱机版本。 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-09 10:02:50 "},"chapter3/section2/Inside_an_LSF_cluster.html":{"url":"chapter3/section2/Inside_an_LSF_cluster.html","title":"3.2 LSF 细观","keywords":"","body":"3.2 LSF 细观 了解在 LSF 主机上运行的各种守护进程，LSF 集群通信路径，以及 LSF 如何容许集群中的主机故障。 LSF 守护程序和进程 集群中的每个主机上都运行多个 LSF 进程。 正在运行的进程的类型和数量，取决于主机是主节点还是计算节点。 LSF 集群通信路径 了解集群中 LSF daemon 之间的通信路径。 容错和自动主控主机故障转移 LSF 的强大体系结构在设计时考虑了容错能力。 系统中的每个组件，都有一个恢复操作。关键组件由另一个组件监视，并且可以自动从故障中恢复。 安全性 了解 LSF 安全模型，身份验证和用户角色。 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-09 10:45:53 "},"chapter3/section2/LSF_daemons_and_processes.html":{"url":"chapter3/section2/LSF_daemons_and_processes.html","title":"LSF 服务与进程","keywords":"","body":"LSF 服务与进程 集群中的每个主机上都运行多个 LSF 进程。 正在运行的进程的类型和数量，取决于主机是主节点还是计算节点。 主节点守护程序进程 LSF 主机根据它们在集群中的角色，运行各种守护进程。 守护程序 角色 mbatchd 作业请求与分配 mbschd 作业调度 sbatchd 作业执行 res 作业执行 lim 节点信息 pim 作业进程信息 elim 动态负荷指标 mbatchd 在主节点上运行的主批处理守护程序。 负责系统中作业的总体状态。 接收作业提交和信息学查询请求。管理队列中保留的作业。由 mbschd 确定将作业分配给主机。 mbschd 在主节点上运行的主批处理调度守护程序。 与 mbatchd 一起使用。 根据作业要求，策略和资源可用性制定调度决策。 将调度决策发送到 mbatchd sbatchd 在每个服务器主机（包括主主机）上运行的从属批处理守护程序。 从 mbatchd 接收运行作业的请求，并管理作业的本地执行。 负责执行本地策略并维护主机上的作业状态。 sbatchd 会为每个作业分出一个子sbatchd。 子 sbatchd 运行一个 res 实例，以创建作业在其中运行的执行环境。 作业完成后，子sbatchd 退出。 res 在每个服务器主机上运行的远程执行服务器（RES）。 接受远程执行请求，以提供清晰，安全的作业和任务的远程执行。 lim 在每个服务器主机上运行的负载信息管理器（LIM）。 收集主机负载和配置信息，并将其转发到在主节点上运行的主LIM。报告由 lsload 和 lshosts 显示的信息。 当 LIM 启动或 CPU（ncpus）数量更改时，将报告静态索引。 Master LIM 在主节点上运行的 LIM。从集群中的节点上运行的 LIM，接收负载信息。 将负载信息转发到 mbatchd，后者将信息转发到 mbschd 以支持调度决策。如果主 LIM 不可用，则候选主节点上的 LIM 将自动接管。 PIM 在每个服务器主机上运行的进程信息管理器（PIM）。 由 LIM 启动，它会定期检查 PIM 并在 PIM 挂掉后重新启动。 收集有关主机上运行的作业进程的信息，例如作业使用的 CPU 和内存，并将该信息报告给 sbatchd。 ELIM 外部LIM（ELIM）是一个可在站点定义的可执行文件，用于收集和跟踪自定义动态负载索引。 ELIM 可以是 Shell 脚本或编译的二进制程序，它们返回您定义的动态资源的值。 ELIM 可执行文件必须命名为 elim.anything，并且位于 LSF_SERVERDIR 中。 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-10 12:06:10 "},"chapter3/section2/cluster_communication_paths.html":{"url":"chapter3/section2/cluster_communication_paths.html","title":"集群通信方式","keywords":"","body":"集群通信方式 了解集群中 LSF daemon 之间的通信路径。 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-09 09:11:45 "},"chapter3/section2/falut_tolerance.html":{"url":"chapter3/section2/falut_tolerance.html","title":"容错","keywords":"","body":"容错 LSF 的强大体系结构在设计时考虑了容错能力。 系统中的每个组件都具有恢复操作，因此，重要组件可以由另一个组件监视，并可以自动从故障中恢复。 即使集群中的某些主机不可用，LSF 也可以继续运行。 集群中的一个主机充当主节点，但是如果该主节点不可用，则由另一台主机候选节点接管。当集群中有一个主节点候选时，LSF 可用。 LSF 可以容许集群中，任何主机或主机组的故障。当某主机不可用时，在该主机上运行的所有作业，将会重新排队运行或丢失，具体取决于该作业是否被标记为可重新运行。其他挂起或正在运行的作业，则不会受到影响。 故障转移的工作原理 容错能力取决于事件日志文件 lsb.events，该文件保存在主文件服务器上。系统中的每个事件都记录在该文件中，包括所有作业提交以及作业和主机状态更改。如果主节点不可用，则从主节点候选列表中选择一个新的主节点，新的主节点上的 sbatchd 守护程序，将启动一个新的 mbatchd 守护程序。 新的 mbatchd 守护程序，会读取lsb.events 文件以恢复系统状态。 重复事件记录 对于不希望仅依靠中央文件服务器获取恢复信息的站点，可以将 LSF 配置为通过保留 lsb.events 文件的副本来维护重复的事件日志。 副本存储在文件服务器上，并且在主副本不可用时使用。 启用重复事件日志记录后，主事件日志将本地存储在第一个主主机上，并在主机恢复时与复制的副本重新同步。 主机故障转移 LSF 主节点是动态选择的。如果当前的主节点不可用，则另一台主机将自动接管。故障转移主机，是从 lsf.conf 文件（在安装时在 install.config 文件中指定）的 LSF_MASTER_LIST 参数中定义的列表中选择的。列表中的第一个可用节点充当主机。 正在运行的作业由每个服务器主机上的 sbatchd 守护程序管理。 当新的 mbatchd 守护程序启动时，它将轮询每个主机上的 sbatchd 守护程序，并找到其作业状态。如果 sbatchd 守护程序失效，但主机仍在运行，则主机上正在运行的作业不会丢失。 重新启动 sbatchd 守护程序后，它将重新获得对主机上正在运行的所有作业的控制。 作业故障转移 作业可以通过可重新运行的方式来提交，如此一来，它们可以从头开始自动运行，也可以通过可检查点的形式提交，如此一来，如果由于主机故障而挂掉，则可以从另一个主机上的检查点重新开始。 如果集群中的所有主机都关闭，则所有正在运行的作业都将丢失。 当主节点的候选节点，恢复并接管为主节点时，它将读取 lsb.events 文件，以获取所有批处理作业的状态。 除非系统将其标记为可重新运行，否则系统关闭时，正在运行的作业将被认为已退出，并且电子邮件将发送给提交用户。等待的作业则保留在队列中，并在主机可用时，进行调度。 分区集群 如果集群因网络故障而分区，则 master LIM 会接管分区的每一侧，而候选主节点则在分区的每一侧都可用。 当每个主机仍然可以访问 LSF 可执行文件时，交互式负载共享仍然可用。. 分区网络 如果对网络进行了分区，则只有一个分区可以访问 lsb.events 文件，因此 LSF 服务仅在分区的一侧可用。一个锁定文件，用于确保集群中仅运行一个 mbatchd 守护程序。 作业异常处理 您可以配置主机和队列，以便 LSF 在作业运行时检测到异常情况，并自动采取适当的措施。 您可以自定义检测到哪些异常以及相应的操作。 例如，您可以将 LSF 设置为在作业退出并显示特定错误代码时，自动重新启动。 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-10 10:44:54 "},"chapter3/section2/security.html":{"url":"chapter3/section2/security.html","title":"安全","keywords":"","body":"安全 了解 LSF 安全模型，身份验证和用户角色。 LSF 安全模型 默认情况下，LSF 安全模型在内部跟踪用户帐户。 LSF 中定义的用户帐户，包括用于提供身份验证的密码和用于提供授权的已分配角色，例如管理员。 LSF 用户角色 没有启用EGO的 LSF 支持以下用户角色： LSF 用户 有权将作业提交到 LSF 集群，并查看作业和群集的状态。 LSF 主要管理员 有权执行集群范围的操作，更改配置文件，重新配置集群以及控制所有用户提交的作业。lsb.params 和lsb.hosts 等配置文件，可配置 LSF 的各个方面。 LSF 管理员 有权执行影响其他LSF用户的操作。集群管理员可以对集群中的所有作业和队列，执行管理操作。可能没有更改 LSF 配置文件的权限。 队列管理员的管理权限仅限于指定的队列。 主机组的管理员管理权限仅限于指定的主机组。 用户组的管理员管理权限仅限于指定的用户组。 启用 EGO 的 LSF 用户角色 启用 EGO 的 LSF，支持以下角色： 集群管理员 可以管理集群中的任何对象和工作负载。 消费者管理员 可以管理他们有权访问的使用者中的任何对象和工作负载。 消费者用户 可以在他们有权访问的使用者中运行工作负载。 用户帐户是在 EGO 中创建和管理的。 EGO 从其用户数据库授权用户。 LSF 用户组 在可以指定 LSF 用户组的任何位置上，指定 UNIX 或 Linux 用户组，以此来直接使用任何现有的 UNIX 和 Linux 用户组。 外部认证 LSF 为倾向于使用外部或第三方安全机制（例如 Kerberos，LDAP 或 ActiveDirectory）的站点，提供了一个安全插件。 您可以创建自定义的 eauth 可执行文件，以提供用户，主机和守护程序的外部身份验证。 凭证是从外部安全系统传递的。还可以通过自定义的 eauth 可执行文件，来从操作系统或从诸如 Kerberos 的身份验证协议获取凭据。 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-09 10:45:35 "},"chapter3/section3/inside_workload_management.html":{"url":"chapter3/section3/inside_workload_management.html","title":"3.3 作业负载管理","keywords":"","body":"3.3 作业负载管理 了解 LSF 作业生命周期。 使用 bsub 将作业提交到队列，并指定作业提交选项以修改默认作业行为。 提交的作业在队列中等待，直到将它们调度并分配到主机上来执行。 在作业分发时，LSF 会检查哪些主机有资格运行该作业。 作业生命周期 LSF 作业会经历几种状态，从作业提交开始，到分发，执行和最终返回作业结果。 作业提交 使用 bsub 命令在命令行上提交作业。 您可以使用 bsub 命令指定许多选项来修改默认行为。 作业必须提交到队列中。 作业调度和分配 提交的作业在队列中等待，直到将它们调度并分配到主机上来执行。 主机选择 每次 LSF 尝试分派作业时，它都会检查哪些主机有资格运行该作业。 作业执行环境 当 LSF 运行作业时，它将环境从提交主机，复制到执行主机。 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-11 09:29:33 "},"chapter3/section3/job_lifecycle.html":{"url":"chapter3/section3/job_lifecycle.html","title":"作业生命周期","keywords":"","body":"作业生命周期 LSF 作业经历几种状态，从作业提交开始，到调度，执行和最终返回作业结果。 1. 提交一个作业 您可以使用 bsub 命令从 LSF 客户端或服务器提交作业。 如果在提交作业时未指定队列，则该作业将提交到默认队列。 作业在等待调度时被排在队列中。 等待的作业处于 PEND 状态。 如果在配置文件lsb.params中定义了 MAX_INFO_DIRS 参数，则该作业将保存在 LSF_SHAREDIR/cluster_name/logdir/info/ 目录中的作业文件中，或者在其子目录之一中。 作业 ID 提交作业时，LSF 为每个作业分配唯一的作业 ID。 作业名 您还可以使用 bsub的 -J 选项为作业分配一个任意名称。 与作业 ID 不同，作业名称不一定是唯一的。 2. 调度该作业 1 主批处理守护程序（mbatchd）查看队列中的作业，然后将要调度的作业，发送到主批处理调度程序守护程序（mbschd）。 作业以预设的时间间隔进行调度（由配置文件 lsb.params 中的参数 JOB_SCHEDULING_INTERVAL 定义）。 2 mbschd 根据以下内容评估作业并制定计划决策： 作业优先级 调度策略 可用资源 3 mbschd 选择作业可以运行的最佳主机，并将其决策发送回 mbatchd。资源信息由主负载信息管理器（LIM）以预设的时间间隔，从服务器主机上的 LIM 上收集。 主 LIM 将此信息传达给 mbatchd，后者又将其传达给 mbschd 以支持调度决策。 3. 分配该作业 mbatchd 收到调度决策后，便立即将作业分配给主机。 4. 运行该作业 从属批处理守护程序（sbatchd）具有以下功能： 1 从 mbatchd 接收请求。 2 为作业创建一个子 sbatchd 。 3 创建执行环境。 4 通过使用远程执行服务器（res）启动作业。 LSF 将执行环境从提交主机复制到执行主机： 作业所需的环境变量 作业开始运行的工作目录 其他与系统有关的环境设置 在 UNIX 和 Linux 上，资源限制和 umask 在 Windows，桌面和 Windows 根目录 作业在提交该作业的用户帐户下运行，并且状态为 RUN。 5. 返回结果 作业完成后，如果作业完成没有任何问题，则会被分配为 \"DONE\" 状态。 如果错误导致作业无法完成，则会为该作业分配 \"EXIT\" 状态。 sbatchd 传递作业信息，例如错误信息，并输出到 mbatchd。 6. 向客户发送 email mbatchd 通过电子邮件将作业输出，作业错误和作业信息返回给提交主机。 使用 bsub 的 -o 和 -e 选项将作业输出和错误发送到文件。 作业报告通过电子邮件发送给 LSF 客户端，其中包括以下信息： 作业信息: CPU 使用 内存使用 提交作业的帐户名称 作业输出 错误 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-11 10:04:29 "},"chapter3/section3/job_submission.html":{"url":"chapter3/section3/job_submission.html","title":"作业提交","keywords":"","body":"作业提交 使用 bsub 命令在命令行上提交作业。 您可以使用 bsub 命令指定许多选项来修改默认行为。 而作业必须提交到队列中。 队列 队列代表一组挂起等待的作业，它们按定义的顺序排列，并等待其使用资源的机会。 队列执行不同的作业调度和控制策略。 队列有以下特征: 优先级 名称 队列限制 (对主机、作业数、用户、组、或者处理器的限制) 标准 UNIX 和 Linux 限制（内存，交换，进程，CPU） 调度策略 管理员 运行条件 负载共享阈值条件 UNIX nice 值 (设置 UNIX 和 Linux 调度程序优先级) 队列优先级 定义搜索队列，以确定要处理的作业的顺序。 LSF 管理员为队列分配了优先级，其中数值越高，优先级越高。 LSF按从高到低的优先级为队列提供服务。 如果多个队列具有相同的优先级，则 LSF 按照先来先服务的顺序，调度这些队列中的所有作业。 自动队列选择 提交作业时，LSF 会考虑作业要求，并自动从候选默认队列列表中，选择合适的队列。 LSF 根据以下约束条件选择合适的队列： 用户访问限制 不允许该用户提交作业的队列，不会予以考虑。 节点限制 如果作业明确指定了可以在其上运行作业的主机节点列表，则必须将所选的队列，配置为作业发送到列表中的主机。 队列状态 不考虑关闭的队列。 排他执行限制 如果作业需要排他执行，则未配置为接受排他作业的队列，将不予考虑。 作业所需的资源 作业请求的资源，必须在所选队列的资源分配限制内。 如果多个队列满足上述要求，则选择满足要求的候选队列中，列出的第一个队列。 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-11 10:04:19 "},"chapter3/section3/job_scheduling_and_dispatch.html":{"url":"chapter3/section3/job_scheduling_and_dispatch.html","title":"作业调度","keywords":"","body":"作业调度与分配 提交的作业在队列中等待，直到将它们调度并分配到主机以执行。 将作业提交给 LSF 时，许多因素控制着作业开始的时间和地点: 队列或主机的活动时间窗口 作业的资源需求 合格主机的可用性 各种作业槽位限制 作业依赖条件 Fairshare 限制（已配置的用户共享策略） 负载条件 调度策略 为了解决各种问题，LSF 允许在同一集群中，使用多种调度策略。 LSF 有几种队列调度策略，例如排他，抢占，公平共享和分层公平共享。 先来先服务（FCFS）调度: 默认情况下，队列中的作业按 FCFS 顺序分派。 这意味着作业将根据其在队列中的顺序进行调度。 服务水平协议（SLA）调度: LSF 中的 SLA 是 “及时” 的调度策略，用于调度 LSF 管理员和 LSF 用户之间约定的服务。 SLA 调度策略定义应从每个 SLA 运行多少作业，以达到配置的目标。 公平共享调度: 如果您为队列指定一个公平共享调度策略，或者如果已配置主机分区，则 LSF 会根据分配的用户份额，资源使用，或其他因素在用户之间调度作业。 抢占式调度: 您可以指定所需的行为，以便当两个或多个作业竞争同一资源时，一个作业优先于另一个作业。 抢占不仅适用于作业槽位，而且还适用于提前预订（为特定作业保留主机节点）和许可证（使用 IBM Platform License Scheduler）。 回填式调度： 允许小型作业在为其他作业保留的作业槽位上运行，前提是，回填作业在保留时间到期，且资源使用到期之前完成。 调度与分配 定期安排作业（默认为5秒）。 一旦安排了作业，就可以立即将其分配给主机。 为了防止任何节点过载，默认情况下，LSF 在将作业分发到同一节点之间，会等待一小段时间。 分配顺序 作业不一定按提交顺序分派。 定义队列时，每个队列都有一个由 LSF 管理员设置的优先级编号。LSF 会尝试首先从优先级最高的队列中，启动作业。 LSF 按以下顺序考虑要分派的作业： 对于每个队列，从最高优先级到最低优先级。 如果多个队列具有相同的优先级，则 LSF 会按照先来先服务（FCFS）的顺序，调度这些队列中的所有作业。 对于队列中的每个作业，根据 FCFS 顺序。 如果有任何主机有资格运行此作业，将在最合格的主机上启动该作业，并标记该主机不具备启动任何其他作业的资格，直到经过 JOB_ACCEPT_INTERVAL 参数所指定的时间段为止。 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-11 10:35:27 "},"chapter3/section3/host_selection.html":{"url":"chapter3/section3/host_selection.html","title":"节点选择","keywords":"","body":"节点选择 每次 LSF 尝试分配作业时，它都会检查哪些主机有资格运行该作业。 许多条件决定了节点是否符合条件: 主机调度窗口 作业的资源需求 队列的资源需求 队列中的节点列表 节点负载水平 节点的作业槽位限制 用户配额与用户限制 只有满足所有条件，主机节点才有资格运行作业。 如果队列中已有一个作业，并且该作业的合格主机可用，则该作业将放置在该主机上。 如果有多个主机符合条件，则根据作业和队列资源要求，在最佳主机上启动作业。 主机负载级别 如果主机的负载索引（例如r1m，pg，mem）的值，在配置的调度阈值之内，则该主机可用。 您可以配置两种调度阈值：主机和队列。 如果主机上的任何负载索引，超过相应的主机阈值或队列阈值，则该主机不符合运行任何作业的条件。 合格的主机 当 LSF 尝试放置作业时，它将获取所有主机的当前负载信息。 将每个主机上的负载级别，与在 lsb.hosts 的 Host 部分中，为该主机配置的调度阈值，以及在 lsb.queues 中配置的按队列调度阈值进行比较。 如果任何负载索引，超过其按队列或按主机调度的阈值，则不会在该主机上启动任何新作业。 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-11 10:44:42 "},"chapter3/section3/job_execution_environment.html":{"url":"chapter3/section3/job_execution_environment.html","title":"作业运行环境","keywords":"","body":"作业运行环境 当 LSF 运行作业时，它将环境，从提交主机复制到执行主机。 执行环境包括以下信息: 作业所需的环境变量 作业开始运行的工作目录 其他与系统有关的环境设置，例如资源使用限制 共享的用户目录 为了提供透明的远程执行，LSF 命令确定用户的当前工作目录，并在远程主机上使用该目录。 可执行文件和 PATH 环境变量 可执行文件的搜索路径（PATH环境变量），未更改地传递到远程执行主机。 注意 在混合集群中，当用户二进制目录，在不同主机类型上具有相同路径名时，LSF 效果最佳。使用相同的路径名，可使 PATH 变量在所有主机上均有效。 为了便于管理，LSF 配置文件存储在共享目录中。 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-11 10:11:23 "},"chapter3/section4/LSF_with_EGO_enabled.html":{"url":"chapter3/section4/LSF_with_EGO_enabled.html","title":"3.4 启用 EGO 的 LSF","keywords":"","body":"3.4 启用 EGO 的 LSF Enable the enterprise grid orchestrator (EGO) with LSF to provide a system infrastructure to control and manage cluster resources. Resources are physical and logical entities that are used by applications. LSF resources are shared as defined in the EGO resource distribution plan. EGO component overview EGO can be enabled with LSF to provide a system infrastructure to control and manage cluster resources. Resources Resources are physical and logical entities that are used by applications to run. While resource is a generic term, and can include low-level things such as shared memory segments or semaphores. In LSF, EGO manages CPU slots. How LSF shares resources through EGO LSF resources can be shared by defining an EGO resource distribution plan. LSF requests resources from the EGO resource manager. Based on the values specified in the resource distribution plan, the resource manager returns the number of available slots (m) and the names of the hosts on which the slots reside. © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-11 09:17:01 "},"chapter3/section4/EGO_component_overview.html":{"url":"chapter3/section4/EGO_component_overview.html","title":"EGO 组件概览","keywords":"","body":"EGO 组件概览 EGO can be enabled with LSF to provide a system infrastructure to control and manage cluster resources. Just as an operating system running on a single machine aggregates and virtualizes physical resources and allocates them to applications, EGO performs similar functions, but across a distributed environment. EGO manages both logical and physical resources and supports all forms of applications. EGO manages the supply of resources, making them available to applications. Hosts can be divided into two groups: management hosts and compute hosts. Management hosts provide specialized services to the cluster, while compute hosts run user workload. Management hosts Management hosts provide both cluster and workload management services within the cluster, and are not expected to run workload for users. The master host, all master candidate hosts, and session manager hosts must be management hosts. Other management hosts include the host running the data loaders and data purger for the reporting feature. Management hosts all run on the same operating system: all Windows, all UNIX, or all Linux. Master host The master host is the first host installed in the cluster. The resource manager (vemkd) for the cluster resides on this host. The master host controls the rest of the hosts in the cluster and is the interface to the clients of the cluster. Master candidates There is only one master host at a time. If the master host should fail, another host automatically takes over the master host role. Hosts that can act as the master are called master candidates. Session manager host One or more management hosts run session managers. There is one session manager per available slot on a management host. There is one session manager per application. Compute hosts Compute hosts are those hosts in the cluster that provide computing resources to consumers. A cluster may contain any number of compute hosts, but must have at least one compute host. CPU slots A CPU slot is the unit used to measure compute resources. A single CPU slot can run one service instance on a compute host, or one session manager on a management host. Daemons vemkd The VEM kernel daemon that runs on the master host. It starts other daemons and responds to allocation requests. egosc The v service controller requests appropriate resources from the vemkd daemon and controls service instances. pem Process execution manager works for the vemkd daemon, starting, controlling, and monitoring activities, as well as collecting and sending run time resource usage. © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-08 09:25:43 "},"chapter3/section4/resources.html":{"url":"chapter3/section4/resources.html","title":"资源","keywords":"","body":"资源 Resources are physical and logical entities that are used by applications to run. While resource is a generic term, and can include low-level things such as shared memory segments or semaphores. In LSF, EGO manages CPU slots. A resource of a particular type has attributes. For example, a compute host has the attributes of memory, CPU utilization, operating system type. Resource groups Resources can be grouped into logical groups to simplify identification, resource allocation, or for administration and monitoring purposes. These resource groups are used to provide a consumer with a like group of hosts to run workload. Any host in a resource group can be able to run the same workload. The following figure shows two resource groups: ManagementHosts ComputeHosts If all of your hosts are identical, these resource groups might suffice. If your application requires a specific type of hosts (for example, with a minimum processor speed), and not all hosts meet this criteria, you likely need to create resource groups to group like hosts together. For example, a simple way to group resources might be to group your hosts by operating system type. EGO provides a common grouping mechanism for resources. Resources might come and go from the system, so EGO supports dynamic membership in a resource group. Hosts can be placed explicitly into individual resource groups, or the resource groups can be defined with a dynamic membership based on specific criteria. This criteria includes operating system type, CPU speed, total memory, or swap configuration, or custom attributes. © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-08 09:25:55 "},"chapter3/section4/LSF_resource_sharing.html":{"url":"chapter3/section4/LSF_resource_sharing.html","title":"LSF 资源共享","keywords":"","body":"LSF 资源共享 可以通过定义 EGO 资源分配计划来共享 LSF 资源。 LSF 向 EGO 资源管理器请求资源。 根据资源分配计划中指定的值，资源管理器返回可用槽数（m）和该槽位所在的主机的名称。 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-10 10:47:01 "},"chapter4/administrator_fundations.html":{"url":"chapter4/administrator_fundations.html","title":"Chapter 4 管理员操作基础","keywords":"","body":"Chapter 4 管理员操作基础 本章内容是 IBM Spectrum LSF 的管理员概述，掌握本章，可以了解如何管理各种类型的工作负载和集群操作。 LSF 集群概览 概述您的集群以及重要的 LSF 目录和配置文件的位置。 使用 LSF 调度 启动和停止 LSF 守护程序，重新配置集群属性。 检查 LSF 状态并提交 LSF 作业。 解决 LSF 问题 解决常见的 LSF 问题并了解 LSF 错误信息。 如果在这里找不到解决问题的方法，请联系 IBM 支持。 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-09 12:01:57 "},"chapter4/section1/cluster_overview.html":{"url":"chapter4/section1/cluster_overview.html","title":"4.1 集群概览","keywords":"","body":"4.1 集群概览 概述您的集群以及重要的 LSF 目录和配置文件的位置。 LSF术语和概念 首次使用 LSF 之前，应先阅读 LSF Foundations Guide，以基本了解作业负载管理和作业提交，以及 Administrator Foundations Guide，以概述集群管理和操作。 集群特征 在安装后查询集群的名称，集群管理员以及定义主机的位置。 文件系统，目录，文件 LSF 设计用于所有主机都具有共享文件系统，且所有主机上文件都同名的网络。 重要目录和配置文件 通过多个配置文件管理 LSF 配置，您可以使用这些文件来修改集群的行为。 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-10 11:31:50 "},"chapter4/section1/terms_and_concepts.html":{"url":"chapter4/section1/terms_and_concepts.html","title":"术语与概念","keywords":"","body":"术语与概念 Before you use LSF for the first time, you should read the LSF Foundations Guide for a basic understanding of workload management and job submission, and the Administrator Foundations Guide for an overview of cluster management and operations. Job states IBM Spectrum LSF jobs have several states. PEND Waiting in a queue for scheduling and dispatch. RUN Dispatched to a host and running. DONE Finished normally with zero exit value. EXIT Finished with nonzero exit value. PSUSP Suspended while the job is pending. USUSP Suspended by user. SSUSP Suspended by the LSF system. POST_DONE Post-processing completed without errors. POST_ERR Post-processing completed with errors. UNKWN The mbatchd daemon lost contact with the sbatchd daemon on the host where the job runs. WAIT For jobs submitted to a chunk job queue, members of a chunk job that are waiting to run. ZOMBI A job becomes ZOMBI if the execution host is unreachable when a non-rerunnable job is killed or a rerunnable job is requeued. Host An LSF host is an individual computer in the cluster. Each host might have more than one processor. Multiprocessor hosts are used to run parallel jobs. A multiprocessor host with a single process queue is considered a single machine. A box full of processors that each have their own process queue is treated as a group of separate machines. Tip The names of your hosts should be unique. They cannot be the same as the cluster name or any queue that is defined for the cluster. Job An LSF job is a unit of work that runs in the LSF system. A job is a command that is submitted to LSF for execution, by using the bsub command. LSF schedules, controls, and tracks the job according to configured policies. Jobs can be complex problems, simulation scenarios, extensive calculations, anything that needs compute power. Job files When a job is submitted to a queue, LSF holds it in a job file until conditions are right for it run. Then, the job file is used to run the job. On UNIX, the job file is a Bourne shell script that is run at execution time. On Windows, the job file is a batch file that is processed at execution time. Interactive batch job An interactive batch job is a batch job that allows you to interact with the application and still take advantage of LSFscheduling policies and fault tolerance. All input and output are through the terminal that you used to type the job submission command. When you submit an interactive job, a message is displayed while the job is awaiting scheduling. A new job cannot be submitted until the interactive job is completed or terminated. Interactive task An interactive task is a command that is not submitted to a batch queue and scheduled by LSF, but is dispatched immediately. LSF locates the resources that are needed by the task and chooses the best host among the candidate hosts that has the required resources and is lightly loaded. Each command can be a single process, or it can be a group of cooperating processes. Tasks are run without using the batch processing features of LSF but still with the advantage of resource requirements and selection of the best host to run the task based on load. Local task A local task is an application or command that does not make sense to run remotely. For example, the ls command on UNIX. Remote task A remote task is an application or command that that can be run on another machine in the cluster. Host types and host models Hosts in LSF are characterized by host type and host model. The following example is a host with type X86_64, with host models Opteron240, Opteron840, Intel_EM64T, and so on. Host type An LSF host type is the combination of operating system and host CPU architecture. All computers that run the same operating system on the same computer architecture are of the same type. These hosts are binary-compatible with each other. Each host type usually requires a different set of LSF binary files. Host model An LSF host model is the host type of the computer, which determines the CPU speed scaling factor that is applied in load and placement calculations. The CPU factor is considered when jobs are being dispatched. Resources LSF resources are objects in the LSF system resources that LSF uses track job requirements and schedule jobs according to their availability on individual hosts. Resource usage The LSF system uses built-in and configured resources to track resource availability and usage. Jobs are scheduled according to the resources available on individual hosts. Jobs that are submitted through the LSF system will have the resources that they use monitored while they are running. This information is used to enforce resource limits and load thresholds as well as fairshare scheduling. LSF collects the following kinds of information: Total CPU time that is consumed by all processes in the job Total resident memory usage in KB of all currently running processes in a job Total virtual memory usage in KB of all currently running processes in a job Currently active process group ID in a job Currently active processes in a job On UNIX and Linux, job-level resource usage is collected through PIM. Load indices Load indices measure the availability of dynamic, non-shared resources on hosts in the cluster. Load indices that are built into the LIM are updated at fixed time intervals. External load indices Defined and configured by the LSF administrator and collected by an External Load Information Manager (ELIM) program. The ELIM also updates LIM when new values are received. Static resources Built-in resources that represent host information that does not change over time, such as the maximum RAM available to user processes or the number of processors in a machine. Most static resources are determined by the LIM at start-up time. Static resources can be used to select appropriate hosts for particular jobs that are based on binary architecture, relative CPU speed, and system configuration. Load thresholds Two types of load thresholds can be configured by your LSF administrator to schedule jobs in queues. Each load threshold specifies a load index value: The loadSched load threshold determines the load condition for dispatching pending jobs. If a host’s load is beyond any defined loadSched, a job cannot be started on the host. This threshold is also used as the condition for resuming suspended jobs. The loadStop load threshold determines when running jobs can be suspended. To schedule a job on a host, the load levels on that host must satisfy both the thresholds that are configured for that host and the thresholds for the queue from which the job is being dispatched. The value of a load index might either increase or decrease with load, depending on the meaning of the specific load index. Therefore, when you compare the host load conditions with the threshold values, you need to use either greater than (>) or less than ( Runtime resource usage limits Limit the use of resources while a job is running. Jobs that consume more than the specified amount of a resource are signaled. Hard and soft limits Resource limits that are specified at the queue level are hard limits while limits that are specified with job submission are soft limits. See the setrlimit man page for information about hard and soft limits. Resource allocation limits Restrict the amount of a resource that must be available during job scheduling for different classes of jobs to start, and which resource consumers the limits apply to. If all of the resource is consumed, no more jobs can be started until some of the resource is released. Resource requirements (bsub -R) The bsub -R option specifies resources requirements for the job. Resource requirements restrict which hosts the job can run on. Hosts that match the resource requirements are the candidate hosts. When LSF schedules a job, it collects the load index values of all the candidate hosts and compares them to the scheduling conditions. Jobs are only dispatched to a host if all load values are within the scheduling thresholds. © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-08 09:01:31 "},"chapter4/section1/cluster_characteristics.html":{"url":"chapter4/section1/cluster_characteristics.html","title":"集群特征","keywords":"","body":"集群特征 Find the name of your cluster after installation, cluster administrators, and where hosts are defined. Cluster name and administrators Your cluster is installed according to the installation options specified by the lsfinstall -f install.config command and the options you chose in the install.config file. The cluster name that you specified at installation is part of the name of the LSF_CONFDIR/lsf.cluster.cluster_name file. /usr/share/lsf/lsf_10/conf/lsf.cluster.lsf_10 Cluster administrators are listed in the ClusterAdmins section of the LSF_CONFDIR/lsf.cluster.cluster_namefile. LSF hosts Host types that are installed in your cluster are listed in the Hosts section of theLSF_CONFDIR/lsf.cluster.cluster_name file. The LSF master host is the first host that is configured in the Hosts section ofLSF_CONFDIR/lsf.cluster.cluster_name file. LSF server hosts that are defined in your cluster are indicated by 1 in the server column of the Hosts section in theLSF_CONFDIR/lsf.cluster.cluster_name file. LSF client-only hosts that are defined in your cluster are indicated by 0 in the server column of the Hosts section in theLSF_CONFDIR/lsf.cluster.cluster_name file. © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-10 11:32:51 "},"chapter4/section1/filesystems_directories_and_files.html":{"url":"chapter4/section1/filesystems_directories_and_files.html","title":"文件系统、目录和文件","keywords":"","body":"文件系统、目录和文件 LSF is designed for networks where all hosts have shared file systems, and files have the same names on all hosts. LSF includes support for copying user data to the execution host before a batch job runs, and for copying results back after the job runs. In networks where the file systems are not shared, this support can be used to give remote jobs access to local data. Supported file systems UNIX On UNIX systems, LSF supports the following shared file systems: Network File System (NFS) NFS file systems can be mounted permanently or on demand by using the automount command. Andrew File System (AFS) Supported on an on-demand basis under the parameters of the 9.1.2 integration with some published configuration parameters. Supports sequential and parallel user jobs that access AFS, JOB_SPOOL_DIR on AFS, and job output and error files on AFS. Distributed File System (DCE/DFS) Supported on an on-demand basis. Windows On Windows, directories that contain LSF files can be shared among hosts from a Windows server machine. Non-shared directories and files LSF is used in networks with shared file space. When shared file space is not available, LSF can copy needed files to the execution host before the job runs, and copy result files back to the submission host after the job completes. Some networks do not share files between hosts. LSF can still be used on these networks, with reduced fault tolerance. Example directory structures The following figures show typical directory structures for a new installation on UNIX and Linux or on Microsoft Windows. Depending on which products you installed and platforms you selected, your directory structure might be different. The following figures show typical directory structures for a new installation on UNIX and Linux or on Microsoft Windows. Depending on which products you installed and platforms you selected, your directory structure might be different. UNIX and Linux The following figure shows a typical directory structure for a new UNIX or Linux installation with the lsfinstall command.Microsoft Windows directory structure The following figure shows a typical directory structure for a new Windows installation. The following figure shows a typical directory structure for a new UNIX or Linux installation with the lsfinstall command. The following figure shows a typical directory structure for a new Windows installation. © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-10 11:36:10 "},"chapter4/section1/important_directories_and_configuration_files.html":{"url":"chapter4/section1/important_directories_and_configuration_files.html","title":"重要的文件目录与配置文件","keywords":"","body":"重要的文件目录与配置文件 LSF configuration is administered through several configuration files, which you use to modify the behavior of your cluster. Four important LSF configuration files The following are the four most important files you work with most often: LSF_CONFDIR/lsf.conf LSF_CONFDIR/lsf.cluster.cluster_name LSF_CONFDIR/lsf.shared LSB_CONFDIR/cluster_name/configdir/lsb.queues These files are created during product installation according to the options you specified in the install.config file. After installation, you can change the configuration parameters in these files to suit the needs of your site. Who owns these files Except for LSF_CONFDIR/lsf.conf, which is owned by root, all of these files are owned by the primary LSF administrator, and readable by all cluster users. lsf.conf The most important file in LSF. It contains the paths to the configuration directories, log directories, libraries, and other global configuration information. The location of the lsf.conf file is defined by the LSF_ENVDIR variable. If LSF cannot find this file, it cannot start properly.By default, LSF checks the directory that is defined by the LSF_ENVDIR parameter for the location of the lsf.conf file. If the lsf.conf file is not in LSF_ENVDIR, LSF looks for it in the /etc directory. lsf.cluster.cluster_name Defines the host names, models, and types of all of the hosts in the cluster. It also defines the user names of the LSF administrators, and the locations of different shared resources for one cluster. lsf.shared This file is like a dictionary that defines all the keywords that are used by the cluster. You can add your own keywords to specify the names of resources or host types. lsb.queues Defines the workload queues and their parameters for one cluster. LSF directories The following directories are owned by the primary LSF administrator and are readable by all cluster users: Directory Description Example LSF_CONFDIR LSF configuration directory /usr/share/lsf/cluster1/conf/ LSB_CONFDIR Batch system configuration directory /usr/share/lsf/cluster1/conf/lsbatch/ LSB_SHAREDIR Job history directory /usr/share/lsf/cluster1/work/ LSF_LOGDIR Server daemon error logs, one for each daemon /usr/share/lsf/cluster1/log/ The following directories are owned by root and are readable by all cluster users: Directory Description Example LSF_BINDIR LSF user commands, which are shared by all hosts of the same type /usr/share/lsf/cluster1/10.1/sparc-sol10/bin/ LSF_INCLUDEDIR Header files lsf/lsf.h and lsf/lsbatch.h /usr/share/lsf/cluster1/10.1/include/ LSF_LIBDIR LSF libraries, which are shared by all hosts of the same type /usr/share/lsf/cluster1/10.1/sparc-sol10/lib/ LSF_MANDIR LSF man pages /usr/share/lsf/cluster1/10.1/man/ LSF_MISC Examples and other miscellaneous files /usr/share/lsf/cluster1/10.1/misc/ LSF_SERVERDIR Server daemon binary files, scripts, and other utilities, which are shared by all hosts of the same type /usr/share/lsf/cluster1/10.1/sparc-sol10/etc/ LSF_TOP Top-level installation directory /usr/share/lsf/cluster1/ Other configuration directories can be specified in the LSF_CONFDIR/lsf.conf file. LSF cluster configuration files The following files are owned by the primary LSF administrator and are readable by all cluster users: File Example Global configuration files, which describe the configuration and operation of the cluster /usr/share/lsf/cluster1/conf/ego/cluster1/kernel/ego.conf/usr/share/lsf/cluster1/conf/lsf.conf Keyword definition file that is shared by all clusters. Defines cluster name, host types, host models, and site-specific resources /usr/share/lsf/cluster1/conf/lsf.shared Cluster configuration file that defines hosts, administrators, and location of site-defined shared resources /usr/share/lsf/cluster1/conf/lsf.cluster.cluster1 Mapping files for task names and their default resource requirements /usr/share/lsf/cluster1/conf/lsf.task/usr/share/lsf/cluster1/conf/lsf.task.cluster1 LSF batch workload system configuration files The following files are owned by the primary LSF administrator and are readable by all cluster users: File Example Server hosts and their attributes, such as scheduling load thresholds, dispatch windows, and job slot limits. If no hosts are defined in this file, then all LSF server hosts listed in LSF_CONFDIR/lsf.cluster.cluster_name are assumed to be LSF batch server hosts. /usr/share/lsf/cluster1/conf/lsbatch/cluster1/configdir/lsb.hosts LSF scheduler and resource broker plug-in modules. If no scheduler or resource broker modules are configured, LSF uses the default scheduler plug-in module named schmod_default. /usr/share/lsf/cluster1/conf/lsbatch/cluster1/configdir/lsb.modules LSF batch system parameter file /usr/share/lsf/cluster1/conf/lsbatch/cluster1/configdir/lsb.params Job queue definitions /usr/share/lsf/cluster1/conf/lsbatch/cluster1/configdir/lsb.queues Resource allocation limits, exports, and resource usage limits. /usr/share/lsf/cluster1/conf/lsbatch/cluster1/configdir/lsb.resources LSF user groups, hierarchical fairshare for users and user groups, and job slot limits for users and user groups. Also used to configure account mappings for the LSF multicluster capability. /usr/share/lsf/cluster1/conf/lsbatch/cluster1/configdir/lsb.users Application profiles, which contain common parameters for the same type of jobs, including the execution requirements of the applications, the resources they require, and how they are run and managed. This file is optional. Use the DEFAULT_APPLICATION parameter in the lsb.params file to specify a default application profile for all jobs. LSF does not automatically assign a default application profile. /usr/share/lsf/cluster1/conf/lsbatch/cluster1/configdir/lsb.applicatons LSF batch log files File Example Batch events log /usr/share/lsf/cluster1/work/ cluster1/logdir/lsb.events Batch accounting log /usr/share/lsf/cluster1/work/ cluster1/logdir/lsb.acct Daemon log files LSF server daemon log files are stored in the directory that is specified by LSF_LOGDIR in LSF_CONFDIR/lsf.conf. File Example Load information manager (lim) /usr/share/lsf/cluster1/log/lim.log.hosta Remote execution server (res) /usr/share/lsf/cluster1/log/res.log.hosta Master batch daemon (mbatchd) /usr/share/lsf/cluster1/log/ mbatchd.log.hosta Master scheduler daemon (mbschd) /usr/share/lsf/cluster1/log/mbschd.log.hosta Slave batch daemon (sbatchd) /usr/share/lsf/cluster1/log/sbatchd.log.hosta Process information manager (pim) /usr/share/lsf/cluster1/log/ pim.log.hosta Who owns and who should write to LSF_LOGDIR Note： Make sure that the primary LSF administrator owns the LSF log directory (LSF_LOGDIR parameter), and that root can write to this directory. If an LSF server cannot write to LSF_LOGDIR parameter, the error logs are created in /tmp. Where to go next Use your new IBM Spectrum LSF cluster, described in Work with LSF. © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-10 11:38:26 "},"chapter4/section2/work_with_LSF.html":{"url":"chapter4/section2/work_with_LSF.html","title":"4.2 使用 LSF","keywords":"","body":"4.2 使用 LSF Start and stop LSF daemons, and reconfigure cluster properties. Check LSF status and submit LSF jobs. Start, stop, and reconfigure LSF Use LSF administration commands lsadmin and badmin to start and stop LSF daemons, and reconfigure cluster properties. Check LSF status Use LSF administration commands to check cluster configuration, see cluster status, and LSF batch workload system configuration and status. Run LSF jobs Use the bsub and lsrun commands to run jobs through LSF. Use the bjobs command to see the status of your jobs. Control job execution with the bstop, bresume, and bkill commands. Manage users, hosts, and queues Make your cluster available to users with cshrc.lsf and profile.lsf. Add or remove hosts and queues from your cluster. Configure LSF startup Use the lsf.sudoers file so that LSF administrators can start and stop LSF daemons. Set up LSF to start automatically. Manage software licenses and other shared resources Set up an LSF external LIM (ELIM) to monitor software licenses as dynamic shared resources. © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-10 11:39:02 "},"chapter4/section2/subsection1/start_stop_and_reconfigure_LSF.html":{"url":"chapter4/section2/subsection1/start_stop_and_reconfigure_LSF.html","title":"开启、结束与重配置 LSF","keywords":"","body":"开启、结束与重配置 LSF Use LSF administration commands lsadmin and badmin to start and stop LSF daemons, and reconfigure cluster properties. Two LSF administration commands (lsadmin and badmin) ImportantOnly LSF administrators or root can run these commands. To start and stop LSF, and to reconfigure LSF after you change any configuration file, use the following commands: The lsadmin command controls the operation of the lim and res daemons. The badmin command controls the operation of the mbatchd and sbatchd daemons. If you installed LSF as a non-root user By default, only root can start LSF daemons. If the lsfinstall command detected that you installed as non-root user, you chose to configure either a multi-user cluster or a single-user cluster: Multi-user configuration Only root can start LSF daemons. Any user can submit jobs to your cluster.For information about changing ownership and permissions for the lsadmin and badmincommands, see Troubleshooting LSF problems.To permit LSF administrators to start and stop LSF daemons, set up the /etc/lsf.sudoers file, as described in Configure LSF Startup. Single-user Your user account must be primary LSF administrator. You are able to start LSF daemons, but only your user account can submit jobs to the cluster. Your user account must be able to read the system kernel information, such as /dev/kmem. Setting up the LSF environment with cshrc.lsf and profile.lsf Before you use LSF, you must set up the LSF execution environment with the cshrc.lsf or profile.lsf file.Starting your cluster Use the lsadmin and badmin commands to start the LSF daemons.Stopping your cluster Use the lsadmin and badmin commands to stop the LSF daemons.Reconfiguring your cluster with lsadmin and badmin Use the lsadmin and badmin commands to reconfigure LSF after you change any configuration file. © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-06 21:29:02 "},"chapter4/section2/subsection1/Setting up the LSF environment.html":{"url":"chapter4/section2/subsection1/Setting up the LSF environment.html","title":"Setting up the LSF environment","keywords":"","body":"Setting up the LSF environment Before you use LSF, you must set up the LSF execution environment with the cshrc.lsf or profile.lsf file. Procedure After you log in to an LSF host, use one of the following shell environment files to set your LSF environment. In the csh or tcsh shell, run the source command: % source /conf/cshrc.lsf In the sh , ksh , or bash shell run the following command: $ . /conf/profile.lsf The files cshrc.lsf and profile.lsf are created during installation by the lsfinstall command to set up the LSF operating environment. © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-06 21:29:36 "},"chapter4/section2/subsection1/Starting your cluster.html":{"url":"chapter4/section2/subsection1/Starting your cluster.html","title":"Starting your cluster","keywords":"","body":"Starting your cluster Use the lsadmin and badmin commands to start the LSF daemons. Procedure Log in as root to each LSF server host. If you installed a single-user cluster as a non-root user, log in as primary LSF administrator. Start with the LSF master host, and repeat these steps on all LSF hosts. Use the following commands to start the LSF cluster: # lsadmin limstartup # lsadmin resstartup # badmin hstartup Before you use any LSF commands, wait a few minutes for the lim daemon all hosts to do the following operations: Contact each other Select the master host Exchange initialization information © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-06 21:29:47 "},"chapter4/section2/subsection1/Stopping your cluster.html":{"url":"chapter4/section2/subsection1/Stopping your cluster.html","title":"Stopping your cluster","keywords":"","body":"Stopping your cluster Use the lsadmin and badmin commands to stop the LSF daemons. Procedure Log in as root to each LSF server host. If you installed a single-user cluster as a non-root user, log in as primary LSF administrator. Use the following commands to stop the LSF cluster: # badmin hshutdown all # lsadmin resshutdown all # lsadmin limshutdown all © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-06 21:29:57 "},"chapter4/section2/subsection1/Reconfiguring your cluster.html":{"url":"chapter4/section2/subsection1/Reconfiguring your cluster.html","title":"Reconfiguring your cluster","keywords":"","body":"Reconfiguring your cluster Use the lsadmin and badmin commands to reconfigure LSF after you change any configuration file. Procedure Log in as root to each LSF server host. If you installed a single-user cluster as a non-root user, log in as primary LSF administrator. Use the following commands to reconfigure the LSF cluster: Reload modified LSF configuration files and restart lim : ``` # lsadmin reconfig ``` Reload modified LSF batch configuration files: ``` # badmin reconfig ``` Reload modified LSF batch configuration files and restart mbatchd : ``` # badmin mbdrestart ``` This command also reads the LSF_LOGDIR/lsb.events file, so it can take some time to complete if a lot of jobs are running. © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-06 21:30:08 "},"chapter4/section2/subsection2/check_LSF_status.html":{"url":"chapter4/section2/subsection2/check_LSF_status.html","title":"检查 LSF 状态","keywords":"","body":"检查 LSF 状态 Use LSF administration commands to check cluster configuration, see cluster status, and LSF batch workload system configuration and status. Example command output The LSF commands that are shown in this section show examples of typical output. The output that you see might differ according to your configuration. The commands are described briefly so that you can easily use them to verify your LSF installation. See the LSF Command Reference or the LSF man pages for complete usage and command options. You can use these commands on any LSF host. If you get proper output from these commands, your cluster is ready to use. If your output has errors, see Troubleshooting LSF problems for help. Check cluster configuration with the lsadmin command The lsadmin command controls the operation of an LSF cluster and administers the LSF daemons lim and res.Check cluster status with the lsid and lsload commands The lsid command tells you if your LSF environment is set up properly. The lsload command displays the current load levels of the cluster.Check LSF batch system configuration with badmin The badmin command controls and monitors the operation of the LSF batch workload system.Find out batch system status with bhosts and bqueues Use the bhosts command to see whether the LSF batch workload system is running properly. The bqueues command displays the status of available queues and their configuration parameters. © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-06 21:26:21 "},"chapter4/section2/subsection2/Check cluster configuration.html":{"url":"chapter4/section2/subsection2/Check cluster configuration.html","title":"Check cluster configuration","keywords":"","body":"Check cluster configuration The lsadmin command controls the operation of an LSF cluster and administers the LSF daemons lim and res. Use the lsadmin ckconfig command to check the LSF configuration files. The -v option displays detailed information about the LSF configuration: The messages that are shown in the following output are typical of lsadmin ckconfig -v. Other messages might indicate problems with your LSF configuration. % lsadmin ckconfig -v Checking configuration files ... EGO 3.6.0 build 800000, Jul 25 2017 Copyright International Business Machines Corp. 1992, 2016. US Government Users Restricted Rights - Use, duplication or disclosure restricted by GSA ADP Schedule Contract with IBM Corp. binary type: linux2.6-glibc2.3-x86_64 Reading configuration from /opt/lsf/conf/lsf.conf Aug 3 13:45:27 2017 20884 6 3.6.0 Lim starting... Aug 3 13:45:27 2017 20884 6 3.6.0 LIM is running in advanced workload execution mode. Aug 3 13:45:27 2017 20884 6 3.6.0 Master LIM is not running in EGO_DISABLE_UNRESOLVABLE_HOST mode. Aug 3 13:45:27 2017 20884 5 3.6.0 /opt/lsf/10.1/linux2.6-glibc2.3-x86_64/etc/lim -C Aug 3 13:45:27 2017 20884 7 3.6.0 Could not construct product entitlement version array Aug 3 13:45:27 2017 20884 Last message repeated 1 time(s). Aug 3 13:45:27 2017 20884 6 3.6.0 initEntitlement: EGO_AUDIT_MAX_SIZE was not set. Default value will be used. Aug 3 13:45:27 2017 20884 6 3.6.0 initEntitlement: EGO_AUDIT_MAX_ROTATE was not set. Default value will be used. Aug 3 13:45:27 2017 20884 6 3.6.0 LIM is running as IBM Spectrum LSF Standard Edition. Aug 3 13:45:27 2017 20884 6 3.6.0 reCheckClass: numhosts 1 so reset exchIntvl to 15.00 Aug 3 13:45:27 2017 20884 6 3.6.0 Checking Done. --------------------------------------------------------- No errors found. See Troubleshooting LSF problems or the LSF Command Reference for help with some common configuration errors. © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-06 21:30:40 "},"chapter4/section2/subsection2/Check cluster status.html":{"url":"chapter4/section2/subsection2/Check cluster status.html","title":"Check cluster status","keywords":"","body":"Check cluster status The lsid command tells you if your LSF environment is set up properly. The lsload command displays the current load levels of the cluster. lsid command The lsid command displays the current LSF version number, cluster name, and host name of the current LSF master host for your cluster. The LSF master name that is displayed by the lsid command can vary, but it is usually the first host that is configured in the Hosts section of the LSF_CONFDIR/lsf.cluster.cluster_name file. % lsid IBM Spectrum LSF Standard 10.1.0.0, Apr 04 2016 Copyright International Business Machines Corp, 1992-2016. US Government Users Restricted Rights - Use, duplication or disclosure restricted by GSA ADP Schedule Contract with IBM Corp. My cluster name is cluster1 My master name is hosta If you see the message Cannot open lsf.conf file The LSF_ENVDIR environment variable is probably not set correctly. Use the cshrc.lsf or profile.lsf file to set up your environment. See Troubleshooting LSF problems for more help lsload command The output of the lsload command contains one line for each host in the cluster. Normal status is ok for all hosts in your cluster. % lsload HOST_NAME status r15s r1m r15m ut pg ls it tmp swp mem hosta ok 0.0 0.0 0.1 1% 0.0 1 224 43G 67G 3G hostc -ok 0.0 0.0 0.0 3% 0.0 3 0 38G 40G 7G hostf busy *6.2 6.9 9.5 85% 1.1 30 0 5G 400G 385G hosth busy 0.1 0.1 0.3 7% *17 6 0 9G 23G 28G hostv unavail A busy status is shown for hosts with any load index beyond their configured thresholds. An asterisk (*) marks load indexes that are beyond their thresholds, causing the host status to be busy. A minus sign (-) in front of the value ok means that res is not running on that host. If you see one of the following messages after you start or reconfigure LSF, wait a few seconds and try the lsload command again to give the lim daemon on all hosts time to initialize. lsid: getentitlementinfo() failed: LIM is down; try later or LSF daemon (LIM) not responding ... still trying If the problem persists, see Troubleshooting LSF problems for help. Other useful commands The bparams command displays information about the LSF batch system configuration parameters. The bhist command displays historical information about jobs. © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-06 21:30:52 "},"chapter4/section2/subsection2/Check LSF batch system configuration.html":{"url":"chapter4/section2/subsection2/Check LSF batch system configuration.html","title":"Check LSF batch system configuration","keywords":"","body":"Check LSF batch system configuration The badmin command controls and monitors the operation of the LSF batch workload system. Use the badmin ckconfig command to check the LSF batch system configuration files. The -v option displays detailed information about the configuration: The messages in the following output are typical of badmin ckconfig -v. Other messages might indicate problems with your LSF batch workload system configuration. % badmin ckconfig -v Checking configuration files ... Dec 20 12:22:55 2015 20246 9 9.1.3 minit: Trying to call LIM to get cluster name ... Dec 20 12:22:55 2015 20246 9 9.1.3 Batch is enabled Dec 20 12:22:55 2015 4433 9 9.1.3 Checking Done --------------------------------------------------------- No errors found. See Troubleshooting LSF problems or the LSF Command Reference for help with some common configuration errors. © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-06 21:31:05 "},"chapter4/section2/subsection2/Find out batch system status.html":{"url":"chapter4/section2/subsection2/Find out batch system status.html","title":"Find out batch system status","keywords":"","body":"Find out batch system status Use the bhosts command to see whether the LSF batch workload system is running properly. The bqueues command displays the status of available queues and their configuration parameters. To use LSF batch commands, the cluster must be up and running. See Starting your cluster for information about starting LSF daemons. bhosts command The bhosts command displays the status of LSF batch server hosts in the cluster, and other details about the batch hosts: Maximum number of job slots that are allowed by a single user Total number of jobs in the system, running jobs, jobs that are suspended by users, and jobs that are suspended by the system Total number of reserved job slots Normal status ok for all hosts in your cluster. % bhosts HOST_NAME STATUS JL/U MAX NJOBS RUN SSUSP USUSP RSV hosta ok - - 0 0 0 0 0 hostb ok - - 0 0 0 0 0 hostc ok - - 0 0 0 0 0 hostd ok - - 0 0 0 0 0 If you see the following message when you start or reconfigure LSF, wait a few seconds and try the bhosts command again to give the mbatchd daemon time to initialize. batch system daemon not responding ... still trying If the problem persists, see Solving common LSF problems for help. bqueues command LSF queues organize jobs with different priorities and different scheduling policies. The bqueues command displays the status of available queues and their configuration parameters. For a queue to accept and dispatch jobs, the status must be Open:Active. % bqueues QUEUE_NAME PRIO STATUS MAX JL/U JL/P JL/H NJOBS PEND RUN SUSP owners 43 Open:Active - - - - 0 0 0 0 priority 43 Open:Active - - - - 0 0 0 0 night 40 Open:Inact - - - - 0 0 0 0 chkpnt_rerun_qu 40 Open:Active - - - - 0 0 0 0 short 35 Open:Active - - - - 0 0 0 0 license 33 Open:Active - - - - 0 0 0 0 normal 30 Open:Active - - - - 0 0 0 0 idle 20 Open:Active - - - - 0 0 0 0 To see more detailed queue information, use the bqueues -l command: % bqueues -l normal QUEUE: normal -- For normal low priority jobs, running only if hosts are lightly loaded. This is the default queue. PARAMETERS/STATISTICS PRIO NICE STATUS MAX JL/U JL/P JL/H NJOBS PEND RUN SSUSP USUSP RSV 30 20 Open:Active - - - - 0 0 0 0 0 0 Interval for a host to accept two jobs is 0 seconds SCHEDULING PARAMETERS r15s r1m r15m ut pg io ls it tmp swp mem loadSched - - - - - - - - - - - loadStop - - - - - - - - - - - SCHEDULING POLICIES: FAIRSHARE NO_INTERACTIVE USER_SHARES: [default, 1] USERS: all HOSTS: all The bqueues -l command shows the following kinds of information about the queue: What kinds of jobs are meant to run on the queue Resource usage limits Hosts and users able to use the queue Scheduling threshold values: loadSched is the threshold for LSF to stop dispatching jobs automatically loadStop is the threshold for LSF to suspend a job automatically Other useful commands The bparams command displays information about the LSF batch system configuration parameters. The bhist command displays historical information about jobs. © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-06 21:31:17 "},"chapter4/section2/subsection3/run_jobs.html":{"url":"chapter4/section2/subsection3/run_jobs.html","title":"运行作业","keywords":"","body":"运行作业 Use the bsub and lsrun commands to run jobs through LSF. Use the bjobs command to see the status of your jobs. Control job execution with the bstop, bresume, and bkill commands. Run LSF jobs with bsub and lsrun Use two basic commands to run jobs through LSF: bsub submits jobs to the LSF batch scheduler. LSF schedules and dispatches jobs to the best available host based on the scheduling policies you configure in your LSF queues. The lsrun command runs an interactive task on the best available host, based on current system load information gathered by the lim daemon. For most jobs, all you need to do is add either the lsrun or bsub command in front of the job commands you normally use. You usually don't need to modify your executable applications or execution scripts. Submit batch jobs with bsub The bsub command submits jobs to LSF batch scheduling queues.Display job status with bjobs Use the bjobs command to see the job ID and other information about your jobs.Control job execution with bstop, bresume, and bkill Use LSF commands to suspend (bstop), resume (bresume), and kill (bkill) jobs.Run interactive tasks with lsrun and lsgrun The lsrun command runs a task on either the current local host or remotely on the best available host, provided it can find the necessary resources and the appropriate host type. The lsgrun command is similar to lsrun, but it runs a task on a group of hosts.Integrate your applications with LSF By integrating your applications with LSF, you can make sure that your users can submit and run their jobs with correct and complete job submission options without making them learn LSF commands. © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-06 21:31:39 "},"chapter4/section2/subsection3/Submit batch jobs.html":{"url":"chapter4/section2/subsection3/Submit batch jobs.html","title":"Submit batch jobs","keywords":"","body":"Submit batch jobs The bsub command submits jobs to LSF batch scheduling queues. The following command submits a sleep job to the default queue (normal): % bsub sleep 60 Job is submitted to default queue . When a job is submitted to LSF, it is assigned a unique job ID, in this case 3616. You can specify a wide range of job options on the bsub command. For example, you can specify a queue, and the job command sleep 60 is the last option: % bsub -q short sleep 60 Job is submitted to queue . What LSF does with job output By default, when the job is finished, LSF sends email with a job report and any output and error messages to the user account from which the job was submitted. You can optionally save standard output and standard error to files with the -o and -e options. The following command appends the standard output and standard error of the job to the files output.3640 and errors.3640 in the jobs subdirectory of the home directory of user1. % bsub -q short -o /home/user1/job/output.%J -e /home/user1/job/errors.%J ls -l Job is submitted to queue . The %J variable is replaced by the job ID when the files are created. Using %J helps you find job output when you run a lot of jobs. Interactive batch jobs with bsub -I To submit an interactive job through LSF, use the -I option: The following command submits a batch interactive job that displays the output of the ls command: % bsub -I ls To submit a batch interactive job by using a pseudo-terminal, use the bsub -Ip option. To submit a batch interactive job and create a pseudo-terminal with shell mode support, use the bsub -Is option. © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-06 21:31:55 "},"chapter4/section2/subsection3/Display job status.html":{"url":"chapter4/section2/subsection3/Display job status.html","title":"Display job status","keywords":"","body":"Display job status Use the bjobs command to see the job ID and other information about your jobs. The status of each LSF job is updated periodically. % bjobs JOBID USER STAT QUEUE FROM_HOST EXEC_HOST JOB_NAME SUBMIT_TIME 1266 user1 RUN normal hosta hostb sleep 60 Jun 5 17:39:58 The job that is named sleep 60 runs for 60 seconds. When the job completes, LSF sends email to report the job completion. You can use the job ID to monitor the status of a specific job. If all hosts are busy, the job is not started immediately and the STAT column says PEND. © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-06 21:32:05 "},"chapter4/section2/subsection3/Control job execution.html":{"url":"chapter4/section2/subsection3/Control job execution.html","title":"Control job execution","keywords":"","body":"Control job execution Use LSF commands to suspend (bstop), resume (bresume), and kill (bkill) jobs. bstop command To suspend a running job, use the bstop command and specify the job ID: % bstop 1266 Job is being stopped If the job was running when it was stopped, the bjobs command shows USUSP status for job 1266: % bjobs JOBID USER STAT QUEUE FROM_HOST EXEC_HOST JOB_NAME SUBMIT_TIME 1266 user1 USUSP normal hosta hostb sleep 60 Jun 5 17:39:58 Job owners can suspend only their own jobs. LSF administrators can suspend any job. bresume command To resume a suspended job, use the bresume command. % bresume 1266 Job is being resumed If the job resumes immediately, the bjobs command shows RUN status for job 1266: % bjobs JOBID USER STAT QUEUE FROM_HOST EXEC_HOST JOB_NAME SUBMIT_TIME 1266 user1 RUN normal hosta hostb sleep 60 Jun 5 17:39:58 Job owners can resume only their own jobs. LSF administrators can resume any job. bkill command To kill a job, use the bkill command, which sends a signal to the specified jobs. For example, if the job owner or the LSF administrator runs the following command, job 1266 is killed: % bkill 1266 Job is being terminated © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-06 21:32:19 "},"chapter4/section2/subsection3/Run interactive tasks.html":{"url":"chapter4/section2/subsection3/Run interactive tasks.html","title":"Run interactive tasks","keywords":"","body":"Run interactive tasks The lsrun command runs a task on either the current local host or remotely on the best available host, provided it can find the necessary resources and the appropriate host type. The lsgrun command is similar to lsrun, but it runs a task on a group of hosts. The following command runs the ls command. In this case, the command ran through LSF on the local host: % lsrun ls -l /usr/share/lsf/cluster1/conf/ total 742 -rw-r--r-- 1 root lsf 11372 Jul 16 16:23 cshrc.lsf -rw-r--r-- 1 root lsf 365 Oct 25 10:55 hosts drwxr-xr-x 3 lsfadmin lsf 512 Jul 16 15:53 lsbatch -rw-r--r-- 1 lsfadmin lsf 1776 Nov 23 15:13 lsf.conf -rw-r--r-- 1 lsfadmin lsf 8453 Nov 16 17:46 lsf.shared -rw-r--r-- 1 lsfadmin lsf 578 Jul 16 15:53 lsf.task -rw-r--r-- 1 root lsf 10485 Jul 16 17:08 profile.lsf You can also specify a host where you want to run a command. For example, the following command runs the hostname command on the remote host hosta: % lsrun -v -m hosta hostname > hosta The following command runs the hostname command on three remote hosts: % lsgrun -v -m \"hosta hostb hostc\" hostname > hosta > hostb > hostc © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-06 21:32:30 "},"chapter4/section2/subsection3/Integrate your applications with LSF.html":{"url":"chapter4/section2/subsection3/Integrate your applications with LSF.html","title":"Integrate your applications with LSF","keywords":"","body":"Integrate your applications with LSF By integrating your applications with LSF, you can make sure that your users can submit and run their jobs with correct and complete job submission options without making them learn LSF commands. Integrate applications with LSF three ways: Wrapper shell scripts Wrapper binary executables Modifying existing application source code and interfaces Wrapper shell scripts The easiest integration method is to put the bsub command into an executable file like a shell script. A wrapper script is an executable file for launching your application through LSF. It gives users a simple interface to run their jobs that is easy to deploy and maintain. For example, if your application is called abc, rename abc to abc_real and create a wrapper script that is called abc: #! /bin/sh bsub -R \"rusage[abc_license=1:duration=1]\" abc_real When users run abc, they are actually running a script to submit a job abc_real to LSF that uses 1 shared resource named abc_license. For more information about specifying shared resources by using the resource requirement (rusage) string on the -R option of the bsub command, see Manage software licenses and other shared resources. By adding appropriate options to the script, you can enhance your integration: Requeue jobs based on license availability Copy input and output files to and from the local directory on the execution host Calculate and estimate resource requirements Wrapper binary programs A wrapper binary is similar to a wrapper shell script in the form of a compiled binary executable. Compiled wrapper files usually run faster and more efficiently than shell scripts, and they also have access to the LSF API (LSLIB and LSBLIB). Binary code is also more secure because users cannot modify it without the source code and appropriate libraries, but it is more time consuming to develop wrapper binary programs than wrapper shell scripts. Modifying existing application source code and interfaces LSF is already integrated closely with many commonly used software products. IBM and other software application vendors provide facilities and services for closer integration of LSF and other applications. By modifying existing application user interfaces, you can enable easy job submission, license maximization, parallel execution, and other advanced LSF features. In some cases, you are able to run an LSF job directly from the application user interface. Where to go next Learn more about administering your cluster, described in Manage users, hosts, and queues. © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-06 21:32:42 "},"chapter4/section2/subsection4/manage_users_hosts_and_queues.html":{"url":"chapter4/section2/subsection4/manage_users_hosts_and_queues.html","title":"管理用户、节点与队列","keywords":"","body":"管理用户、节点与队列 Make your cluster available to users with cshrc.lsf and profile.lsf. Add or remove hosts and queues from your cluster. Making your cluster available to users with cshrc.lsf and profile.lsf Make sure that all LSF users include either the cshrc.lsf or profile.lsf file at the end of their own .cshrc or .profile file, or run one of these two files before you use LSF.Adding a host to your cluster Use the LSF installation script lsfinstall to add new hosts and host types to your cluster.Removing a host from your cluster Removing a host from LSF involves closing a host to prevent any additional jobs from running on the host and removing references to the host from the lsf.cluster.cluster_name file and other configuration files.Adding a queue Edit the lsb.queues file to add the new queue definition. Adding a queue does not affect pending or running jobs.Removing a queue Edit lsb.queues to remove a queue definition. © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-06 21:33:04 "},"chapter4/section2/subsection4/Making your cluster available to users.html":{"url":"chapter4/section2/subsection4/Making your cluster available to users.html","title":"Making your cluster available to users","keywords":"","body":"Making your cluster available to users Make sure that all LSF users include either the cshrc.lsf or profile.lsf file at the end of their own .cshrc or .profile file, or run one of these two files before you use LSF. About this task To set up the LSF environment for your users, use the following two shell files: LSF_CONFDIR/cshrc.lsf Use this file for csh or tcsh shell. LSF_CONFDIR/profile.lsf Use this file for sh, ksh, or bash shell. Procedure For csh or tcsh shell: Add the cshrc.lsf file to the end of the .cshrc file for all users: Copy the contents of the cshrc.lsf file into the .cshrc file. Add a line with the source command to the end of the .cshrc file: For example, if your the LSF_TOP directory for your cluster is /usr/share/lsf/conf, add the following line to the .cshrc file: ``` source /usr/share/lsf/conf/cshrc.lsf ``` For sh, ksh, or bash shell: Add the profile.lsf file to the end of the .profile file for all users: Copy the contents of the profile.lsf file into the .profile file. For example, if your the LSF_TOP directory for your cluster is /usr/share/lsf/conf , add a line similar to the following to the end of the .profile file: ``` . /usr/share/lsf/conf/profile.lsf ``` © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-06 21:33:30 "},"chapter4/section2/subsection4/Adding a host to your cluster.html":{"url":"chapter4/section2/subsection4/Adding a host to your cluster.html","title":"Adding a host to your cluster","keywords":"","body":"Adding a host to your cluster Use the LSF installation script lsfinstall to add new hosts and host types to your cluster. Before you begin Make sure that you have the LSF distribution files for the host types you want to add. For example, to add a Linux system that runs x86-64 Kernel 2.6 and 3.x to your cluster, get the file lsf10.1_linux2.6-glibc2.3-x86_64.tar.Z. Distribution packages for all supported LSF releases are available for download through IBM Passport Advantage. See LSF System Requirements on IBM developerWorks for a complete list of supported operating systems. The following videos provide more help about downloading LSF through IBM Passport Advantage: YouTube IBM Education Assistant About this task Adding a host to your cluster has the following major steps: Install LSF binary files for the host type. Add host information to the lsf.cluster.cluster_name file. Set up the new host. Procedure Install the binary files for a new host type. Use the lsfinstall command to add new host types to your cluster. If you already have the distribution files for the host types you want to add, you can skip these steps. Log on as root to any host that can access the LSF installation script directory. Change to the installation script directory. # cd /usr/share/lsf/cluster1/10.1/install Edit the install.config file to specify the options you want for new host types. For more information about the install.config file, see the IBM Spectrum LSF Configuration Reference. For information about the lsfinstall command, see Installing IBM Spectrum LSF on UNIX and Linux and the IBM Spectrum LSF Command Reference. Run the ./lsfinstall -f install.config command. Follow the steps for host setup in After Installing LSF in Installing IBM Spectrum LSF on UNIX and Linux (or in the lsf_getting_started.html file that is generated by the lsfinstall script) to set up the new hosts. Add host information to the lsf.cluster.cluster_name file. Log on to the LSF master host as the primary LSF administrator. Edit the LSF_CONFDIR/lsf.cluster.cluster_name file, and add host information for the new host to the Host section. Add the name of the host. Add model or type. If you enter the ! keyword in the model and type columns, the host model is automatically detected by lim running on the host. You might want to use the default values for that host type now, and change them later on when you have more experience or more information. Specify LSF server or client in the server column: - 1 (one) indicates an LSF server host. - 0 (zero) indicates an LSF client-only host. By default, all hosts are considered LSF server hosts. ``` HOSTNAME model type server r1m mem RESOURCES REXPRI hosta ! SUNSOL 1 1.0 4 () 0 hostb ! LINUX 0 1.0 4 () 0 hostc ! HPPA 1 1.0 4 () 0 End Host ``` Save the changes to LSF_CONFDIR/lsf.cluster.cluster_name. Reconfigure lim to enable the new host in the cluster. % lsadmin reconfig Checking configuration files ... No errors found. Do you really want to restart LIMs on all hosts? [y/n] y Restart LIM on ...... done Restart LIM on ...... done Restart LIM on ...... done The lsadmin reconfig command checks for configuration errors. If no unrecoverable errors are found, you are asked to confirm that you want to restart lim on all hosts and lim is reconfigured. If unrecoverable errors are found, reconfiguration exits. Reconfigure mbatchd. % badmin reconfig Checking configuration files ... No errors found. Do you want to reconfigure? [y/n] y Reconfiguration initiated The badmin reconfig command checks for configuration errors. If no unrecoverable errors are found, you are asked to confirm reconfiguration. If unrecoverable errors are found, reconfiguration exits. (Optional) Use the hostsetup command to set up the new host. Log on as root to any host that can access the LSF installation script directory. Change to the installation script directory. # cd /usr/share/lsf/cluster1/10.1/install Run the hostsetup command to set up the new host. # ./hostsetup --top=\"/usr/share/lsf/lsf_62\" --boot=\"y\" For information about the hostsetup command, see Installing IBM Spectrum LSF on UNIX and Linux and the IBM Spectrum LSF Command Reference. Start LSF on the new host. Run the following commands: # lsadmin limstartup # lsadmin resstartup # badmin hstartup Run the bhosts and lshosts commands to verify your changes. If any host type or host model is UNKNOWN or DEFAULT, see Working with hosts in IBM Spectrum LSF Cluster Management and Operations to fix the problem. Results Use dynamic host configuration to add hosts to the cluster without manually changing the LSF configuration. For more information about adding hosts dynamically, see IBM Spectrum LSF Cluster Management and Operations. If you get errors, see Troubleshooting LSF problems for help with some common configuration errors. © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-06 21:33:58 "},"chapter4/section2/subsection4/Removing a host from your cluster.html":{"url":"chapter4/section2/subsection4/Removing a host from your cluster.html","title":"Removing a host from your cluster","keywords":"","body":"Removing a host from your cluster Removing a host from LSF involves closing a host to prevent any additional jobs from running on the host and removing references to the host from the lsf.cluster.cluster_name file and other configuration files. About this task CAUTION Never remove the master host from LSF. If you want to change your current default master host, change the lsf.cluster.cluster_name file to assign a different default master host. Then remove the host that was formerly the master host. Procedure Log on to the LSF host as root. Run badmin hclose to close the host. Closing the host prevents jobs from being dispatched to the host and allows running jobs to finish. Stop all running daemons manually. Remove any references to the host in the Host section of the LSF_CONFDIR/lsf.cluster.cluster_name file. Remove any other references to the host, if applicable, from the following configuration files: LSF_CONFDIR/lsf.shared LSB_CONFDIR/cluster_name/configdir/lsb.hosts LSB_CONFDIR/cluster_name/configdir/lsb.queues LSB_CONFDIR/cluster_name/configdir/lsb.resources Log off the host to be removed, and log on as root or the primary LSF administrator to any other host in the cluster. Run the lsadmin reconfig command to reconfigure LIM. % lsadmin reconfig Checking configuration files ... No errors found. Do you really want to restart LIMs on all hosts? [y/n] y Restart LIM on ...... done Restart LIM on ...... done The lsadmin reconfig command checks for configuration errors. If no errors are found, you are asked to confirm that you want to restart lim on all hosts and lim is reconfigured. If unrecoverable errors are found, reconfiguration exits. Run the badmin mbdrestart command to restart mbatchd. % badmin reconfig Checking configuration files ... No errors found. Do you want to reconfigure? [y/n] y Reconfiguration initiated The badmin mbdrestart command checks for configuration errors. If no unrecoverable errors are found, you are asked to confirm reconfiguration. If unrecoverable errors are found, reconfiguration exits. If you configured LSF daemons to start automatically at system startup, remove the LSF section from the host’s system startup files. For more information about automatic LSF daemon startup, see Setting up automatic LSF startup If any users of the host use the lstcsh shell as their login shell, change their login shell to tcsh or csh. Remove lstcsh from the /etc/shells file. Results Use dynamic host configuration to remove hosts to the cluster without manually changing the LSF configuration. For more information about removing hosts dynamically, see IBM Platform LSF Cluster Management and Operations. If you get errors, see ../lsf_admin/chap_troubleshooting_lsf.html#v3523448 for help with some common configuration errors. © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-06 21:34:14 "},"chapter4/section2/subsection4/Adding a queue.html":{"url":"chapter4/section2/subsection4/Adding a queue.html","title":"Adding a queue","keywords":"","body":"Adding a queue Edit the lsb.queues file to add the new queue definition. Adding a queue does not affect pending or running jobs. Procedure Log in as the administrator on any host in the cluster. Edit the LSB_CONFDIR/cluster_name/configdir/lsb.queues file to add the new queue definition. You can copy another queue definition from this file as a starting point. Remember to change the QUEUE_NAME parameter of the copied queue. Save the changes to the lsb.queues file. When the configuration files are ready, run the badmin ckconfig command to check the new queue definition. If any errors are reported, fix the problem and check the configuration again. Run the badmin reconfig command to reconfigure the cluster. % badmin reconfig Checking configuration files ... No errors found. Do you want to reconfigure? [y/n] y Reconfiguration initiated The badmin reconfig command also checks for configuration errors. If no unrecoverable errors are found, you are asked to confirm reconfiguration. If unrecoverable errors are found, reconfiguration exits. Results If you get errors, see Troubleshooting LSF problems for help with some common configuration errors. For more information about the lsb.queues file, see the Configuration Reference. For more information about the badmin reconfig command, see the Command Reference. Example Begin Queue QUEUE_NAME = normal PRIORITY = 30 STACKLIMIT= 2048 DESCRIPTION = For normal low priority jobs, running only if hosts are lightly loaded. QJOB_LIMIT = 60 # job limit of the queue PJOB_LIMIT = 2 # job limit per processor ut = 0.2 io = 50/240 USERS = all HOSTS = all NICE = 20 End Queue © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-06 21:34:28 "},"chapter4/section2/subsection4/Removing a queue.html":{"url":"chapter4/section2/subsection4/Removing a queue.html","title":"Removing a queue","keywords":"","body":"Removing a queue Edit lsb.queues to remove a queue definition. Before you begin Important Before you remove a queue, make sure that no jobs are running in the queue. Use the bqueues command to view a list of existing queues and the jobs that are running in those queues. If jobs are in the queue that you want to remove, you must switch pending and running jobs to another queue, then remove the queue. If you remove a queue that has pending jobs in it, the jobs are temporarily moved to a lost_and_found queue. The job state does not change. Running jobs continue, and jobs that are pending in the original queue are pending in the lost_and_found queue. Jobs remain pending until the user or the queue administrator uses the bswitch command to switch the jobs into a regular queue. Jobs in other queues are not affected. Procedure Log in as the primary administrator on any host in the cluster. Close the queue to prevent any new jobs from being submitted. badmin qclose night Queue night is closed Switch all pending and running jobs into another queue. For example, the bswitch -q night idle 0 command chooses jobs from the night queue to the idle queue. The job ID number 0 switches all jobs. bjobs -u all -q night JOBID USER STAT QUEUE FROM_HOST EXEC_HOST JOB_NAME SUBMIT_TIME 5308 user5 RUN night hostA hostD job5 Nov 21 18:16 5310 user5 PEND night hostA hostC job10 Nov 21 18:17 bswitch -q night idle 0 Job is switched to queue Job is switched to queue Edit the LSB_CONFDIR/cluster_name/configdir/lsb.queues file and remove or comment out the definition for the queue that you want to remove. Save the changes to the lsb.queues file. Run the badmin reconfig command to reconfigure the cluster. % badmin reconfig Checking configuration files ... No errors found. Do you want to reconfigure? [y/n] y Reconfiguration initiated The badmin reconfig command checks for configuration errors. If no unrecoverable errors are found, you are asked to confirm reconfiguration. If unrecoverable errors are found, reconfiguration exits. Results If you get errors, see Troubleshooting LSF problems for help with some common configuration errors. For more information about the lsb.queues file, see the Configuration Reference. For more information about the badmin reconfig command, see the Command Reference. © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-06 21:34:41 "},"chapter4/section2/subsection5/configure_LSF_startup.html":{"url":"chapter4/section2/subsection5/configure_LSF_startup.html","title":"配置 LSF 启动","keywords":"","body":"配置 LSF 启动 Use the lsf.sudoers file so that LSF administrators can start and stop LSF daemons. Set up LSF to start automatically. Allowing LSF administrators to start LSF daemons with lsf.sudoers To allow LSF administrators to start and stop LSF daemons, configure the /etc/lsf.sudoers file. If the lsf.sudoers file does not exist, only root can start and stop LSF daemons.Setting up automatic LSF startup Configure LSF daemons to start automatically on every LSF server host in the cluster. © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-06 21:35:06 "},"chapter4/section2/subsection5/Allowing LSF administrators to start LSF daemons.html":{"url":"chapter4/section2/subsection5/Allowing LSF administrators to start LSF daemons.html","title":"Allowing LSF administrators to start LSF daemons","keywords":"","body":"Allowing LSF administrators to start LSF daemons To allow LSF administrators to start and stop LSF daemons, configure the /etc/lsf.sudoers file. If the lsf.sudoers file does not exist, only root can start and stop LSF daemons. About this task Using the lsf.sudoers file requires you to enable the setuid bit. Since this allows LSF administration commands to run with root privileges, do not proceed if you do not want these commands to run with root privileges. Procedure Log on as root to each LSF server host. Start with the LSF master host, and repeat these steps on all LSF hosts. Create an /etc/lsf.sudoers file on each LSF host and specify the LSF_STARTUP_USERS and LSF_STARTUP_PATH parameters. LSF_STARTUP_USERS=\"lsfadmin user1\" LSF_STARTUP_PATH=/usr/share/lsf/cluster1/10.1/sparc-sol2/etc LSF_STARTUP_PATH is normally the path to the LSF_SERVERDIR directory, where the LSF server binary files (lim, res, sbatchd, mbatchd, mbschd, and so on) are installed, as defined in your LSF_CONFDIR/lsf.conf file. The lsf.sudoers file must have file permission mode -rw------- (600) and be readable and writable only by root: # ls -la /etc/lsf.sudoers -rw------- 1 root lsf 95 Nov 22 13:57 lsf.sudoers Run the lsfrestart command to restart the cluster: # lsfrestart © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-06 21:35:19 "},"chapter4/section2/subsection5/Setting up automatic LSF startup.html":{"url":"chapter4/section2/subsection5/Setting up automatic LSF startup.html","title":"Setting up automatic LSF startup","keywords":"","body":"Setting up automatic LSF startup Configure LSF daemons to start automatically on every LSF server host in the cluster. Procedure Use the boot=y option of the hostsetup command. © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-06 21:35:30 "},"chapter4/section2/subsection6/manage_software_licenses_and_other_resources.html":{"url":"chapter4/section2/subsection6/manage_software_licenses_and_other_resources.html","title":"管理软件许可证及其他共享资源","keywords":"","body":"管理软件许可证及其他共享资源 Set up an LSF external LIM (ELIM) to monitor software licenses as dynamic shared resources. How LSF uses dynamic shared resources LSF recognizes two main types of resources: Host-based resources are available on all hosts in the cluster, for example, host type and model, or nodelocked software licenses. Shared resources are managed as dynamic load indexes available for a group of hosts in the cluster, for example, networked floating software licenses, shared file systems. Shared resources are shared by a group of LSF hosts. LSF manages shared resources for host selection and batch or interactive job execution. These resources are dynamic resources because the load on the system changes with the availability of the resources. Software licenses as shared resources The most common application of shared resources is to manage software application licenses. You submit jobs that require those licenses and LSF runs the jobs according to their priorities when licenses are available. When licenses are not available, LSF queues the jobs then dispatches them when licenses are free. Configuring application licenses as shared resources ensures optimal use of costly and critical resources. Define dynamic shared resources in an ELIM For LSF to use a shared resource like a software license, you must define the resource in the Resource section of the lsf.shared file. You define the type of resource and how often you want LSF to refresh the value of the resource. For LSF to track the resources correctly over time, you must define them as external load indexes. LSF updates load indexes periodically with a program called an External Load Information Manager (ELIM). An ELIM can be a shell script or a compiled binary program, which returns the values of the shared resources you define. The ELIM must be named elim and located in the LSF_SERVERDIR directory: /usr/share/lsf/lsf/cluser1/10.1/sparc-sol2/etc/elim You can find examples of sample ELIMs in the misc/examples directory. Example of shared licenses In the lsf.shared file, define two dynamic shared resources for software licenses, named license1 and license2: Begin Resource RESOURCENAME TYPE INTERVAL INCREASING RELEASE DESCRIPTION # Keywords license1 Numeric 30 N Y (license1 resource) license2 Numeric 30 N Y (license2 resource) End Resource The TYPE parameter for a shared resource can be one of the following types: Numeric Boolean String In this case, the resource is Numeric. The INTERVAL parameter specifies how often you want the value to be refreshed. In this example, the ELIM updates the value of the shared resources license1 and license2 every 30 seconds. The N in the INCREASING column means that the license resources are decreasing; that is, as more licenses become available, the load becomes lower. The Y in the RELEASE column means that the license resources are released when a job that uses the license is suspended. Map dynamic shared resources to hosts To make LSF aware of where the defined dynamic shared resources license1 and license2 you defined, map them to the hosts where they are located. In the LSF_CONFDIR/lsf.cluster.cluster_name file, configure a ResourceMap section to specify the mapping between shared resources license1 and license2 you defined in the LSF_CONFDIR/lsf.shared file, and the hosts you want to map them to: Begin ResourceMap RESOURCENAME LOCATION license1 [all] license1 [all] End ResourceMap In this resource map, the [all] attribute under the LOCATION parameter means that resources license1 and license2 under the RESOURCENAME parameter are available on all hosts in the cluster. Only one ELIM needs to run on the master host because the two resources are the same for all hosts. If the location of the resources is different on different hosts, a different ELIM must run on every host. Monitor dynamic shared resources For LSF to receive external load indexes correctly, the ELIM must send a count of the available resources to standard output in the following format: number_indexes [index_name index_value] ... The fields in this example contain the following information: 2 license1 3 license2 2 The total number of external load indexes (2) The name of the first external load index (license1) The value of the first load index (3) The name of the second external load index (license2) The value of the second load index (2) Write the ELIM program The ELIM must be an executable program, named elim, located in the LSF_SERVERDIR directory. When the lim daemon is started or restarted, it runs the elim program on the same host and takes the standard output of the external load indexes that are sent by the elim program. In general, you can define any quantifiable resource as an external load index, write an ELIM to report its value, and use it as an LSF resource. The following example ELIM program uses license1 and license2 and assumes that the FLEXlm license server controls them: #!/bin/sh NUMLIC=2 # number of dynamic shared resources while true do TMPLICS='/usr/share/lsf/cluster1/10.1/sparc-sol2/etc/lic -c /usr/share/lsf/cluster1/conf/license.dat -f license1, license2' LICS='echo $TMPLICS | sed -e s/-/_/g' echo $NUMLIC $LICS # $NUMLIC is number of dynamic shared resource sleep 30 # Resource done In the script, the sed command changes the minus sign (-) to underscore (_) in the license feature names because LSF uses the minus sign for calculation, and it is not allowed in resource names. The lic utility is available from IBM Support. You can also use the FLEXlm command lmstat to make your own ELIM. Use the dynamic shared resources To enable the new shared resources in your cluster, restart LSF with the following commands: lsadmin reconfig badmin reconfig If no errors are found, use the lsload -l command to verify the value of your dynamic shared resources: HOST_NAME status r15s r1m r15m ut pg io ls it tmp swp mem license1 license2 hosta ok 0.1 0.3 0.4 8% 0.2 50 73 0 62M 700M 425M 3 0 hostb ok 0.1 0.1 0.4 4% 5.7 3 3 0 79M 204M 64M 3 0 Submit jobs that use shared resources To submit a batch job that uses one license1 resource, use the command following command: % bsub -R 'rusage[license1=1:duration=1]' myjob In the resource requirement (rusage) string, duration=1 means that license1 is reserved for 1 minute to give LSF time to check it out from FLEXlm. You can also specify the resource requirement string at queue level, in the RES_REQ parameter for the queue. In the LSB_CONFDIR/cluster_name/configdir/lsb.queues file, specify the following resource requirement string: Begin Queue QUEUE_NAME = license1 RES_REQ=rusage[license1=1:duration=1] ... End Queue Then, submit a batch job that uses one license1 resource by using the following command: % bsub -q license1 myjob When licenses are available, LSF runs your jobs right away; when all licenses are in use, LSF puts your job in a queue and dispatches them as licenses become available. This way, all of your licenses are used to the best advantage. For more information For more information about the lsf.shared and lsf.cluster.cluster_name files and the parameters for configuring shared resources, see the Configuration Reference. For more information about adding external resources to your cluster and configuring an ELIM to customize resources, see External load indices in Administering IBM Spectrum LSF. © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-06 21:35:50 "},"chapter4/section3/troubleshooting_LSF_problems.html":{"url":"chapter4/section3/troubleshooting_LSF_problems.html","title":"4.3 LSF 排错","keywords":"","body":"4.3 LSF 排错 Troubleshoot common LSF problems and understand LSF error messages. If you cannot find a solution to your problem here, contact IBM Support. Solving common LSF problems Most problems are due to incorrect installation or configuration. Before you start to troubleshoot LSF problems, always check the error log files first. Log messages often point directly to the problem.LSF error messages The following error messages are logged by the LSF daemons, or displayed by the lsadmin ckconfig and badmin ckconfig commands. © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-09 12:01:30 "},"chapter4/section3/solving_common_LSF_problems.html":{"url":"chapter4/section3/solving_common_LSF_problems.html","title":"常见 LSF 问题","keywords":"","body":"常见 LSF 问题 Most problems are due to incorrect installation or configuration. Before you start to troubleshoot LSF problems, always check the error log files first. Log messages often point directly to the problem. Parent topic: Troubleshooting LSF problems Finding LSF error logs When something goes wrong, LSF server daemons log error messages in the LSF log directory (specified by the LSF_LOGDIR parameter in the lsf.conf file). Procedure Make sure that the primary LSF administrator owns LSF_LOGDIR, and that root can write to this directory. If an LSF server is unable to write to LSF_LOGDIR, then the error logs are created in /tmp. LSF logs errors to the following files: lim.log.host_name res.log.host_name pim.log.host_name mbatchd.log.master_host mbschd.log.master_host sbatchd.log.host_name vemkd.log.master_host If these log files contain any error messages that you do not understand, contact IBM Support. Diagnosing and fixing most LSF problems General troubleshooting steps for most LSF problems. Procedure Run the lsadmin ckconfig -v command and note any errors that are shown in the command output. Look for the error in one of the problems described in this section. If none of these troubleshooting steps applies to your situation, contact IBM Support. Use the following commands to restart the LSF cluster: # lsadmin limrestart all # lsadmin resrestart all # badmin hrestart all Run the ps -ef command to see whether the LSF daemons are running. Look for the processes similar to the following command output: root 17426 1 0 13:30:40 ? 0:00 /opt/lsf/cluster1/10.1/sparc-sol10/etc/lim root 17436 1 0 13:31:11 ? 0:00 /opt/lsf/cluster1/10.1/sparc-sol10/etc/sbatchd root 17429 1 0 13:30:56 ? 0:00 /opt/lsf/cluster1/10.1/sparc-sol10/etc/res Check the LSF error logs on the first few hosts that are listed in the Host section of the LSF_CONFDIR/lsf.cluster.cluster_name file. If the LSF_MASTER_LIST parameter is defined in the LSF_CONFDIR/lsf.conf file, check the error logs on the hosts that are listed in this parameter instead. Cannot open lsf.conf file You might see this message when you run the lsid file. The message usually means that the LSF_CONFDIR/lsf.conf file is not accessible to LSF. About this task By default, LSF checks the directory that is defined by the LSF_ENVDIR parameter for the lsf.conf file. If the lsf.conf file is not in LSF_ENVDIR, LSF looks for it in the /etc directory. For more information, see Setting up the LSF environment with cshrc.lsf and profile.lsf. Procedure Make sure that a symbolic link exists from /etc/lsf.conf to lsf.conf Use the csrhc.lsf or profile.lsf script to set up your LSF environment. Make sure that the cshrc.lsf or profile.lsf script is available for users to set the LSF environment variables. LIM dies quietly When the LSF LIM daemon exits unexpectedly, check for errors in the LIM configuration files. Procedure Run the following commands: lsadmin ckconfig -v This command displays most configuration errors. If the command does not report any errors, check in the LIM error log. LIM communication times out Sometimes the LIM is up, but running the lsload command prints the following error message:Communication time out. About this task If the LIM just started, LIM needs time to get initialized by reading configuration files and contacting other LIMs. If the LIM does not become available within one or two minutes, check the LIM error log for the host you are working on. To prevent communication timeouts when the local LIM is starting or restarting, define the parameter LSF_SERVER_HOSTS in the lsf.conf file. The client contacts the LIM on one of the LSF_SERVER_HOSTS and runs the command. At least one of the hosts that are defined in the list must have a LIM that is up and running. When the local LIM is running but the cluster has no master, LSF applications display the following message: Cannot locate master LIM now, try later. Procedure Check the LIM error logs on the first few hosts that are listed in the Host section of the lsf.cluster.cluster_name file. If the LSF_MASTER_LIST parameter is defined in the lsf.conf file, check the LIM error logs on the hosts that are listed in this parameter instead. Master LIM is down Sometimes the master LIM is up, but running the lsload or lshosts command displays the following error message: Master LIM is down; try later. About this task If the /etc/hosts file on the host where the master LIM is running is configured with the host name that is assigned to the loopback IP address (127.0.0.1), LSF client LIMs cannot contact the master LIM. When the master LIM starts up, it sets its official host name and IP address to the loopback address. Any client requests get the master LIM address as 127.0.0.1, and try to connect to it, and in fact tries to access itself. Procedure Check the IP configuration of your master LIM in /etc/hosts. The following example incorrectly sets the master LIM IP address to the loopback address: 127.0.0.1 localhost myhostname The following example correctly sets the master LIM IP address: 127.0.0.1 localhost 192.168.123.123 myhostname For a master LIM running on a host that uses an IPv6 address, the loopback address is ::1 The following example correctly sets the master LIM IP address by using an IPv6 address: ::1 localhost ipv6-localhost ipv6-loopback fe00::0 ipv6-localnet ff00::0 ipv6-mcastprefix ff02::1 ipv6-allnodes ff02::2 ipv6-allrouters ff02::3 ipv6-allhosts User permission denied If the remote host cannot securely determine the user ID of the user that is requesting remote execution, remote execution fails with the following error message: User permission denied.. Procedure Check the RES error log on the remote host for more detailed error message. If you do not want to configure an identification daemon (LSF_AUTH in lsf.conf), all applications that do remote executions must be owned by root with the setuid bit set. Run the following command: chmod 4755 filename If the application binary files are on an NFS-mounted file system, make sure that the file system is not mounted with the nosuid flag. If you are using an identification daemon (the LSF_AUTH parameter in the lsf.conf file), the inetd daemon must be configured. The identification daemon must not be run directly. Inconsistent host names in a name server with /etc/hosts and /etc/hosts.equiv can also cause this problem. If the LSF_USE_HOSTEQUIV parameter is defined in the lsf.conf file, check that the /etc/hosts.equiv file or the HOME/.rhosts file on the destination host has the client host name in it. For Windows hosts, users must register and update their Windows passwords by using the lspasswd command. Passwords must be 3 characters or longer, and 31 characters or less. For Windows password authentication in a non-shared file system environment, you must define the parameter LSF_MASTER_LIST in the lsf.conf file so that jobs run with correct permissions. If you do not define this parameter, LSF assumes that the cluster uses a shared file system environment. Remote execution fails because of non-uniform file name space A non-uniform file name space might cause a command to fail with the following error message: chdir(...) failed: no such file or directory. About this task You are trying to run a command remotely, but either your current working directory does not exist on the remote host, or your current working directory is mapped to a different name on the remote host. If your current working directory does not exist on a remote host, do not run commands remotely on that host. Procedure If the directory exists, but is mapped to a different name on the remote host, you must create symbolic links to make them consistent. LSF can resolve most, but not all, problems by using automount. The automount maps must be managed through NIS. Contact IBM Support if you are running automount and LSF is not able to locate directories on remote hosts. Batch daemons die quietly When the LSF batch daemons sbatchd and mbatchd exit unexpectedly, check for errors in the configuration files. About this task If the mbatchd daemon is running but the sbatchd daemon dies on some hosts, it might be because mbatchd is not configured to use those hosts. Procedure Check the sbatchd and mbatchd daemon error logs. Run the badmin ckconfig command to check the configuration. Check for email in the LSF administrator mailbox. sbatchd starts but mbatchd does not When the sbatchd daemon starts but the mbatchd daemon is not running, it is possible that mbatchd is temporarily unavailable because the master LIM is temporarily unknown. The following error message is displayed: sbatchd: unknown service. Procedure Run the lsid command to check whether LIM is running. If LIM is not running properly, follow the steps in the following topics to fix LIM problems: LIM dies quietly LIM communication times out Master LIM is down Check whether services are registered properly. Avoiding orphaned job processes LSF uses process groups to track all the processes of a job. However, if the application forks a child, the child becomes a new process group. The parent dies immediately, and the child process group is orphaned from the parent process, and cannot be tracked. About this task For more information about process tracking with Linux cgroups, see Memory and swap limit enforcement based on Linux cgroup memory subsystem. Procedure When a job is started, the application runs under the job RES or root process group. If an application creates a new process group, and its parent process ID (PPID) still belongs to the job, PIM can track this new process group as part of the job. The only reliable way to not lose track of a process is to prevent it from using a new process group. Any process that daemonizes itself is lost when child processes are orphaned from the parent process group because it changes its process group right after it is detached. Host not used by LSF The mbatchd daemon allows the sbatchd daemon to run only on the hosts that are listed in the Host section of the lsb.hosts file. If you configure an unknown host in the following configurations, mbatchd logs an error message: HostGroup or HostPartition sections of the lsb.hosts file, or as a HOSTS definition for a queue in the lsb.queues file. About this task If you try to configure a host that is not listed in the Host section of the lsb.hosts file, the mbatchd daemon logs the following message. mbatchd on host: LSB_CONFDIR/cluster1/configdir/file(line #): Host hostname is not used by lsbatch; ignored If you start the sbatchd daemon on a host that is not known by the mbatchd daemon, mbatchd rejects the sbatchd. The sbatchd daemon logs the following message and exits. This host is not used by lsbatch system. Procedure Add the unknown host to the list of hosts in the Host section of the lsb.hosts file. Start the LSF daemons on the new host. Run the following commands to reconfigure the cluster: lsadmin reconfig badmin reconfig UNKNOWN host type or model A model or type UNKNOWN indicates that the host is down or the LIM on the host is down. You need to take immediate action to restart LIM on the UNKNOWN host. Procedure Start the host. Run the lshosts command to see which host has the UNKNOWN host type or model. lshosts HOST_NAME type model cpuf ncpus maxmem maxswp server RESOURCES hostA UNKNOWN Ultra2 20.2 2 256M 710M Yes () Run the lsadmin limstartup command to start LIM on the host. lsadmin limstartup hostA Starting up LIM on .... done If EGO is enabled in the LSF cluster, you can run the following command instead: egosh ego start lim hostA Starting up LIM on .... done You can specify more than one host name to start LIM on multiple hosts. If you do not specify a host name, LIM is started on the host from which the command is submitted. To start LIM remotely on UNIX or Linux, you must be root or listed in the lsf.sudoers file (or the ego.sudoers file if EGO is enabled in the LSF cluster). You must be able to run the rsh command across all hosts without entering a password. Wait a few seconds, then run the lshosts command again. The lshosts command displays a specific model or type for the host or DEFAULT. If you see DEFAULT, it means that automatic detection of host type or model failed, and the host type that is configured in the lsf.shared file cannot be found. LSF works on the host, but a DEFAULT model might be inefficient because of incorrect CPU factors. A DEFAULT type might also cause binary incompatibility because a job from a DEFAULT host type can be migrated to another DEFAULT host type. DEFAULT host type or model If you see DEFAULT in lim -t, it means that automatic detection of host type or model failed, and the host type that is configured in the lsf.shared file cannot be found. LSF works on the host, but a DEFAULT model might be inefficient because of incorrect CPU factors. A DEFAULT type might also cause binary incompatibility because a job from a DEFAULT host type can be migrated to another DEFAULT host type. Procedure Run the lshosts command to see which host has the DEFAULT host model or type. lshosts HOST_NAME type model cpuf ncpus maxmem maxswp server RESOURCES hostA DEFAULT DEFAULT 1 2 256M 710M Yes () If Model or Type are displayed as DEFAULT when you use the lshosts command and automatic host model and type detection is enabled, you can leave it as is or change it. If the host model is DEFAULT, LSF works correctly but the host has a CPU factor of 1, which might not make efficient use of the host model. If the host type is DEFAULT, there might be binary incompatibility. For example, if one host is Linux and another is AIX, but both hosts are set to type DEFAULT, jobs that are running on the Linux host might be migrated to the AIX host and vice versa, which might cause the job to file. Run lim -t on the host whose type is DEFAULT: lim -t Host Type : NTX64 Host Architecture : EM64T_1596 Total NUMA Nodes : 1 Total Processors : 2 Total Cores : 4 Total Threads : 2 Matched Type : NTX64 Matched Architecture : EM64T_3000 Matched Model : Intel_EM64T CPU Factor : 60.0 NoteThe value of HostType and Host Architecture. Edit the lsf.shared file to configure the host type and host model for the host. In the HostType section, enter a new host type. Use the host type name that is detected with the lim -t command. Begin HostType TYPENAME DEFAULT CRAYJ NTX64 ... End HostType In the HostModel section, enter the new host model with architecture and CPU factor. Use the architecture that is detected with the lim -t commmand. Add the host model to the end of the host model list. The limit for host model entries is 127. Lines commented out with # are not counted in the 127-line limit. Begin HostModel MODELNAME CPUFACTOR ARCHITECTURE # keyword Intel_EM64T 20 EM64T_1596 End HostModel Save changes to the lsf.shared file. Run the lsadmin reconfig command to reconfigure LIM. Wait a few seconds, and run the lim -t command again to check the type and model of the host. © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-09 12:01:19 "},"chapter4/section3/LSF_error_messages.html":{"url":"chapter4/section3/LSF_error_messages.html","title":"LSF 错误信息","keywords":"","body":"LSF 错误信息 The following error messages are logged by the LSF daemons, or displayed by the lsadmin ckconfig and badmin ckconfig commands. General errors The following messages can be generated by any LSF daemon: can’t open file: error The daemon might not open the named file for the reason that is given by error. This error is usually caused by incorrect file permissions or missing files. All directories in the path to the configuration files must have execute (x) permission for the LSF administrator, and the actual files must have read (r) permission. Missing files might be caused by the following errors: Incorrect path names in the lsf.conf file Running LSF daemons on a host where the configuration files are not installed Having a symbolic link that points to a file or directory that does not exist file(line): malloc failed Memory allocation failed. Either the host does not have enough available memory or swap space, or there is an internal error in the daemon. Check the program load and available swap space on the host. If the swap space is full, you must add more swap space or run fewer (or smaller) programs on that host. auth_user: getservbyname(ident/tcp) failed: error; ident must be registered in services The LSF_AUTH=ident parameter is defined in the lsf.conf file, but the ident/tcp service is not defined in the services database. Add ident/tcp to the services database, or remove the LSF_AUTH=ident parameter from the lsf.conf file and use the setuid root command on the LSF files that require authentication. auth_user: operation(/) failed: error The LSF_AUTH=ident parameter is defined in the lsf.conf file, but the LSF daemon failed to contact the identd daemon on the host. Check that identd is defined in inetd.conf and the identd daemon is running on host. auth_user: Authentication data format error (rbuf=) from / auth_user: Authentication port mismatch (...) from / The LSF_AUTH=ident parameter is defined in the lsf.conf file, but there is a protocol error between LSF and the ident daemon on host. Make sure that the identd daemon on the host is configured correctly. userok: Request from bad port (), denied The LSF_AUTH=ident parameter is not defined, and the LSF daemon received a request that originates from a non-privileged port. The request is not serviced. Set the LSF binary files to be owned by root with the setuid bit set, or define the LSF_AUTH=ident parameter and set up an ident server on all hosts in the cluster. If the files are on an NFS-mounted file system, make sure that the file system is not mounted with the nosuid flag. userok: Forged username suspected from /: / The service request claimed to come from user claimed_user but ident authentication returned that the user was actual_user. The request was not serviced. userok: ruserok(,) failed The LSF_USE_HOSTEQUIV parameter is defined in the lsf.conf file, but host is not set up as an equivalent host in /etc/host.equiv, and user uid is not set up in a .rhosts file. init_AcceptSock: RES service(res) not registered, exiting init_AcceptSock: res/tcp: unknown service, exiting initSock: LIM service not registered. initSock: Service lim/udp is unknown. Read LSF Guide for help get_ports: service not registered The LSF services are not registered. init_AcceptSock: Can’t bind daemon socket to port : error, exiting init_ServSock: Could not bind socket to port : error These error messages can occur if you try to start a second LSF daemon (for example, RES is already running, and you run RES again). If so, and you want to start the new daemon, kill the running daemon or use the lsadmin or badmin commands to shut down or restart the daemon. Configuration errors The following messages are caused by problems in the LSF configuration files. General errors are listed first, and then errors from specific files. file(line): Section name expected after Begin; ignoring section file(line): Invalid section name name; ignoring section The keyword Begin at the specified line is not followed by a section name, or is followed by an unrecognized section name. file(line): section section: Premature EOF The end of file was reached before reading the End section line for the named section. file(line): keyword line format error for section section; Ignore this section The first line of the section must contain a list of keywords. This error is logged when the keyword line is incorrect or contains an unrecognized keyword. file(line): values do not match keys for section section; Ignoring line The number of fields on a line in a configuration section does not match the number of keywords. This error can be caused by not putting () in a column to represent the default value. file: HostModel section missing or invalid file: Resource section missing or invalid file: HostType section missing or invalid The HostModel, Resource, or HostType section in the lsf.shared file is either missing or contains an unrecoverable error. file(line): Name name reserved or previously defined. Ignoring index The name that is assigned to an external load index must not be the same as any built-in or previously defined resource or load index. file(line): Duplicate clustername name in section cluster. Ignoring current line A cluster name is defined twice in the same lsf.shared file. The second definition is ignored. file(line): Bad cpuFactor for host model model. Ignoring line The CPU factor declared for the named host model in the lsf.shared file is not a valid number. file(line): Too many host models, ignoring model name You can declare a maximum of 127 host models in the lsf.shared file. file(line): Resource name name too long in section resource. Should be less than 40 characters. Ignoring line The maximum length of a resource name is 39 characters. Choose a shorter name for the resource. file(line): Resource name name reserved or previously defined. Ignoring line. You attempted to define a resource name that is reserved by LSF or already defined in the lsf.shared file. Choose another name for the resource. file(line): illegal character in resource name: name, section resource. Line ignored. Resource names must begin with a letter in the set [a-zA-Z], followed by letters, digits, or underscores [a-zA-Z0-9_]. LIM messages The following messages are logged by the LIM: findHostbyAddr/: Host / is unknown by function: Gethostbyaddr_(/) failed: error main: Request from unknown host /: error function: Received request from non-LSF host / The daemon does not recognize host. The request is not serviced. These messages can occur if host was added to the configuration files, but not all the daemons were reconfigured to read the new information. If the problem still occurs after reconfiguring all the daemons, check whether the host is a multi-addressed host. rcvLoadVector: Sender (/) may have different config? MasterRegister: Sender (host) may have different config? LIM detected inconsistent configuration information with the sending LIM. Run the following command so that all the LIMs have the same configuration information. lsadmin reconfig Note any hosts that failed to be contacted. rcvLoadVector: Got load from client-only host /. Kill LIM on / A LIM is running on a client host. Run the following command, or go to the client host and kill the LIM daemon. lsadmin limshutdown host saveIndx: Unknown index name from ELIM LIM received an external load index name that is not defined in the lsf.shared file. If name is defined in lsf.shared, reconfigure the LIM. Otherwise, add name to the lsf.shared file and reconfigure all the LIMs. saveIndx: ELIM over-riding value of index This warning message is logged when the ELIM sent a value for one of the built-in index names. LIM uses the value from ELIM in place of the value that is obtained from the kernel. getusr: Protocol error numIndx not read (cc=num): error getusr: Protocol error on index number (cc=num): error Protocol error between ELIM and LIM. RES messages The following messages are logged by the RES: doacceptconn: getpwnam(@/) failed: error doacceptconn: User has uid on client host /, uid on RES host; assume bad user authRequest: username/uid /@/ does not exist authRequest: Submitter’s name @ is different from name on this host RES assumes that a user has the same user ID and user name on all the LSF hosts. These messages occur if this assumption is violated. If the user is allowed to use LSF for interactive remote execution, make sure the user’s account has the same user ID and user name on all LSF hosts. doacceptconn: root remote execution permission denied authRequest: root job submission rejected Root tried to run or submit a job but LSF_ROOT_REX is not defined in the lsf.conf file. resControl: operation permission denied, uid = The user with user ID uid is not allowed to make RES control requests. Only the LSF administrator can make RES control requests. If the LSF_ROOT_REX parameter is defined in the lsf.conffile, can also make RES control requests. resControl: access(respath, X_OK): error The RES received a restart request, but failed to find the file respath to re-execute itself. Make sure respath contains the RES binary, and it has execution permission. mbatchd and sbatchd messages The following messages are logged by the mbatchd and sbatchd daemons: renewJob: Job : rename(,) failed: error mbatchd failed in trying to resubmit a rerunnable job. Check that the file from exists and that the LSF administrator can rename the file. If from is in an AFS directory, check that the LSF administrator’s token processing is properly setup. logJobInfo_: fopen() failed: error logJobInfo_: write failed: error logJobInfo_: seek failed: error logJobInfo_: write xdrpos failed: error logJobInfo_: write xdr buf len failed: error logJobInfo_: close() failed: error rmLogJobInfo: Job : can’t unlink(): error rmLogJobInfo_: Job : can’t stat(): error readLogJobInfo: Job can’t open(): error start_job: Job : readLogJobInfo failed: error readLogJobInfo: Job : can’t read() size size: error initLog: mkdir() failed: error : fopen( failed: error getElogLock: Can’t open existing lock file : error getElogLock: Error in opening lock file : error releaseElogLock: unlink() failed: error touchElogLock: Failed to open lock file : error touchElogLock: close failed: error mbatchd failed to create, remove, read, or write the log directory or a file in the log directory, for the reason that is given in error. Check that the LSF administrator has read, write, and execute permissions on the logdir directory. replay_newjob: File at line : Queue not found, saving to queue replay_switchjob: File at line : Destination queue not found, switching to queue When the mbatchd daemon was reconfigured, jobs were found in queue but that queue is no longer in the configuration. replay_startjob: JobId : exec host not found, saving to host When the mbatchd daemon was reconfigured, the event log contained jobs that are dispatched to host, but that host is no longer configured to be used by LSF. do_restartReq: Failed to get hData of host / mbatchd received a request from sbatchd on host host_name, but that host is not known to mbatchd. Either the configuration file has changed but mbatchd was not reconfigured to pick up the new configuration, or host_name is a client host but the sbatchd daemon is running on that host. Run the following command to reconfigure the mbatchd daemon or kill the sbatchd daemon on host_name. badmin reconfig LSF command messages LSF daemon (LIM) not responding ... still trying During LIM restart, LSF commands might fail and display this error message. User programs that are linked to the LIM API also fail for the same reason. This message is displayed when LIM running on the master host list or server host list is restarted after configuration changes, such as adding new resources, or binary upgrade. Use the LSF_LIM_API_NTRIES parameter in the lsf.conf file or as an environment variable to define how many times LSF commands retry to communicate with the LIM API while LIM is not available. The LSF_LIM_API_NTRIES parameter is ignored by LSF and EGO daemons and all EGO commands. When the LSB_API_VERBOSE=Y parameter is set in the lsf.conf file, LSF batch commands display the not responding retry error message to stderr when LIM is not available. When the LSB_API_VERBOSE=N parameter is set in the lsf.conf file, LSF batch commands do not display the retry error message when LIM is not available. Batch command client messages LSF displays error messages when a batch command cannot communicate with the mbatchd daemon. The following table provides a list of possible error reasons and the associated error message output. Point of failure Possible reason Error message output Establishing a connection with the mbatchd daemon The mbatchd daemon is too busy to accept new connections. The connect() system call times out. LSF is processing your request. Please wait… The mbatchd daemon is down or no process is listening at either the LSB_MBD_PORT or the LSB_QUERY_PORT LSF is down. Please wait… The mbatchd daemon is down and the LSB_QUERY_PORT is busy bhosts displays LSF is down. Please wait. . .bjobs displays Cannot connect to LSF. Please wait… Socket error on the client side Cannot connect to LSF. Please wait… connect() system call fails Cannot connect to LSF. Please wait… Internal library error Cannot connect to LSF. Please wait… Send/receive handshake message to/from the mbatchd daemon The mbatchd daemon is busy. Client times out when LSF is waiting to receive a message from mbatchd. LSF is processing your request. Please wait… Socket read()/write() fails Cannot connect to LSF. Please wait… Internal library error Cannot connect to LSF. Please wait… EGO command messages You cannot run the egosh command because the administrator has chosen not to enable EGO in lsf.conf: LSF_ENABLE_EGO=N. If EGO is not enabled, the egosh command cannot find the ego.conf file or cannot contact the vemkd daemon (likely because it is not started). © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-09 12:00:40 "},"chapter5/run_jobs.html":{"url":"chapter5/run_jobs.html","title":"Chapter 5 作业调度管理","keywords":"","body":"Chapter 5 作业调度管理 运行，监视和控制提交给 LSF 的作业。 关于IBM Spectrum LSF 作业运行 作业监控 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-08 09:04:03 "},"chapter5/section1/about_IBM_Spectrum_LSF.html":{"url":"chapter5/section1/about_IBM_Spectrum_LSF.html","title":"5.1 关于 IBM Spectrum LSF","keywords":"","body":"5.1 关于 IBM Spectrum LSF Clusters, jobs, and queues The IBM Spectrum LSF (\"LSF\", short for load sharing facility) software is industry-leading enterprise-class software that distributes work across existing heterogeneous IT resources to create a shared, scalable, and fault-tolerant infrastructure, that delivers faster, balanced, more reliable workload performance and reduces cost. Hosts LSF daemons Batch jobs and tasks Host types and host models Users and administrators Resources Job lifecycle © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-08 09:27:32 "},"chapter5/section1/LSF_clusters_jobs_and_queues.html":{"url":"chapter5/section1/LSF_clusters_jobs_and_queues.html","title":"LSF 集群，作业与队列","keywords":"","body":"LSF 集群，作业与队列 The IBM Spectrum LSF (\"LSF\", short for load sharing facility) software is industry-leading enterprise-class software that distributes work across existing heterogeneous IT resources to create a shared, scalable, and fault-tolerant infrastructure, that delivers faster, balanced, more reliable workload performance and reduces cost. © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-08 09:27:35 "},"chapter5/section1/hosts.html":{"url":"chapter5/section1/hosts.html","title":"节点","keywords":"","body":"节点 A host is an individual computer in the cluster. Each host may have more than one processor. Multiprocessor hosts are used to run parallel jobs. A multiprocessor host with a single process queue is considered a single machine, while a box full of processors that each have their own process queue is treated as a group of separate machines. Commands lsload — View load on hosts lshosts — View configuration information about hosts in the cluster including number of CPUS, model, type, and whether the host is a client or server bhosts — View batch server hosts in the cluster TipThe names of your hosts should be unique. They should not be the same as the cluster name or any queue defined for the cluster. © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-08 09:27:47 "},"chapter5/section1/LSF_daemons.html":{"url":"chapter5/section1/LSF_daemons.html","title":"LSF daemons","keywords":"","body":"LSF daemons LSF daemon Role mbatchd Job requests and dispatch mbschd Job scheduling sbatchd**res** Job execution Parent topic: About IBM Spectrum LSF mbatchd Master Batch Daemon running on the master host. Started by sbatchd. Responsible for the overall state of jobs in the system. Receives job submission, and information query requests. Manages jobs that are held in queues. Dispatches jobs to hosts as determined by mbschd. Configuration Port number is defined in lsf.conf. mbschd Master Batch Scheduler Daemon running on the master host. Works with mbatchd. Started by mbatchd. Makes scheduling decisions based on job requirements, and policies, and resource availability. Sends scheduling decisions to mbatchd. sbatchd Slave Batch Daemon running on each server host. Receives the request to run the job from mbatchd and manages local execution of the job. Responsible for enforcing local policies and maintaining the state of jobs on the host. The sbatchd forks a child sbatchd for every job. The child sbatchd runs an instance of res to create the execution environment in which the job runs. The child sbatchd exits when the job is complete. Commands badmin hstartup — Starts sbatchd badmin hshutdown — Shuts down sbatchd badmin hrestart — Restarts sbatchd Configuration Port number is defined in lsf.conf res Remote Execution Server (res) running on each server host. Accepts remote execution requests to provide transparent and secure remote execution of jobs and tasks. Commands lsadmin resstartup — Starts res lsadmin resshutdown — Shuts down res lsadmin resrestart — Restarts res Configuration Port number is defined in lsf.conf. lim Load Information Manager (LIM) running on each server host. Collects host load and configuration information and forwards it to the master LIM running on the master host. Reports the information that is displayed by lsload and lshosts. Static indices are reported when the LIM starts up or when the number of CPUs (ncpus) change. Static indices are: Number of CPUs (ncpus) Number of disks (ndisks) Total available memory (maxmem) Total available swap (maxswp) Total available temp (maxtmp) Dynamic indices for host load collected at regular intervals are: Hosts status (status) 15 second, 1 minute, and 15 minute run queue lengths (r15s, r1m, and r15m) CPU utilization (ut) Paging rate (pg) Number of login sessions (ls) Interactive idle time (it) Available swap space (swp) Available memory (mem) Available temp space (tmp) Disk IO rate (io) Commands lsadmin limstartup — Starts LIM lsadmin limshutdown — Shuts down LIM lsadmin limrestart — Restarts LIM lsload — View dynamic load values lshosts — View static host load values Configuration Port number is defined in lsf.conf. Master LIM The LIM running on the master host. Receives load information from the LIMs running on hosts in the cluster. Forwards load information to mbatchd, which forwards this information to mbschd to support scheduling decisions. If the master LIM becomes unavailable, a LIM on another host automatically takes over. Commands lsadmin limstartup — Starts LIM lsadmin limshutdown — Shuts down LIM lsadmin limrestart — Restarts LIM lsload — View dynamic load values lshosts — View static host load values Configuration Port number is defined in lsf.conf. ELIM External LIM (ELIM) is a site-definable executable that collects and tracks custom dynamic load indices. An ELIM can be a shell script or a compiled binary program, which returns the values of the dynamic resources you define. The ELIM executable must be named elim and located in LSF_SERVERDIR. pim Process Information Manager (PIM) running on each server host. Started by LIM, which periodically checks on pim and restarts it if it dies. Collects information about job processes running on the host such as CPU and memory that is used by the job, and reports the information to sbatchd. Commands bjobs — View job information © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-08 09:28:04 "},"chapter5/section1/Batch_jobs_and_tasks.html":{"url":"chapter5/section1/Batch_jobs_and_tasks.html","title":"Batch jobs and tasks","keywords":"","body":"Batch jobs and tasks You can either run jobs through the batch system where jobs are held in queues, or you can interactively run tasks without going through the batch system, such as tests. Parent topic: About IBM Spectrum LSF Job A unit of work that is run in the LSF system. A job is a command that is submitted to LSF for execution, using the bsub command. LSF schedules, controls, and tracks the job according to configured policies. Jobs can be complex problems, simulation scenarios, extensive calculations, anything that needs compute power. Commands bjobs — View jobs in the system bsub — Submit jobs Interactive batch job A batch job that allows you to interact with the application and still take advantage of LSF scheduling policies and fault tolerance. All input and output are through the terminal that you used to type the job submission command. When you submit an interactive job, a message is displayed while the job is awaiting scheduling. A new job cannot be submitted until the interactive job is completed or terminated. The bsub command stops display of output from the shell until the job completes, and no mail is sent to you by default. Use Ctrl-C at any time to terminate the job. Commands bsub -I — Submit an interactive job Interactive task A command that is not submitted to a batch queue and scheduled by LSF, but is dispatched immediately. LSF locates the resources that are needed by the task and chooses the best host among the candidate hosts that has the required resources and is lightly loaded. Each command can be a single process, or it can be a group of cooperating processes. Tasks are run without using the batch processing features of LSF but still with the advantage of resource requirements and selection of the best host to run the task based on load. Commands lsrun — Submit an interactive task lsgrun — Submit an interactive task to a group of hosts See also LSF utilities such as ch, lsacct, lsacctmrg, lslogin, lsplace, lsload, lsloadadj, lseligible, lsmon, lstcsh. Local task An application or command that does not make sense to run remotely. For example, the ls command on UNIX. Commands lsltasks — View and add tasks Configuration lsf.task— Configure system-wide resource requirements for tasks lsf.task.cluster — Configure cluster-wide resource requirements for tasks .lsftasks — Configure user-specific tasks Remote task An application or command that can be run on another machine in the cluster. Commands lsrtasks — View and add tasks Configuration lsf.task — Configure system-wide resource requirements for tasks lsf.task.cluster — Configure cluster-wide resource requirements for tasks .lsftasks — Configure user-specific tasks © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-08 09:28:20 "},"chapter5/section1/Host_types_and_host_models.html":{"url":"chapter5/section1/Host_types_and_host_models.html","title":"Host types and host models","keywords":"","body":"Host types and host models Hosts in LSF are characterized by host type and host model. The following example has a host type of X86_64. Host models are Opteron240, Opteron840, Intel_EM64T, Intel_IA64. Host type The combination of operating system version and host CPU architecture. All computers that run the same operating system on the same computer architecture are of the same type — in other words, binary-compatible with each other. Each host type usually requires a different set of LSF binary files. Commands: lsinfo -t — View all host types that are defined in lsf.shared Configuration: Defined in lsf.shared Mapped to hosts in lsf.cluster.cluster_name Host model The combination of host type and CPU speed (CPU factor) of the computer. All hosts of the same relative speed are assigned the same host model. The CPU factor is taken into consideration when jobs are being dispatched. Commands: lsinfo -m — View a list of currently running models lsinfo -M — View all models that are defined in lsf.shared Configuration: Defined in lsf.shared Mapped to hosts in lsf.cluster.cluster_name © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-08 09:28:31 "},"chapter5/section1/Users_and_administrators.html":{"url":"chapter5/section1/Users_and_administrators.html","title":"Users and administrators","keywords":"","body":"Users and administrators LSF user A user account that has permission to submit jobs to the LSF cluster. LSF administrator In general, you must be an LSF administrator to perform operations that will affect other LSF users. Each cluster has one primary LSF administrator, specified during LSF installation. You can also configure additional administrators at the cluster level and at the queue level. Primary LSF administrator The first cluster administrator specified during installation and first administrator listed in lsf.cluster.cluster_name. The primary LSF administrator account owns the configuration and log files. The primary LSF administrator has permission to perform clusterwide operations, change configuration files, reconfigure the cluster, and control jobs submitted by all users. Cluster administrator May be specified during LSF installation or configured after installation. Cluster administrators can perform administrative operations on all jobs and queues in the cluster. Cluster administrators have the same cluster-wide operational privileges as the primary LSF administrator except that they do not necessarily have permission to change LSF configuration files. For example, a cluster administrator can create an LSF host group, submit a job to any queue, or terminate another user’s job. Queue administrator An LSF administrator user account that has administrative permissions limited to a specified queue. For example, an LSF queue administrator can perform administrative operations on the specified queue, or on jobs running in the specified queue, but cannot change LSF configuration or operate on LSF daemons. © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-08 09:28:47 "},"chapter5/section1/Resources.html":{"url":"chapter5/section1/Resources.html","title":"Resources","keywords":"","body":"Resources Resource usage The LSF system uses built-in and configured resources to track resource availability and usage. Jobs are scheduled according to the resources available on individual hosts. Jobs that are submitted through the LSF system will have the resources that they use monitored while they are running. This information is used to enforce resource limits and load thresholds as well as fairshare scheduling. LSF collects information such as: Total CPU time consumed by all processes in the job Total resident memory usage in KB of all currently running processes in a job Total virtual memory usage in KB of all currently running processes in a job Currently active process group ID in a job Currently active processes in a job On UNIX, job-level resource usage is collected through PIM. Commands lsinfo — View the resources available in your cluster bjobs -l — View current resource usage of a job Configuration SBD_SLEEP_TIME in lsb.params — Configures how often resource usage information is sampled by PIM, collected by sbatchd, and sent to mbatchd Load indices Load indices measure the availability of dynamic, non-shared resources on hosts in the cluster. Load indices that are built into the LIM are updated at fixed time intervals. Commands lsload -l — View all load indices bhosts -l — View load levels on a host External load indices Defined and configured by the LSF administrator and collected by an External Load Information Manager (ELIM) program. The ELIM also updates LIM when new values are received. Commands lsinfo — View external load indices Static resources Built-in resources that represent host information that does not change over time, such as the maximum RAM available to user processes or the number of processors in a machine. Most static resources are determined by the LIM at startup. Static resources can be used to select appropriate hosts for particular jobs based on binary architecture, relative CPU speed, and system configuration. Load thresholds Two types of load thresholds can be configured by your LSF administrator to schedule jobs in queues. Each load threshold specifies a load index value: loadSched determines the load condition for dispatching pending jobs. If a host’s load is beyond any defined loadSched, a job will not be started on the host. This threshold is also used as the condition for resuming suspended jobs. loadStop determines when running jobs should be suspended. To schedule a job on a host, the load levels on that host must satisfy both the thresholds that are configured for that host and the thresholds for the queue from which the job is being dispatched. The value of a load index may either increase or decrease with load, depending on the meaning of the specific load index. Therefore, when comparing the host load conditions with the threshold values, you need to use either greater than (>) or less than ( Commands bhosts -l — View suspending conditions for hosts bqueues -l — View suspending conditions for queues bjobs -l — View suspending conditions for a particular job and the scheduling thresholds that control when a job is resumed Configuration lsb.hosts — Configure thresholds for hosts lsb.queues — Configure thresholds for queues Runtime resource usage limits Limit the use of resources while a job is running. Jobs that consume more than the specified amount of a resource are signaled or have their priority lowered. Configuration lsb.queues — Configure resource usage limits for queues Hard and soft limits Resource limits that are specified at the queue level are hard limits while those specified with job submission are soft limits. See setrlimit(2) man page for concepts of hard and soft limits. Resource allocation limits Restrict the amount of a given resource that must be available during job scheduling for different classes of jobs to start, and which resource consumers the limits apply to. If all of the resource has been consumed, no more jobs can be started until some of the resource is released. Configuration lsb.resources — Configure queue-level resource allocation limits for hosts, users, queues, and projects Resource requirements (bsub -R) Restrict which hosts the job can run on. Hosts that match the resource requirements are the candidate hosts. When LSF schedules a job, it collects the load index values of all the candidate hosts and compares them to the scheduling conditions. Jobs are only dispatched to a host if all load values are within the scheduling thresholds. Commands bsub -R — Specify resource requirement string for a job Configuration lsb.queues — Configure resource requirements for queues © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-08 09:29:02 "},"chapter5/section1/Job_lifecycle.html":{"url":"chapter5/section1/Job_lifecycle.html","title":"Job lifecycle","keywords":"","body":"Job lifecycle Parent topic: About IBM Spectrum LSF 1 Submit a job You submit a job from an LSF client or server with the bsub command. If you do not specify a queue when submitting the job, the job is submitted to the default queue. Jobs are held in a queue waiting to be scheduled and have the PEND state. The job is held in a job file in the LSF_SHAREDIR/cluster_name/logdir/info/ directory. Job ID LSF assigns each job a unique job ID when you submit the job. Job name You can also assign a name to the job with the -J option of bsub. Unlike the job ID, the job name is not necessarily unique. 2 Schedule job mbatchd looks at jobs in the queue and sends the jobs for scheduling to mbschd at a preset time interval (defined by the parameter JOB_SCHEDULING_INTERVAL in lsb.params). mbschd evaluates jobs and makes scheduling decisions based on the following: Job priority Scheduling policies Available resources mbschd selects the best hosts where the job can run and sends its decisions back to mbatchd. Resource information is collected at preset time intervals by the master LIM from LIMs on server hosts. The master LIM communicates this information to mbatchd, which in turn communicates it to mbschd to support scheduling decisions. 3 Dispatch job As soon as mbatchd receives scheduling decisions, it immediately dispatches the jobs to hosts. 4 Run job sbatchd handles job execution. It does the following: Receives the request from mbatchd Creates a child sbatchd for the job Creates the execution environment Starts the job using res The execution environment is copied from the submission host to the execution host and includes the following: Environment variables that are needed by the job Working directory where the job begins running Other system-dependent environment settings; for example: On UNIX and Linux, resource limits and umask On Windows, desktop and Windows root directory The job runs under the user account that submitted the job and has the status RUN. 5 Return output When a job is completed, it is assigned the DONE status if the job was completed without any problems. The job is assigned the EXIT status if errors prevented the job from completing. sbatchd communicates job information including errors and output to mbatchd. 6 Send email to client mbatchd returns the job output, job error, and job information to the submission host through email. Use the -o and -e options of bsub to send job output and errors to a file. Job report A job report is sent by email to the LSF client and includes the following information: Job information such as the following: CPU use Memory use Name of the account that submitted the job Job output Errors © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-08 09:29:20 "},"chapter5/section2/working_with_jobs.html":{"url":"chapter5/section2/working_with_jobs.html","title":"5.2 作业运行","keywords":"","body":"5.2 作业运行 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-06 10:07:19 "},"chapter5/section2/submitting_jobs_using_bsub.html":{"url":"chapter5/section2/submitting_jobs_using_bsub.html","title":"Submitting jobs (bsub)","keywords":"","body":"Submitting jobs (bsub) © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-06 10:07:19 "},"chapter5/section3/monitoring_jobs.html":{"url":"chapter5/section3/monitoring_jobs.html","title":"5.3 作业监控","keywords":"","body":"5.3 作业监控 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-06 10:07:19 "},"chapter6/Administer_LSF.html":{"url":"chapter6/Administer_LSF.html","title":"Chapter 6 LSF 集群维护管理","keywords":"","body":"Chapter 6 LSF 集群维护管理 了解如何管理 IBM Spectrum LSF 集群，如何控制守护程序，更改集群配置以及如何使用主机和队列。管理您的LSF 作业和作业调度策略。查看工作信息并控制工作。 了解如何为 LSF 作业配置和分配资源。 了解如何在 LSF 群集中提交，监视和控制高吞吐量和并行工作负载。 了解有关 LSF 错误和事件日志记录，以及 LSF 如何处理作业异常的信息。 调整 LSF 集群的性能和可伸缩性。 IBM Spectrum LSF 集群管理要点 了解如何管理 LSF 集群，控制守护程序，更改集群配置以及使用主机，队列和用户。 监视 IBM Spectrum LSF 集群操作和运行状况 了解如何监视集群性能，作业资源使用情况以及有关队列，作业和用户的其他信息。 管理 IBM Spectrum LSF 作业执行 了解如何管理 LSF 作业和作业调度策略。 查看作业信息，控制作业，并管理作业相关性，作业优先级，作业阵列，交互式作业，作业预处理和后处理，以及作业启动器。 配置和共享 IBM Spectrum LSF 作业资源 了解如何为 LSF 作业配置和分配资源。 在用户和项目之间公平地共享计算资源。 将资源分配限制应用于作业，管理主机和用户组，保留资源并指定作业的资源要求。 GPU 资源 了解如何为 LSF 作业配置和使用 GPU 资源。 使用 LSF 配置容器 为容器配置和使用 LSF 集成。 管理 IBM Spectrum LSF 的高吞吐量工作负载 了解如何在 LSF 集群中提交，监视和控制高吞吐量工作负载。 配置调度策略，以实现对短期作业的有效排队，调度和执行。 管理 IBM Spectrum LSF 并行工作负载 了解如何在 LSF 集群中提交，监视和控制并行工作负载。 配置保留资源的调度策略，以保证大型并行作业高效执行。 IBM Spectrum LSF 安全性 了解如何优化 LSF 集群的安全性。 IBM Spectrum LSF 高级配置 了解关于 LSF 错误和事件日志记录以及 LSF 如何处理作业异常的信息。 配置高级 LSF 功能。 IBM Spectrum LSF 性能调优 调整 LSF 集群的性能和可伸缩性。 IBM Spectrum LSF 能源感知调度 在大规模 LSF 安装中配置，管理和使用 IBM Spectrum LSF 能源感知调度功能，其中运行大型系统的能源需求，已成为这些系统总体成本的重要因素。 IBM Spectrum LSF 多集群功能 了解如何使用和管理 IBM Spectrum LSF 多集群功能，来实现跨 LSF 集群之间的资源共享。 IBM Spectrum LSF 高级版 配置和使用 IBM Spectrum LSF 高级版本（LSF Advanced Edition）。 学习使用专为具有高性能工作负载要求的大型集群，而设计的 LSF 的高级功能。 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-11 09:17:01 "},"chapter6/section1/Cluster management essentials.html":{"url":"chapter6/section1/Cluster management essentials.html","title":"6.1 Cluster management essentials","keywords":"","body":"6.1 Cluster management essentials © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-11 09:17:01 "},"chapter6/section2/Monitoring cluster operations and health.html":{"url":"chapter6/section2/Monitoring cluster operations and health.html","title":"6.2 Monitoring cluster operations and health","keywords":"","body":"6.2 Monitoring cluster operations and health © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-11 09:17:01 "},"chapter6/section3/Managing job execution.html":{"url":"chapter6/section3/Managing job execution.html","title":"6.3 Managing job execution","keywords":"","body":"6.3 Managing job execution © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-11 09:17:01 "},"chapter6/section4/Configuring and sharing job resources.html":{"url":"chapter6/section4/Configuring and sharing job resources.html","title":"6.4 Configuring and sharing job resources","keywords":"","body":"6.4 Configuring and sharing job resources © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-11 09:17:01 "},"chapter6/section5/GPU resources.html":{"url":"chapter6/section5/GPU resources.html","title":"6.5 GPU resources","keywords":"","body":"6.5 GPU resources © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-11 09:17:01 "},"chapter6/section6/Configuring containers.html":{"url":"chapter6/section6/Configuring containers.html","title":"6.6 Configuring containers","keywords":"","body":"6.6 Configuring containers © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-11 09:17:01 "},"chapter6/section7/High throughput workload administration.html":{"url":"chapter6/section7/High throughput workload administration.html","title":"6.7 High throughput workload administration","keywords":"","body":"6.7 High throughput workload administration © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-11 09:17:01 "},"chapter6/section8/Parallel workload administration.html":{"url":"chapter6/section8/Parallel workload administration.html","title":"6.8 Parallel workload administration","keywords":"","body":"6.8 Parallel workload administration © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-11 09:17:01 "},"chapter6/section9/Security in LSF.html":{"url":"chapter6/section9/Security in LSF.html","title":"6.9 Security in LSF","keywords":"","body":"6.9 Security in LSF © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-11 09:17:01 "},"chapter6/section10/Advanced configuration.html":{"url":"chapter6/section10/Advanced configuration.html","title":"6.10 Advanced configuration","keywords":"","body":"6.10 Advanced configuration © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-11 09:17:01 "},"chapter6/sectio11/Performance tuning.html":{"url":"chapter6/sectio11/Performance tuning.html","title":"6.11 Performance tuning","keywords":"","body":"6.11 Performance tuning © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-11 09:17:01 "},"chapter6/section12/Energy aware scheduling.html":{"url":"chapter6/section12/Energy aware scheduling.html","title":"6.12 Energy aware scheduling","keywords":"","body":"6.12 Energy aware scheduling © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-11 09:17:01 "},"chapter6/section13/LSF multicluster capability.html":{"url":"chapter6/section13/LSF multicluster capability.html","title":"6.13 LSF multicluster capability","keywords":"","body":"6.13 LSF multicluster capability © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-11 09:17:01 "},"chapter6/section14/LSF Advanced Edition.html":{"url":"chapter6/section14/LSF Advanced Edition.html","title":"6.14 LSF Advanced Edition","keywords":"","body":"6.14 LSF Advanced Edition © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-11 09:17:01 "},"chapter7/Reference.html":{"url":"chapter7/Reference.html","title":"Chapter 7 参考文档","keywords":"","body":"Chapter 7 参考文档 LSF 命令和配置参数的参考信息。 IBM Spectrum LSF 命令参考 IBM Spectrum LSF命令的参考。 IBM Spectrum LSF 配置参考 了解 IBM Spectrum LSF 的相关功能，文件，事件和环境变量的配置参数。 IBM Spectrum LSF API 参考 请参阅对 IBM Spectrum LSF API 的全面参考。 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-11 09:17:01 "},"chapter7/section1/Command_reference.html":{"url":"chapter7/section1/Command_reference.html","title":"7.1 Command reference","keywords":"","body":"7.1 命令参考 IBM Spectrum LSF命令的参考。 bacct Displays accounting statistics about finished jobs. badmin The badmin command is the administrative tool for LSF. bapp Displays information about application profile configuration. battach Runs a shell process to connect to an existing job execution host or container. battr Provides a set of subcommands to manage LSF host attributes for attribute affinity scheduling. bbot Moves a pending job to the bottom of the queue relative to the last job in the queue. bchkpnt Checkpoints one or more checkpointable jobs bclusters Displays information about IBM Spectrum LSF multicluster capability bconf Submits live reconfiguration requests, updating configuration settings in active memory without restarting daemons. bdata Provides a set of subcommands to query and manage IBM Spectrum LSF Data Manager. If no subcommands are supplied, bdata displays the command usage. bentags Queries or removes information about the energy policy tag from the mbatchd daemon, which is saved in the energy-aware scheduling database. Used with energy policy, or energy aware scheduling feature. bgadd Creates job groups bgdel Deletes job groups bgmod Modifies job groups bgpinfo Displays information about global fairshare. bhist Displays historical information about jobs bhosts Displays hosts and their static and dynamic resources bhpart Displays information about host partitions bimages Displays information on Docker container images bjdepinfo Displays job dependencies. bjgroup Displays information about job groups bjobs Displays and filters information about LSF jobs. Specify one or more job IDs (and, optionally, an array index list) to display information about specific jobs (and job arrays). bkill Sends signals to kill, suspend, or resume unfinished jobs bladmin Administrative tool for IBM Spectrum LSF License Scheduler. blaunch Launches parallel tasks on a set of hosts. blcollect License information collection daemon for LSF License Scheduler. The blcollect daemon collects license usage information. blcstat Displays dynamic update information from the blcollect daemon for LSF License Scheduler. blhosts Displays the names of all the hosts that are running the LSF License Scheduler daemon (bld). blimits Displays information about resource allocation limits of running jobs. blinfo Displays static LSF License Scheduler configuration information blkill Terminates an interactive (taskman) LSF License Scheduler task. blparams Displays information about configurable LSF License Scheduler parameters that are defined in the files lsf.licensescheduler and lsf.conf blstat Displays dynamic license information. bltasks Displays LSF License Scheduler interactive task information. blusers Displays license usage information for LSF License Scheduler. bmgroup Displays information about host groups and compute units. bmig Migrates checkpointable or rerunnable jobs. bmod Modifies job submission options of a job. bparams Displays information about configurable system parameters in the lsb.params file. bpeek Displays the stdout and stderr output of an unfinished job. bpost Sends external status messages and attaches data files to a job. bqueues Displays information about queues. bread Reads messages and attached data files from a job. brequeue Kills and requeues a job. bresize Decreases or increases tasks that are allocated to a running resizable job, or cancels pending job resize allocation requests. bresources Displays information about resource reservation, resource limits, and guaranteed resource policies. brestart Restarts checkpointed jobs. bresume Resumes one or more suspended jobs. brlainfo Displays host topology information. brsvadd Adds an advance reservation. brsvdel Deletes an advance reservation. brsvjob Shows information about jobs submitted with the brsvsub command to a specific advance reservation. brsvmod Modifies an advance reservation. brsvs Displays advance reservations. brsvsub Creates a dynamically scheduled reservation and submits a job to fill the advance reservation when the resources required by the job are available. brun Forces a job to run immediately. bsla Displays information about service classes. Service classes are used in guaranteed resource policies and service-level agreement (SLA) scheduling. bslots Displays slots available and backfill windows available for backfill jobs. bstage Stages data files for jobs with data requirements by copying files or creating symbolic links for them between the local staging cache and the job execution environment. You must run bstage only within the context of an LSF job (like blaunch). To access a file with the bstage command, you must have permission to read it. bstatus Gets current external job status or sets new job status. bstop Suspends unfinished jobs. bsub Submits a job to LSF by running the specified command and its arguments. bswitch Switches unfinished jobs from one queue to another. btop Moves a pending job relative to the first job in the queue. bugroup Displays information about user groups. busers Displays information about users and user groups. bwait Pauses and waits for the job query condition to be satisfied. ch Changes the host where subsequent commands run. gpolicyd Displays LSF global policy daemon information. lim Load information manager (LIM) daemon or service, monitoring host load. lsacct Displays accounting statistics on finished RES tasks in the LSF system. lsacctmrg Merges LSF RES task log files. lsadmin Administrative tool to control LIM and RES daemon operations in LSF. lsclusters Displays configuration information about LSF clusters. lseligible Displays whether a task is eligible for remote execution. lsfinstall The LSF installation and configuration script. lsfmon Install or uninstall LSF Monitor in an existing cluster. lsfrestart Restarts the LIM, RES, sbatchd, and mbatchd daemons on all hosts in the cluster lsfshutdown Shuts down the LIM, RES, sbatchd, and mbatchd daemons on all hosts in the cluster. lsfstartup Starts the LIM, RES, and sbatchd daemons on all hosts in the cluster. lsgrun Runs a task on a group of hosts. lshosts Displays hosts and their static resource information. lsid Displays the LSF version number, the cluster name, and the master host name. lsinfo Displays LSF configuration information. lsload Displays load information for hosts. lsloadadj Adjusts load indices on hosts. lslogin Remotely logs in to a lightly loaded host. lsltasks Displays or updates a local task list. lsmake Runs LSF make tasks in parallel. lsmon Displays load information for LSF hosts and periodically updates the display. lspasswd Registers Windows user passwords in LSF. Passwords must be 3 - 23 characters long. lsplace Displays hosts available to run tasks. lsportcheck Displays ports that LSF is currently using or the LSF ports that will be used before starting LSF. lsrcp Remotely copies files through LSF. lsreghost (UNIX) UNIX version of the lsreghost command registers UNIX LSF host names and IP addresses with LSF servers so that LSF servers can internally resolve these hosts without requiring a DNS server. lsreghost (Windows) Windows version of the lsreghost command registers Windows LSF host names and IP addresses with LSF servers so that LSF servers can internally resolve these hosts without requiring a DNS server. lsrtasks Displays or updates a remote task list. lsrun Runs an interactive task through LSF. lstcsh Load sharing tcsh for LSF pam Parallel Application Manager – job starter for MPI applications patchinstall UNIX only. Manage patches in LSF cluster. pversions (UNIX) UNIX version of the command. Displays the version information for IBM Spectrum LSF installed on UNIX hosts. pversions (Windows) Windows version of the command. Displays the version information for IBM Spectrum LSF installed on a Windows host. ssacct Displays accounting statistics about finished LSF session scheduler jobs. ssched Submit tasks through LSF session scheduler. taskman Checks out a license token and manages interactive UNIX applications. tspeek Displays the stdout and stderr output of an unfinished Terminal Services job. tssub Submits a Terminal Services job to LSF. wgpasswd Changes a user’s password for a Microsoft Windows workgroup. wguser Modifies user accounts for a Microsoft Windows workgroup © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-11 09:17:01 "},"chapter7/section2/Configuration_reference.html":{"url":"chapter7/section2/Configuration_reference.html","title":"7.2 Configuration reference","keywords":"","body":"7.2 配置参考 了解有关 IBM Spectrum LSF 功能，文件，事件和环境变量的配置参数。 配置文件 LSF 配置文件参考。 环境变量 了解如何为作业执行，作业调整大小通知命令和会话调度程序（ssched）设置 LSF 环境变量。 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-11 09:17:01 "},"chapter7/section2/subsection1/Configuration_files.html":{"url":"chapter7/section2/subsection1/Configuration_files.html","title":"Configuration files","keywords":"","body":"Configuration files LSF configuration files reference. Important Specify any domain names in all uppercase letters in all configuration files. cshrc.lsf and profile.lsf The user environment shell files cshrc.lsf and profile.lsf set the LSF operating environment on an LSF host.hosts For hosts with multiple IP addresses and different official host names configured at the system level, this file associates the host names and IP addresses in LSF.install.config The install.config file contains options for LSF installation and configuration. Use the lsfinstall -f install.config command to install LSF with the options that are specified in the install.config file.lim.acct The lim.acct file is the log file for the LSF Load Information Manager (LIM). Produced by the lsmon command, the lim.acct file contains host load information that is collected and distributed by LIM.lsb.acct The lsb.acct file is the batch job log file of LSF.lsb.applications The lsb.applications file defines application profiles. Use application profiles to define common parameters for the same type of jobs, including the execution requirements of the applications, the resources they require, and how they should be run and managed.lsb.events The LSF batch event log file lsb.events is used to display LSF batch event history and for mbatchd failure recovery.lsb.globalpolicies This configuration file defines global policies for multiple clusters.lsb.hosts lsb.modules The lsb.modules file contains configuration information for LSF scheduler and resource broker modules. The file contains only one section, named PluginModule.lsb.params lsb.queues The lsb.queues file defines batch queues. Numerous controls are available at the queue level to allow cluster administrators to customize site policies.lsb.reasons lsb.resources The lsb.resources file contains configuration information for resource allocation limits, exports, resource usage limits, and guarantee policies. This file is optional.lsb.serviceclasses lsb.threshold The lsb.threshold configuration file defines energy-saving and CPU frequency policies. This file is optional.lsb.users lsf.acct lsf.cluster The cluster configuration file. There is one file for each cluster, called lsf.cluster.cluster_name. The cluster_name suffix is the name of the cluster defined in the Cluster section of the lsf.shared file. All LSF hosts are listed in this file, along with the list of LSF administrators and the installed LSF features.lsf.conf The lsf.conf file controls the operation of LSF. The lsf.conf file is created during installation and records all the settings chosen when LSF was installed. The lsf.conf file dictates the location of the specific configuration files and operation of individual servers and applications.lsf.datamanager The lsf.datamanager file controls the operation of IBM Spectrum LSF Data Manager features. Each cluster has one LSF data management configuration file, called lsf.datamanager.cluster_name. The cluster_name suffix is the name of the cluster that is defined in the Cluster section of the lsf.shared file. The file is read by the LSF data management daemon dmd. Since one LSF data manager can serve multiple LSF clusters, the contents of this file must be identical on each cluster that shares LSF data manager.lsf.licensescheduler The lsf.licensescheduler file contains LSF License Scheduler configuration information. All sections except ProjectGroup are required. In cluster mode, the Project section is also not required.lsf.shared The lsf.shared file contains common definitions that are shared by all load sharing clusters defined by lsf.cluster.cluster_name files.lsf.sudoers lsf.task setup.config slave.config The slave.config file contains options for installing and configuring a server host that can be dynamically added or removed. © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-07 21:08:06 "},"chapter7/section2/subsection2/Environment_variables.html":{"url":"chapter7/section2/subsection2/Environment_variables.html","title":"Environment variables","keywords":"","body":"Environment variables Learn how LSF environment variables are set for job execution, job resize notification command, and for session scheduler (ssched). Environment variables set for job execution In addition to environment variables inherited from the user environment, LSF also sets several other environment variables for batch jobs.Environment variables for resize notification command Environment variables for session scheduler (ssched) Environment variables for data provenance Environment variable reference Reference for LSF environment variables. © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-07 21:09:38 "},"chapter7/section2/subsection2/Environment variables set for job execution.html":{"url":"chapter7/section2/subsection2/Environment variables set for job execution.html","title":"Environment variables set for job execution","keywords":"","body":"Environment variables set for job execution © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-11 09:17:01 "},"chapter7/section2/subsection2/Environment variables for resize notification command.html":{"url":"chapter7/section2/subsection2/Environment variables for resize notification command.html","title":"Environment variables for resize notification command","keywords":"","body":"Environment variables for resize notification command © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-11 09:17:01 "},"chapter7/section2/subsection2/Environment variables for session scheduler.html":{"url":"chapter7/section2/subsection2/Environment variables for session scheduler.html","title":"Environment variables for session scheduler (ssched)","keywords":"","body":"Environment variables for session scheduler (ssched) © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-11 09:17:01 "},"chapter7/section2/subsection2/Environment variables for data provenance.html":{"url":"chapter7/section2/subsection2/Environment variables for data provenance.html","title":"Environment variables for data provenance","keywords":"","body":"Environment variables for data provenance © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-11 09:17:01 "},"chapter7/section2/subsection2/Environment variable reference.html":{"url":"chapter7/section2/subsection2/Environment variable reference.html","title":"Environment variable reference","keywords":"","body":"Environment variable reference © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-11 09:17:01 "},"chapter7/section3/API_reference.html":{"url":"chapter7/section3/API_reference.html","title":"7.3 API reference","keywords":"","body":"7.3 API reference 请参阅对 IBM Spectrum LSF API 的全面参考。 LSF API reference © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-11 09:17:01 "},"chapter8/Extend_LSF.html":{"url":"chapter8/Extend_LSF.html","title":" Chapter 8 LSF 拓展","keywords":"","body":"Chapter 8 LSF 拓展 配置和使用 IBM Spectrum LSF 集成。 IBM Spectrum LSF 会话调度程序 IBM Spectrum LSF Session Scheduler 安装，管理和使用 IBM Spectrum LSF Session Scheduler。 通过使用作业级任务调度程序，在单个 LSF 作业的分配范围内，运行大量短期任务的集合，该任务级任务调度程序为该作业分配一次资源，并为每个任务重用分配的资源。 IBM Spectrum LSF Session Scheduler 是运行短作业的理想选择，无论它们是任务列表还是带有参数执行的作业阵列。 带有 IBM Rational ClearCase 的 IBM Spectrum LSF IBM Spectrum LSF with IBM Rational ClearCase 许多站点使用 IBM®Rational®ClearCase®（ClearCase）环境进行修订版源控制和开发。 了解如何通过 IBM Rational ClearCase软件，来安装，配置和使用 IBM Spectrum LSF。 Cray Linux上的IBM Spectrum LSF IBM Spectrum LSF on Cray Linux IBM Spectrum LSF 与 Cray Linux 的集成适用于 LSF 版本8.0或更高版本，并支持与 Cray Linux Environment 4.0或更高版本的集成。 您必须有 LSF Standard Edition 或 LSF Advanced Edition。 LSF Express Edition不支持Cray Linux集成。 带有Apache Spark的IBM Spectrum LSF IBM Spectrum LSF with Apache Spark 在 Apache Spark 应用程序中配置和使用 IBM Spectrum LSF。 带有Apache Hadoop的IBM Spectrum LSF IBM Spectrum LSF with Apache Hadoop 在 Apache Hadoop 应用程序中，使用 IBM Spectrum LSF。 带有IBM Cluster Systems Manager的IBM Spectrum LSF IBM Spectrum LSF with IBM Cluster Systems Manager IBM Cluster Systems Manager（CSM）是一种系统管理工具，旨在对分布式和集群化的 IBM Power Systems 进行简单，低成本的管理。 了解如何在IBM Cluster Systems Manager中安装，配置和使用IBM Spectrum LSF。 具有 IBM Cloud Private 的 IBM Spectrum LSF IBM Spectrum LSF with IBM Cloud Private 可变地使用许可，用于扩展到云的动态计算工作负载，使您可以通过具有成本效益的按需购买即用许可，来优化基于云的资源使用。 管理员可以配置 IBM Spectrum LSF 集群，以通过外部负载索引监视器（ELIM）将CPU内核，CPU插槽，GPU插槽和主机计数，上载到 IBM Cloud Private 中的计量服务。 然后，管理员可以从IBM Cloud Private中的计量仪表板审核IBM Spectrum LSF资源使用情况。 LSF 作业步骤管理器 LSF Job Step Manager 使用JSDL提交作业 Submitting jobs using JSDL IBM Spectrum LSF模拟器 IBM Spectrum LSF Simulator 使用 LSF Simulator 通过在单独的内部环境中，模拟 LSF 集群来分析和调整 LSF 配置。 使用 LSF Simulator，您可以在不中断 LSF 生产环境的情况下使用不同的工作负载跟踪运行实验。 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-10 11:00:30 "},"chapter9/Best_practices_and_tips.html":{"url":"chapter9/Best_practices_and_tips.html","title":"Chapter 9 最佳实践与建议","keywords":"","body":"Chapter 9 经验与建议总结 查看使用 LSF 的各种最佳实践和技巧。 会计文件管理 Accounting file management 将 CPU 分配为并行作业的块 Allocating CPUs as blocks for parallel jobs 并行作业始终要求运行多个CPU。 如果可以将分配的CPU分配为块，则某些作业可以运行得更快。 清理并行作业执行问题 Cleaning up parallel job execution problems 将 IBM Aspera 配置为数据传输工具 Configuring IBM Aspera as a data transfer tool IBM Aspera 是一种数据传输工具，可以在高延迟网络中高效，基于策略地使用网络带宽。 自定义作业查询输出格式 Customizing job query output format 定义基于主机的外部资源 Defining external host-based resources 加强作业内存并与 Linux cgroup 交换 Enforcing job memory and swap with Linux cgroups 作业访问控制 Job access control 将IBM Spectrum LSF与Andrew File System（AFS）结合使用 Using IBM Spectrum LSF with Andrew File System (AFS) 了解 LSF 如何与 Andrew File System（AFS）集成，以便您可以配置 LSF 以适合您的需求。 维持集群性能 Maintaining cluster performance 在 LSF 中管理浮动软件许可证 Managing floating software licenses in LSF 通常，浮动软件许可证池由 LSF 中的数字资源表示。 每个需要许可证的工作都必须在其 rusag 表达式中包括许可证要求，以确保在分发工作时为该工作释放了足够的许可证。 在启用 CPU 频率调节器的情况下优化 LSF 作业处理 Optimizing LSF job processing with CPU frequency governors enabled Oracle Solaris 和 IBM AIX 上的操作系统分区和虚拟化 Operating system partitioning and virtualization on Oracle Solaris and IBM AIX 本文介绍了 LSF 在 OS 分区和虚拟化环境中的工作方式，重点是 Oracle Solaris 容器和 IBM AIX 分区。 基于主机的可用作业位置放置作业 Placing jobs based on available job slots of hosts 运行 checksum 以验证安装映像 Running checksum to verify installation images 跟踪作业依赖性 Tracking job dependencies 了解 mbatchd 的性能指标 Understanding mbatchd performance metrics 使用计算单元进行拓扑调度 Using compute units for topology scheduling 使用作业目录 Using job directories 使用 lsmake 加速 Android 构建 Using lsmake to accelerate Android builds 将 NVIDIA DGX 系统与 LSF 一起使用 Using NVIDIA DGX systems with LSF 将 ssh X11 转发与 IBM Spectrum LSF 一起使用 Using ssh X11 forwarding with IBM Spectrum LSF 为了使启用 X 的应用程序能够按预期运行，必须通过 ssh 来建立 X 连接，这是一种安全的方法。 为LSF API 使用 Python wrapper Using the Python wrapper for LSF API © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-10 11:00:39 "},"chapter10000000000/DRAFT.html":{"url":"chapter10000000000/DRAFT.html","title":"Chapter 10000000000","keywords":"","body":"Chapter 10000000000 草稿 第 10 ~ 12 章内容的构思与取舍 chapter 10 ：侧重于前置知识 chapter 11： 侧重于集群的实际操作经验总结 chapter 12： 侧重于产品的对比分析，行业调研等 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-10 15:04:19 "},"NOTE.html":{"url":"NOTE.html","title":"后记","keywords":"","body":"后记 联系作者 作者技术博客：BYA's Blog 微信：156 5657 5965 （联系请备注：姓名 - 行业 - 原因） 手机：176 2603 7549 邮箱：bya@mail.ustc.edu.cn; baiyongan1507@163.com 更新说明 原则： 具体内容优先级，按工作需求，择重点进行翻译，总结。 长期不定时更新关于 前置知识，行业经验的总结。 要求： 翻译：主要借助谷歌翻译，保证文字流畅。 排版格式：主要参照 《中文技术文档写作规范》 Github Repo：baiyongan/LSF_Doc 工具： 电子书编辑：gitbook、calibre markdown：typora 版本控制：git 内容评估： 更新频率：平均每天一个 section / subsection chapter 1：5 section chapter 2：略 chapter 3：4 section chapter 4：3 section， 6 subsection chapter 5：3 section，7 subsection chapter 6：14 section，66 subsection chapter 7：只链接，不翻译 chapter 8：10 section，9 subsection chapter 9：23 section 译作记录 2020.7.2 开始，配置 gitbook， 预计一个月左右，业余时间完成翻译初稿。将会是一个漫长、寂寞而枯燥的翻译、遣词造句、排版、美化的过程。 | 内容 | 优先级 | 初稿 | 定稿 | | ------------------------------------------------- | ------ | ---------------------- | ---- | | 说明、后记等，目录框架调整 | 高 | 7/2~7/4，7/5 afternoon | Y | | chapter 1 快速入门部分--section 7 | 高 | 7/7 1:00 am | Y | | 每一章的简介 README 部分 | 高 | 7/8 0:30 am, 9:00 am | Y | | chapter 1 增加第一小节 LSF 简介，更新参考资料 | 低 | 7/8 15:00 pm | Y | | chapter 3 普通用户操作基本章节 共4 section | 高 | 7/8 21:00 pm | | | 添加附录、参考文献等，临时新增 草稿页面 | 低 | 7/9 9:00 am | N | | chapter 3 section2 | 高 | 7/9 10:00 am | Y | | chapter 3 section 2 & 3 & 草稿页面 | 高 | 7/10 11:00 am 11:30 pm | Y | | chapter 3 section 4 & chapter 4 content | 高 | 7/11 9:00 am | | | | | | | | chapter 4 管理员基本操作 共 3 section 6subsection | 高 | | | | chapter 5 作业调度管理章节 共 14 section | 高 | | | | chapter 6 集群管理高级操作 | 中高 | | | | chapter 10~ 12 个人经验部分 | 中高 | | | | chapter 7 参考文档 | 中 | | | | chapter 8 LSF 拓展 | 中低 | | | | chapter 9 最佳实践 | 低 | | | | chapter 2 安装、迁移 | 低 | | | | chapter 1 入门介绍完善 | 低 | | | | 全体校验、发布电子版 | 低 | | | 2020.8 开始着重于 同行业应用 的结合，与经验总结。 2020.9 长期优化更新，搜集反馈。 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-11 09:28:05 "},"APPENDIX.html":{"url":"APPENDIX.html","title":"附录","keywords":"","body":"附录 部分关注企业/机构 超算平台 中科大超算中心 全球 Top 500 超级计算机 HPC 云平台 华为云｜高性能计算解决方案 阿里云｜弹性高性能计算 E-HPC EDA 行业相关 华为海思 AI 行业相关 HPC 行业相关 © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-09 09:59:07 "},"REFERENCE.html":{"url":"REFERENCE.html","title":"参考资料","keywords":"","body":"参考资料 主要参考 IBM Spectrum LSF V10.1 documentation Slurm version 20.02. GitBook 文档（中文版） IBM 系列 IBM Knowledge Center IBM 知识中心 IBM Developer IBM 开发者平台 IBM Support Product List IBM 支持团队产品目录 Platform Computing Legacy Documentation Platform Computing PDF 文档 相关 github 项目 IBM Spectrum Computing Public GitHub Respository SchedMD/slurm baiyongan/HPC_LogFile_Analysis © 2020 小白大侠 all right reserved，powered by Gitbook本文修订于： 2020-07-09 10:10:33 "}}